{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Robot Environment","text":"<p>A comprehensive Python framework for robotic pick-and-place operations with vision-based object detection and manipulation capabilities</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p><code>robot_environment</code> provides a complete software stack for controlling robotic arms with integrated computer vision for object detection, workspace management, and intelligent manipulation. The system combines real-time camera processing, Redis-based communication, and natural language interaction capabilities to enable robust pick-and-place operations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83e\udd16 Multi-Robot Support - Modular architecture supporting Niryo Ned2 and WidowX robotic arms</li> <li>\ud83d\udc41\ufe0f Vision-Based Object Detection - Integration with multiple detection models, using vision_detect_segment</li> <li>\ud83d\uddfa\ufe0f Workspace Management - Flexible workspace definition with camera-to-world coordinate transformation, using robot_workspace</li> <li>\ud83d\udce1 Redis Communication - Efficient image streaming and object data sharing via Redis, using redis_robot_comm</li> <li>\ud83d\udd0a Text-to-Speech - Natural language feedback using text2speech</li> <li>\ud83e\uddf5 Thread-Safe Operations - Concurrent camera updates and robot control with proper locking</li> <li>\ud83c\udfae Simulation Support - Compatible with both real robots and Gazebo simulation</li> <li>\ud83d\udcbe Object Memory Management - Intelligent tracking of detected objects with workspace-aware updates</li> </ul>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Environment Layer                        \u2502\n\u2502  (Central orchestrator coordinating all subsystems)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Robot Control \u2502   \u2502   Vision    \u2502   \u2502    Workspace     \u2502\n\u2502     Layer      \u2502   \u2502    Layer    \u2502   \u2502      Layer       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RobotController\u2502   \u2502FrameGrabber \u2502   \u2502    Workspace     \u2502\n\u2502   (Abstract)   \u2502   \u2502  (Abstract) \u2502   \u2502   (Abstract)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 NiryoRobot     \u2502   \u2502 NiryoFrame  \u2502   \u2502 NiryoWorkspace   \u2502\n\u2502  Controller    \u2502   \u2502   Grabber   \u2502   \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Redis Streams \u2502\n                    \u2502  (Images +    \u2502\n                    \u2502   Objects)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#core-components","title":"Core Components","text":"<p>Environment Layer - <code>Environment</code> - Central orchestrator managing all subsystems - Coordinates camera updates and robot control - Manages object memory with workspace-aware tracking - Handles thread-safe operations with proper locking</p> <p>Robot Control Layer - <code>Robot</code> - High-level robot API implementing pick-and-place operations - <code>RobotController</code> - Abstract base class for hardware control - <code>NiryoRobotController</code> - Niryo Ned2 implementation with pyniryo - <code>WidowXRobotController</code> - WidowX implementation with InterbotixManipulatorXS</p> <p>Vision Layer - <code>FrameGrabber</code> - Abstract camera interface with Redis streaming - <code>NiryoFrameGrabber</code> - Niryo-mounted camera with undistortion - <code>WidowXFrameGrabber</code> - Intel RealSense integration (stub)</p> <p>Workspace Layer - <code>Workspace</code> - Abstract workspace with coordinate transformation - <code>NiryoWorkspace</code> - Niryo-specific workspace implementation - <code>Workspaces</code> - Collection managing multiple workspaces</p> <p>Communication Layer - <code>RedisImageStreamer</code> - Variable-size image streaming (from <code>redis_robot_comm</code>) - <code>RedisMessageBroker</code> - Object detection results publishing - <code>RedisLabelManager</code> - Dynamic object label configuration</p> <p>For detailed architecture documentation, see architecture.md</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python \u2265 3.9</li> <li>Redis Server \u2265 5.0</li> <li>Robot-specific drivers:</li> <li>Niryo: <code>pyniryo</code> or <code>pyniryo2</code></li> <li>WidowX: <code>interbotix-xs-modules</code></li> </ul>"},{"location":"#basic-installation","title":"Basic Installation","text":"<pre><code>git clone https://github.com/dgaida/robot_environment.git\ncd robot_environment\npip install -e .\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Core dependencies are automatically installed:</p> <pre><code>pip install numpy opencv-python redis torch torchaudio\npip install vision-detect-segment redis-robot-comm robot-workspace text2speech\n</code></pre> <p>Robot-specific dependencies:</p> <pre><code># For Niryo Ned2\npip install pyniryo\n\n# For WidowX\npip install interbotix-xs-modules\n</code></pre>"},{"location":"#redis-server","title":"Redis Server","text":"<pre><code># Using Docker (recommended)\ndocker run -p 6379:6379 redis:alpine\n\n# Or install locally\n# Ubuntu/Debian:\nsudo apt-get install redis-server\n\n# macOS:\nbrew install redis\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#basic-pick-and-place","title":"Basic Pick and Place","text":"<pre><code>from robot_environment.environment import Environment\nfrom robot_workspace import Location\nimport threading\nimport time\n\n# Initialize environment\nenv = Environment(\n    el_api_key=\"your_elevenlabs_key\",  # For text-to-speech\n    use_simulation=False,               # Set True for Gazebo\n    robot_id=\"niryo\",                   # or \"widowx\"\n    verbose=True,\n    start_camera_thread=True            # Auto-start camera updates\n)\n\n# Alternative: Manual camera thread control\ndef start_camera_updates(environment, visualize=False):\n    def loop():\n        for img in environment.update_camera_and_objects(visualize=visualize):\n            pass\n    t = threading.Thread(target=loop, daemon=True)\n    t.start()\n    return t\n\n# Move to observation pose\nenv.robot_move2observation_pose(env.get_workspace_home_id())\n\n# Wait for object detection\ntime.sleep(2)\n\n# Get detected objects\ndetected_objects = env.get_detected_objects_from_memory()\nprint(f\"Detected {len(detected_objects)} objects:\")\nfor obj in detected_objects:\n    print(f\"  - {obj.label()} at [{obj.x_com():.2f}, {obj.y_com():.2f}]\")\n\n# Pick and place an object\nrobot = env.robot()\nsuccess = robot.pick_place_object(\n    object_name=\"pencil\",\n    pick_coordinate=[-0.1, 0.01],\n    place_coordinate=[0.1, 0.11],\n    location=Location.RIGHT_NEXT_TO\n)\n\nif success:\n    print(\"\u2713 Object successfully picked and placed\")\nelse:\n    print(\"\u2717 Pick and place operation failed\")\n\n# Cleanup\nenv.cleanup()\n</code></pre>"},{"location":"#multi-workspace-operations","title":"Multi-Workspace Operations","text":"<pre><code>from robot_environment.environment import Environment\nfrom robot_workspace import Location\n\nenv = Environment(\"key\", False, \"niryo\", verbose=True)\n\n# Get workspace IDs\nleft_ws_id = env.workspaces().get_workspace_left_id()\nright_ws_id = env.workspaces().get_workspace_right_id()\n\n# Observe left workspace\nenv.robot_move2observation_pose(left_ws_id)\nenv.set_current_workspace(left_ws_id)\ntime.sleep(2)\n\n# Get objects from left workspace\nleft_objects = env.get_detected_objects_from_workspace(left_ws_id)\nprint(f\"Left workspace: {len(left_objects)} objects\")\n\n# Transfer object to right workspace\nif len(left_objects) &gt; 0:\n    obj = left_objects[0]\n    robot.pick_place_object_across_workspaces(\n        object_name=obj.label(),\n        pick_workspace_id=left_ws_id,\n        pick_coordinate=[obj.x_com(), obj.y_com()],\n        place_workspace_id=right_ws_id,\n        place_coordinate=[0.25, -0.05],\n        location=Location.RIGHT_NEXT_TO\n    )\n</code></pre> <p>For complete multi-workspace examples, see examples/multi_workspace_example.py</p>"},{"location":"#advanced-features","title":"Advanced Features","text":""},{"location":"#object-detection-and-filtering","title":"Object Detection and Filtering","text":"<pre><code>from robot_workspace import Location\n\n# Get objects from memory (persists during robot motion)\ndetected_objects = env.get_detected_objects_from_memory()\n\n# Spatial filtering\nobjects_left = detected_objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.2, 0.0],\n    label=\"cube\"\n)\n\n# Find nearest object\nnearest, distance = detected_objects.get_nearest_detected_object(\n    coordinate=[0.25, 0.05],\n    label=\"pencil\"\n)\n\n# Size-based queries\nlargest, size = detected_objects.get_largest_detected_object()\nsmallest, size = detected_objects.get_smallest_detected_object()\n\n# Sort by size\nsorted_objects = detected_objects.get_detected_objects_sorted(ascending=True)\n</code></pre>"},{"location":"#workspace-coordinate-system","title":"Workspace Coordinate System","text":"<pre><code># Get workspace corners\nworkspace = env.get_workspace(0)\nupper_left = workspace.xy_ul_wc()\nlower_right = workspace.xy_lr_wc()\ncenter = workspace.xy_center_wc()\n\n# Transform camera coordinates to world coordinates\npose = workspace.transform_camera2world_coords(\n    workspace_id=\"niryo_ws\",\n    u_rel=0.5,  # Center of image (normalized [0,1])\n    v_rel=0.5,\n    yaw=0.0\n)\n\n# Get workspace dimensions\nwidth = workspace.width_m()\nheight = workspace.height_m()\nprint(f\"Workspace: {width:.3f}m \u00d7 {height:.3f}m\")\n</code></pre>"},{"location":"#object-memory-management","title":"Object Memory Management","text":"<pre><code># Memory is automatically updated when at observation pose\n# Manual memory operations:\n\n# Clear all memory\nenv.clear_memory()\n\n# Remove specific object after manipulation\nenv.remove_object_from_memory(\"pencil\", [0.25, 0.05])\n\n# Update object position after placement\nenv.update_object_in_memory(\n    object_label=\"cube\",\n    old_coordinate=[0.2, 0.0],\n    new_pose=new_pose_object\n)\n\n# Get memory contents\nmemory_objects = env.get_detected_objects_from_memory()\n</code></pre>"},{"location":"#finding-free-space","title":"Finding Free Space","text":"<pre><code># Find largest free area in workspace\nlargest_area_m2, center_x, center_y = env.get_largest_free_space_with_center()\n\nprint(f\"Free space: {largest_area_m2*10000:.2f} cm\u00b2\")\nprint(f\"Center: [{center_x:.2f}, {center_y:.2f}]\")\n\n# Place object at center of free space\nrobot.pick_place_object(\n    object_name=\"box\",\n    pick_coordinate=[0.2, 0.0],\n    place_coordinate=[center_x, center_y],\n    location=Location.NONE\n)\n</code></pre>"},{"location":"#pushing-objects","title":"Pushing Objects","text":"<pre><code># For objects too large to grip\nsuccess = robot.push_object(\n    object_name=\"large_box\",\n    push_coordinate=[0.3, 0.1],\n    direction=\"left\",    # \"up\", \"down\", \"left\", \"right\"\n    distance=50.0        # millimeters\n)\n</code></pre>"},{"location":"#custom-object-labels","title":"Custom Object Labels","text":"<pre><code># Add new detectable object\nmessage = env.add_object_name2object_labels(\"custom_tool\")\nprint(message)  # \"Added custom_tool to recognizable objects\"\n\n# Get current labels\nlabels = env.get_object_labels_as_string()\nprint(labels)  # \"I can recognize these objects: pencil, pen, custom_tool, ...\"\n</code></pre>"},{"location":"#text-to-speech-feedback","title":"Text-to-Speech Feedback","text":"<pre><code># Asynchronous speech (non-blocking)\nthread = env.oralcom_call_text2speech_async(\n    \"I have detected a pencil at position 0.25, 0.05\"\n)\n# Continue with other operations\nrobot.pick_object(\"pencil\", [0.25, 0.05])\nthread.join()  # Wait for speech to complete\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":""},{"location":"#robot-selection","title":"Robot Selection","text":"<pre><code># Niryo Ned2 (real robot)\nenv = Environment(\n    el_api_key=\"key\",\n    use_simulation=False,\n    robot_id=\"niryo\"\n)\n\n# Niryo in Gazebo simulation\nenv = Environment(\n    el_api_key=\"key\",\n    use_simulation=True,\n    robot_id=\"niryo\"\n)\n\n# WidowX robot\nenv = Environment(\n    el_api_key=\"key\",\n    use_simulation=False,\n    robot_id=\"widowx\"\n)\n</code></pre>"},{"location":"#adding-custom-workspaces","title":"Adding Custom Workspaces","text":"<p>Edit <code>niryo_workspace.py</code>:</p> <pre><code>def _set_observation_pose(self) -&gt; None:\n    if self._id == \"my_custom_workspace\":\n        self._observation_pose = PoseObjectPNP(\n            x=0.20, y=0.0, z=0.35,\n            roll=0.0, pitch=math.pi/2, yaw=0.0\n        )\n    # ... existing workspaces\n</code></pre>"},{"location":"#vision-configuration","title":"Vision Configuration","text":"<p>The vision system uses <code>vision_detect_segment</code> with configurable models:</p> <pre><code># Models are configured in environment.py\n# Default: OWL-V2 for open-vocabulary detection\n# Available: \"owlv2\", \"yolo-world\", \"yoloe-11l\", \"grounding_dino\"\n\n# To change model, modify in environment.py:\nself._visual_cortex = VisualCortex(\n    objdetect_model_id=\"yoloe-11l\",  # Fast with built-in segmentation\n    device=\"auto\",\n    verbose=verbose,\n    config=config\n)\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>See api.md.</p>"},{"location":"#performance-considerations","title":"Performance Considerations","text":""},{"location":"#detection-speed","title":"Detection Speed","text":"Model Detection Segmentation Total FPS Best For YOLOE-11L 6-10ms Built-in 100-160 FPS Real-time unified tasks YOLO-World 20-50ms 50-100ms (FastSAM) 10-25 FPS Speed-critical OWL-V2 100-200ms 200-500ms (SAM2) 1-3 FPS Custom classes Grounding-DINO 200-400ms 200-500ms (SAM2) 1-2 FPS Complex queries"},{"location":"#optimization-tips","title":"Optimization Tips","text":"<pre><code># 1. Use faster detection model\nconfig = get_default_config(\"yoloe-11s\")  # Fast variant\n\n# 2. Reduce object labels\nconfig.set_object_labels([\"cube\", \"cylinder\"])  # Only what you need\n\n# 3. Disable segmentation if not needed\nconfig.enable_segmentation = False\n\n# 4. Adjust camera update rate\ntime.sleep(0.5)  # Between camera updates\n\n# 5. Use GPU acceleration\ncortex = VisualCortex(\"yoloe-11l\", device=\"cuda\")\n</code></pre>"},{"location":"#memory-management","title":"Memory Management","text":"<ul> <li>Object memory stores detection history during robot motion</li> <li>Memory automatically updated when at observation pose</li> <li>Old detections removed when workspace visibility changes</li> <li>Manual updates from pick/place operations persist briefly</li> </ul>"},{"location":"#testing","title":"Testing","text":"<p>See TESTING.md</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":""},{"location":"#common-issues","title":"Common Issues","text":"<p>No Objects Detected</p> <pre><code># Check Redis connection\nfrom redis_robot_comm import RedisMessageBroker\nbroker = RedisMessageBroker()\nif broker.test_connection():\n    print(\"\u2713 Redis connected\")\n</code></pre> <p>Objects at Wrong Positions</p> <pre><code># Check workspace calibration\nworkspace = env.get_workspace_by_id(\"niryo_ws\")\nprint(f\"Corners: UL={workspace.xy_ul_wc()}, LR={workspace.xy_lr_wc()}\")\n\n# Ensure workspace is level and stable\n# Verify camera is properly mounted\n\n# Get fresh detection before picking\nenv.robot_move2observation_pose(workspace_id)\ntime.sleep(2)  # Wait for detection\nobjects = env.get_detected_objects_from_memory()\n</code></pre> <p>Robot Won't Move</p> <pre><code># Check connection\nrobot_ctrl = env.get_robot_controller()\npose = robot_ctrl.get_pose()\nprint(f\"Current pose: {pose}\")\n\n# Verify calibration (Niryo)\nrobot_ctrl.calibrate()\n\n# Check coordinates are reachable\nworkspace = env.get_workspace(0)\nprint(f\"Valid range: X=[{workspace.xy_lr_wc().x}, {workspace.xy_ul_wc().x}]\")\nprint(f\"             Y=[{workspace.xy_lr_wc().y}, {workspace.xy_ul_wc().y}]\")\n</code></pre> <p>Memory Issues</p> <pre><code># Clear stale memory\nenv.clear_memory()\n\n# Force fresh detection\nenv.robot_move2observation_pose(workspace_id)\ntime.sleep(2)\n\n# Check memory contents\nmemory = env.get_detected_objects_from_memory()\nprint(f\"Objects in memory: {len(memory)}\")\n</code></pre> <p>For comprehensive troubleshooting, see troubleshooting.md.</p>"},{"location":"#examples","title":"Examples","text":""},{"location":"#complete-examples","title":"Complete Examples","text":"<ul> <li>main.py - Basic pick and place demonstration</li> <li>examples/multi_workspace_example.py - Multi-workspace operations</li> </ul>"},{"location":"#run-examples","title":"Run Examples","text":"<pre><code># Start Redis server\ndocker run -p 6379:6379 redis:alpine\n\n# Run basic example\npython main.py\n\n# Run multi-workspace examples\ncd examples\npython multi_workspace_example.py\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Architecture Documentation - Detailed system architecture</li> <li>API Reference - Complete API documentation</li> <li>Multi-Workspace Guide - Multi-workspace operations</li> <li>Troubleshooting - Common issues and solutions</li> <li>Testing Guide - Testing documentation</li> </ul>"},{"location":"#development","title":"Development","text":""},{"location":"#code-quality","title":"Code Quality","text":"<pre><code># Install development dependencies\npip install -r requirements-dev.txt\n\n# Linting with Ruff\nruff check . --fix\n\n# Formatting with Black\nblack .\n\n# Type checking with mypy\nmypy robot_environment --ignore-missing-imports\n\n# Security scanning with Bandit\nbandit -r robot_environment/ -ll\n</code></pre>"},{"location":"#pre-commit-hooks","title":"Pre-Commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"#cicd","title":"CI/CD","text":"<p>The project includes comprehensive GitHub Actions workflows:</p> <ul> <li>Tests - Multi-platform testing (Ubuntu, Windows, macOS) across Python 3.9-3.11</li> <li>Code Quality - Ruff, Black, mypy checks</li> <li>Security - CodeQL and Bandit security scanning</li> <li>Dependency Review - Automated security audits</li> <li>Release - Automated package building on tags</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>See contributing.md for details.</p>"},{"location":"#related-projects","title":"Related Projects","text":"<p>This package integrates with several companion projects:</p> <ul> <li>vision_detect_segment - Object detection and segmentation</li> <li>redis_robot_comm - Redis-based communication</li> <li>robot_workspace - Workspace management and object representation</li> <li>text2speech - Natural language feedback</li> <li>robot_mcp - LLM-based robot control using Model Context Protocol</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See LICENSE for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this package in your research, please cite:</p> <pre><code>@software{robot_environment,\n  author = {Gaida, Daniel},\n  title = {robot_environment: Vision-Based Robotic Manipulation Framework},\n  year = {2025},\n  url = {https://github.com/dgaida/robot_environment}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This package builds upon:</p> <ul> <li>pyniryo - Niryo robot control</li> <li>InterbotixManipulatorXS - WidowX robot control</li> <li>Supervision - Annotation framework</li> <li>Transformers - Vision models</li> <li>Ultralytics - YOLO models</li> <li>Redis - High-performance messaging</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>GitHub Issues: https://github.com/dgaida/robot_environment/issues</li> <li>Documentation: architecture.md</li> <li>Examples: examples/</li> </ul>"},{"location":"#author","title":"Author","text":"<p>Daniel Gaida Email: daniel.gaida@th-koeln.de GitHub: @dgaida</p> <p>Project Link: https://github.com/dgaida/robot_environment</p>"},{"location":"#roadmap","title":"Roadmap","text":""},{"location":"#planned-features","title":"Planned Features","text":"<ul> <li>[ ] Additional robot support (UR5, Franka Emika)</li> <li>[ ] Improved collision detection and avoidance</li> <li>[ ] Force/torque sensor integration</li> <li>[ ] Advanced grasp planning</li> <li>[ ] Multi-robot coordination</li> <li>[ ] Web-based control interface</li> <li>[ ] ROS2 integration</li> <li>[ ] Improved simulation support</li> </ul>"},{"location":"#recent-additions","title":"Recent Additions","text":"<ul> <li>\u2705 Multi-workspace support</li> <li>\u2705 YOLOE model support with built-in segmentation</li> <li>\u2705 Enhanced object memory management</li> <li>\u2705 Workspace visibility tracking</li> </ul> <p>Last Updated: December 2025</p>"},{"location":"TESTING/","title":"Testing","text":"<p>We use <code>pytest</code> for unit and integration testing. Our goal is to maintain &gt;95% code coverage to ensure system reliability across various robot configurations.</p>"},{"location":"TESTING/#running-tests","title":"Running Tests","text":""},{"location":"TESTING/#standard-execution","title":"Standard Execution","text":"<pre><code># Run all unit tests (skipping integration and slow tests by default)\npython3 -m pytest\n\n# Run with coverage report\npython3 -m pytest --cov=robot_environment --cov-report=term-missing\n\n### Advanced Options\n```bash\n# Run specific test file\npython3 -m pytest tests/test_environment.py\n\n# Run integration tests (these require specific setup)\npython3 -m pytest -m integration\n\n# Run everything EXCEPT slow tests\npython3 -m pytest -m \"not slow\"\n\n# Run tests that require a real robot\npython3 -m pytest -m requires_robot\n</code></pre>"},{"location":"TESTING/#test-markers","title":"Test Markers","text":"<p>We use markers to categorize tests: - <code>integration</code>: Tests that verify interaction between multiple components. - <code>slow</code>: Tests that take a long time to run (e.g. complex simulations). - <code>requires_robot</code>: Tests that can only run when connected to actual hardware. - <code>requires_redis</code>: Tests that require a running Redis server.</p>"},{"location":"TESTING/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # Fixtures and configuration\n\u251c\u2500\u2500 test_environment.py      # Environment tests\n\u251c\u2500\u2500 test_environment_extended.py  # Extended environment tests\n\u251c\u2500\u2500 camera/\n\u2502   \u2514\u2500\u2500 test_niryo_framegrabber.py\n\u251c\u2500\u2500 robot/\n\u2502   \u251c\u2500\u2500 test_robot.py\n\u2502   \u251c\u2500\u2500 test_robot_api.py\n\u2502   \u251c\u2500\u2500 test_niryo_robot_controller.py\n\u2502   \u2514\u2500\u2500 test_widowx_robot_controller.py\n\u2514\u2500\u2500 test_integration.py      # Integration tests\n</code></pre> <p>For detailed testing information, see ../tests/README.md</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for the Robot Environment package.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Environment</li> <li>Robot</li> <li>Objects &amp; Detection</li> <li>Workspaces</li> <li>Camera</li> <li>Robot Controller</li> <li>Text-to-Speech</li> </ul>"},{"location":"api/#uml-class-diagram","title":"UML Class Diagram","text":""},{"location":"api/#environment","title":"Environment","text":"<p>The <code>Environment</code> class is the central orchestrator for the robot system.</p>"},{"location":"api/#constructor","title":"Constructor","text":"<pre><code>Environment(\n    el_api_key: str,\n    use_simulation: bool,\n    robot_id: str,\n    verbose: bool = False,\n    start_camera_thread: bool = True\n)\n</code></pre> <p>Parameters: - <code>el_api_key</code> (str): ElevenLabs API key for text-to-speech - <code>use_simulation</code> (bool): True for Gazebo simulation, False for real robot - <code>robot_id</code> (str): Robot identifier - \"niryo\" or \"widowx\" - <code>verbose</code> (bool): Enable verbose logging output - <code>start_camera_thread</code> (bool): Start automatic camera updates (default: True)</p> <p>Example:</p> <pre><code>env = Environment(\n    el_api_key=\"your_key\",\n    use_simulation=False,\n    robot_id=\"niryo\",\n    verbose=True\n)\n</code></pre>"},{"location":"api/#core-methods","title":"Core Methods","text":""},{"location":"api/#start_camera_updatesvisualizefalse","title":"<code>start_camera_updates(visualize=False)</code>","text":"<p>Start background thread for continuous camera updates and object detection.</p> <p>Parameters: - <code>visualize</code> (bool): Display annotated camera feed in window</p> <p>Returns: Thread object</p> <p>Example:</p> <pre><code>camera_thread = env.start_camera_updates(visualize=True)\n</code></pre>"},{"location":"api/#stop_camera_updates","title":"<code>stop_camera_updates()</code>","text":"<p>Stop the camera update thread.</p>"},{"location":"api/#cleanup","title":"<code>cleanup()</code>","text":"<p>Explicit cleanup method - closes connections and stops threads. Call when done with the environment.</p> <pre><code>env.cleanup()\n</code></pre>"},{"location":"api/#object-detection-methods","title":"Object Detection Methods","text":""},{"location":"api/#get_detected_objects","title":"<code>get_detected_objects()</code>","text":"<p>Get currently detected objects from Redis stream.</p> <p>Returns: <code>Objects</code> collection</p> <p>Example:</p> <pre><code>objects = env.get_detected_objects()\nfor obj in objects:\n    print(f\"{obj.label()} at {obj.xy_com()}\")\n</code></pre>"},{"location":"api/#get_detected_objects_from_memory","title":"<code>get_detected_objects_from_memory()</code>","text":"<p>Get objects from internal memory (thread-safe).</p> <p>Returns: <code>Objects</code> collection (copy)</p> <p>Example:</p> <pre><code># More reliable than get_detected_objects() during robot motion\nobjects = env.get_detected_objects_from_memory()\n</code></pre>"},{"location":"api/#add_object_name2object_labelsobject_name-str","title":"<code>add_object_name2object_labels(object_name: str)</code>","text":"<p>Add new object type to detection system via Redis.</p> <p>Parameters: - <code>object_name</code> (str): Object label to add</p> <p>Returns: Status message (str)</p> <p>Example:</p> <pre><code>env.add_object_name2object_labels(\"screwdriver\")\n# Result: \"Added screwdriver to the list of recognizable objects.\"\n</code></pre>"},{"location":"api/#get_object_labels_as_string","title":"<code>get_object_labels_as_string()</code>","text":"<p>Get comma-separated string of all detectable object labels.</p> <p>Returns: str</p> <p>Example:</p> <pre><code>labels = env.get_object_labels_as_string()\n# \"I can recognize these objects: cube, cylinder, pencil, ...\"\n</code></pre>"},{"location":"api/#memory-management-methods","title":"Memory Management Methods","text":""},{"location":"api/#clear_memory","title":"<code>clear_memory()</code>","text":"<p>Clear all objects from memory.</p> <pre><code>env.clear_memory()\n</code></pre>"},{"location":"api/#remove_object_from_memoryobject_label-str-coordinate-listfloat","title":"<code>remove_object_from_memory(object_label: str, coordinate: List[float])</code>","text":"<p>Remove specific object from memory after manipulation.</p> <p>Parameters: - <code>object_label</code> (str): Label of object to remove - <code>coordinate</code> (List[float]): Last known [x, y] position</p> <p>Example:</p> <pre><code>env.remove_object_from_memory(\"cube\", [0.2, 0.05])\n</code></pre>"},{"location":"api/#update_object_in_memoryobject_label-str-old_coordinate-listfloat-new_pose-poseobjectpnp","title":"<code>update_object_in_memory(object_label: str, old_coordinate: List[float], new_pose: PoseObjectPNP)</code>","text":"<p>Update object position in memory after movement.</p> <p>Parameters: - <code>object_label</code> (str): Object label - <code>old_coordinate</code> (List[float]): Previous [x, y] position - <code>new_pose</code> (PoseObjectPNP): New pose after movement</p> <p>Example:</p> <pre><code>new_pose = PoseObjectPNP(0.3, 0.1, 0.02, 0.0, 1.57, 0.0)\nenv.update_object_in_memory(\"cube\", [0.2, 0.05], new_pose)\n</code></pre>"},{"location":"api/#multi-workspace-methods","title":"Multi-Workspace Methods","text":""},{"location":"api/#get_current_workspace_id","title":"<code>get_current_workspace_id()</code>","text":"<p>Get ID of currently observed workspace.</p> <p>Returns: str or None</p>"},{"location":"api/#set_current_workspaceworkspace_id-str","title":"<code>set_current_workspace(workspace_id: str)</code>","text":"<p>Set the current workspace being observed.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p>"},{"location":"api/#get_detected_objects_from_workspaceworkspace_id-str","title":"<code>get_detected_objects_from_workspace(workspace_id: str)</code>","text":"<p>Get objects from specific workspace memory.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p> <p>Returns: <code>Objects</code> collection</p> <p>Example:</p> <pre><code>left_objects = env.get_detected_objects_from_workspace(\"niryo_ws_left\")\nright_objects = env.get_detected_objects_from_workspace(\"niryo_ws_right\")\n</code></pre>"},{"location":"api/#get_all_workspace_objects","title":"<code>get_all_workspace_objects()</code>","text":"<p>Get objects from all workspaces.</p> <p>Returns: Dict[str, Objects] - mapping workspace_id to Objects collection</p> <p>Example:</p> <pre><code>all_objects = env.get_all_workspace_objects()\nfor ws_id, objects in all_objects.items():\n    print(f\"{ws_id}: {len(objects)} objects\")\n</code></pre>"},{"location":"api/#clear_workspace_memoryworkspace_id-str","title":"<code>clear_workspace_memory(workspace_id: str)</code>","text":"<p>Clear memory for specific workspace.</p>"},{"location":"api/#remove_object_from_workspaceworkspace_id-str-object_label-str-coordinate-list","title":"<code>remove_object_from_workspace(workspace_id: str, object_label: str, coordinate: List)</code>","text":"<p>Remove object from specific workspace memory.</p>"},{"location":"api/#update_object_in_workspacesource_workspace_id-str-target_workspace_id-str-object_label-str-old_coordinate-list-new_coordinate-list","title":"<code>update_object_in_workspace(source_workspace_id: str, target_workspace_id: str, object_label: str, old_coordinate: List, new_coordinate: List)</code>","text":"<p>Move object between workspaces in memory.</p>"},{"location":"api/#workspace-methods","title":"Workspace Methods","text":""},{"location":"api/#get_workspaceindex-int-0","title":"<code>get_workspace(index: int = 0)</code>","text":"<p>Get workspace by index.</p> <p>Parameters: - <code>index</code> (int): 0-based index</p> <p>Returns: <code>Workspace</code> object</p>"},{"location":"api/#get_workspace_by_idworkspace_id-str","title":"<code>get_workspace_by_id(workspace_id: str)</code>","text":"<p>Get workspace by ID.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p> <p>Returns: <code>Workspace</code> object or None</p>"},{"location":"api/#get_workspace_home_id","title":"<code>get_workspace_home_id()</code>","text":"<p>Get ID of home workspace (index 0).</p> <p>Returns: str</p>"},{"location":"api/#get_visible_workspacecamera_pose-poseobjectpnp","title":"<code>get_visible_workspace(camera_pose: PoseObjectPNP)</code>","text":"<p>Get currently visible workspace based on camera pose.</p> <p>Parameters: - <code>camera_pose</code> (PoseObjectPNP): Current camera/gripper pose</p> <p>Returns: <code>Workspace</code> object or None</p>"},{"location":"api/#is_any_workspace_visible","title":"<code>is_any_workspace_visible()</code>","text":"<p>Check if any workspace is currently visible.</p> <p>Returns: bool</p>"},{"location":"api/#get_observation_poseworkspace_id-str","title":"<code>get_observation_pose(workspace_id: str)</code>","text":"<p>Get observation pose for workspace.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p> <p>Returns: <code>PoseObjectPNP</code></p>"},{"location":"api/#get_workspace_coordinate_from_pointworkspace_id-str-point-str","title":"<code>get_workspace_coordinate_from_point(workspace_id: str, point: str)</code>","text":"<p>Get world coordinate of special workspace points.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID - <code>point</code> (str): Point description - \"upper left corner\", \"upper right corner\", \"lower left corner\", \"lower right corner\", \"center point\"</p> <p>Returns: List[float] - [x, y] coordinates</p> <p>Example:</p> <pre><code>upper_left = env.get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left corner\")\ncenter = env.get_workspace_coordinate_from_point(\"niryo_ws\", \"center point\")\n</code></pre>"},{"location":"api/#spatial-analysis-methods","title":"Spatial Analysis Methods","text":""},{"location":"api/#get_largest_free_space_with_center","title":"<code>get_largest_free_space_with_center()</code>","text":"<p>Find largest free space in workspace and its center.</p> <p>Returns: Tuple[float, float, float] - (area_m2, center_x, center_y)</p> <p>Example:</p> <pre><code>area, center_x, center_y = env.get_largest_free_space_with_center()\nprint(f\"Free space: {area*10000:.2f} cm\u00b2 at [{center_x:.2f}, {center_y:.2f}]\")\n\n# Place object at center of free space\nrobot.pick_place_object(\n    object_name='box',\n    pick_coordinate=[0.2, 0.0],\n    place_coordinate=[center_x, center_y],\n    location=Location.NONE\n)\n</code></pre>"},{"location":"api/#robot-control-methods","title":"Robot Control Methods","text":""},{"location":"api/#robot_move2home_observation_pose","title":"<code>robot_move2home_observation_pose()</code>","text":"<p>Move robot to observe home workspace.</p>"},{"location":"api/#robot_move2observation_poseworkspace_id-str","title":"<code>robot_move2observation_pose(workspace_id: str)</code>","text":"<p>Move robot to observe specific workspace.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p> <p>Example:</p> <pre><code>env.robot_move2observation_pose(\"niryo_ws_left\")\n</code></pre>"},{"location":"api/#get_robot_pose","title":"<code>get_robot_pose()</code>","text":"<p>Get current gripper pose.</p> <p>Returns: <code>PoseObjectPNP</code></p>"},{"location":"api/#get_robot_in_motion","title":"<code>get_robot_in_motion()</code>","text":"<p>Check if robot is currently in motion.</p> <p>Returns: bool</p>"},{"location":"api/#get_robot_target_pose_from_relworkspace_id-str-u_rel-float-v_rel-float-yaw-float","title":"<code>get_robot_target_pose_from_rel(workspace_id: str, u_rel: float, v_rel: float, yaw: float)</code>","text":"<p>Convert relative image coordinates to world pose.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID - <code>u_rel</code> (float): Horizontal coordinate [0, 1] - <code>v_rel</code> (float): Vertical coordinate [0, 1] - <code>yaw</code> (float): Orientation in radians</p> <p>Returns: <code>PoseObjectPNP</code></p>"},{"location":"api/#camera-methods","title":"Camera Methods","text":""},{"location":"api/#get_current_frame","title":"<code>get_current_frame()</code>","text":"<p>Capture current camera image.</p> <p>Returns: np.ndarray (BGR image)</p>"},{"location":"api/#get_current_frame_width_height","title":"<code>get_current_frame_width_height()</code>","text":"<p>Get current frame dimensions.</p> <p>Returns: Tuple[int, int] - (width, height)</p>"},{"location":"api/#text-to-speech-methods","title":"Text-to-Speech Methods","text":""},{"location":"api/#oralcom_call_text2speech_asynctext-str","title":"<code>oralcom_call_text2speech_async(text: str)</code>","text":"<p>Asynchronous text-to-speech output.</p> <p>Parameters: - <code>text</code> (str): Message to speak</p> <p>Returns: Thread object</p> <p>Example:</p> <pre><code>thread = env.oralcom_call_text2speech_async(\"Task completed\")\nthread.join()  # Wait for completion\n</code></pre>"},{"location":"api/#properties","title":"Properties","text":"<ul> <li><code>workspaces()</code> - Returns Workspaces collection</li> <li><code>framegrabber()</code> - Returns FrameGrabber object</li> <li><code>robot()</code> - Returns Robot object</li> <li><code>use_simulation()</code> - Returns bool</li> <li><code>verbose()</code> - Returns bool</li> </ul>"},{"location":"api/#robot","title":"Robot","text":"<p>The <code>Robot</code> class provides high-level manipulation operations.</p>"},{"location":"api/#pick-and-place-methods","title":"Pick and Place Methods","text":""},{"location":"api/#pick_place_objectobject_name-str-pick_coordinate-list-place_coordinate-list-location-unionlocation-str-none-none-z_offset-float-0001","title":"<code>pick_place_object(object_name: str, pick_coordinate: List, place_coordinate: List, location: Union[Location, str, None] = None, z_offset: float = 0.001)</code>","text":"<p>Complete pick and place operation.</p> <p>Parameters: - <code>object_name</code> (str): Name of object to pick - <code>pick_coordinate</code> (List): [x, y] world coordinates for picking - <code>place_coordinate</code> (List): [x, y] world coordinates for placement - <code>location</code> (Location or str): Relative placement - Location.LEFT_NEXT_TO, \"right next to\", etc. - <code>z_offset</code> (float): Additional height offset in meters (default: 0.001)</p> <p>Returns: bool - Success status</p> <p>Example:</p> <pre><code>robot = env.robot()\n\n# Simple pick and place\nrobot.pick_place_object(\n    object_name='cube',\n    pick_coordinate=[0.2, 0.05],\n    place_coordinate=[0.3, 0.1],\n    location=Location.RIGHT_NEXT_TO\n)\n\n# Pick stacked object with z_offset\nrobot.pick_place_object(\n    object_name='top_cube',\n    pick_coordinate=[0.2, 0.05],\n    place_coordinate=[0.3, 0.1],\n    location=Location.NONE,\n    z_offset=0.02  # 2cm above detected position\n)\n</code></pre>"},{"location":"api/#pick_objectobject_name-str-pick_coordinate-list-z_offset-float-0001","title":"<code>pick_object(object_name: str, pick_coordinate: List, z_offset: float = 0.001)</code>","text":"<p>Pick up a specific object.</p> <p>Parameters: - <code>object_name</code> (str): Name of object to pick - <code>pick_coordinate</code> (List): [x, y] world coordinates - <code>z_offset</code> (float): Additional height offset (default: 0.001)</p> <p>Returns: bool</p> <p>Example:</p> <pre><code>success = robot.pick_object(\"pencil\", [0.25, 0.05])\n</code></pre>"},{"location":"api/#place_objectplace_coordinate-list-location-unionlocation-str-none-none","title":"<code>place_object(place_coordinate: List, location: Union[Location, str, None] = None)</code>","text":"<p>Place previously picked object.</p> <p>Parameters: - <code>place_coordinate</code> (List): [x, y] world coordinates - <code>location</code> (Location or str): Relative placement position</p> <p>Returns: bool</p> <p>Example:</p> <pre><code># Must call pick_object first\nrobot.pick_object(\"cube\", [0.2, 0.05])\nrobot.place_object([0.3, 0.1], Location.LEFT_NEXT_TO)\n</code></pre>"},{"location":"api/#push_objectobject_name-str-push_coordinate-list-direction-str-distance-float","title":"<code>push_object(object_name: str, push_coordinate: List, direction: str, distance: float)</code>","text":"<p>Push an object (for objects too large to grasp).</p> <p>Parameters: - <code>object_name</code> (str): Name of object to push - <code>push_coordinate</code> (List): [x, y] world coordinates - <code>direction</code> (str): \"up\", \"down\", \"left\", \"right\" - <code>distance</code> (float): Distance to push in millimeters</p> <p>Returns: bool</p> <p>Example:</p> <pre><code>robot.push_object(\n    object_name='large_box',\n    push_coordinate=[0.2, 0.0],\n    direction='right',\n    distance=50  # 5cm\n)\n</code></pre>"},{"location":"api/#multi-workspace-methods_1","title":"Multi-Workspace Methods","text":""},{"location":"api/#pick_place_object_across_workspacesobject_name-str-pick_workspace_id-str-pick_coordinate-list-place_workspace_id-str-place_coordinate-list-location-unionlocation-str-none-none-z_offset-float-0001","title":"<code>pick_place_object_across_workspaces(object_name: str, pick_workspace_id: str, pick_coordinate: List, place_workspace_id: str, place_coordinate: List, location: Union[Location, str, None] = None, z_offset: float = 0.001)</code>","text":"<p>Transfer object between workspaces.</p> <p>Parameters: - <code>object_name</code> (str): Object to transfer - <code>pick_workspace_id</code> (str): Source workspace ID - <code>pick_coordinate</code> (List): [x, y] in source workspace - <code>place_workspace_id</code> (str): Target workspace ID - <code>place_coordinate</code> (List): [x, y] in target workspace - <code>location</code> (Location or str): Relative placement - <code>z_offset</code> (float): Height offset for picking</p> <p>Returns: bool</p> <p>Example:</p> <pre><code>robot.pick_place_object_across_workspaces(\n    object_name='cube',\n    pick_workspace_id='niryo_ws_left',\n    pick_coordinate=[0.2, 0.05],\n    place_workspace_id='niryo_ws_right',\n    place_coordinate=[0.25, -0.05],\n    location=Location.RIGHT_NEXT_TO\n)\n</code></pre>"},{"location":"api/#pick_object_from_workspaceobject_name-str-workspace_id-str-pick_coordinate-list-z_offset-float-0001","title":"<code>pick_object_from_workspace(object_name: str, workspace_id: str, pick_coordinate: List, z_offset: float = 0.001)</code>","text":"<p>Pick object from specific workspace.</p>"},{"location":"api/#place_object_in_workspaceworkspace_id-str-place_coordinate-list-location-unionlocation-str-none-none","title":"<code>place_object_in_workspace(workspace_id: str, place_coordinate: List, location: Union[Location, str, None] = None)</code>","text":"<p>Place object in specific workspace.</p>"},{"location":"api/#movement-methods","title":"Movement Methods","text":""},{"location":"api/#move2observation_poseworkspace_id-str","title":"<code>move2observation_pose(workspace_id: str)</code>","text":"<p>Move to observation pose for workspace.</p> <p>Parameters: - <code>workspace_id</code> (str): Workspace ID</p> <p>Example:</p> <pre><code>robot.move2observation_pose(\"niryo_ws_left\")\ntime.sleep(2)  # Wait for detection\n</code></pre>"},{"location":"api/#calibrate","title":"<code>calibrate()</code>","text":"<p>Calibrate the robot.</p> <p>Returns: bool</p>"},{"location":"api/#utility-methods","title":"Utility Methods","text":""},{"location":"api/#get_pose","title":"<code>get_pose()</code>","text":"<p>Get current gripper pose.</p> <p>Returns: <code>PoseObjectPNP</code></p>"},{"location":"api/#get_target_pose_from_relworkspace_id-str-u_rel-float-v_rel-float-yaw-float","title":"<code>get_target_pose_from_rel(workspace_id: str, u_rel: float, v_rel: float, yaw: float)</code>","text":"<p>Convert relative coordinates to world pose.</p> <p>Returns: <code>PoseObjectPNP</code></p>"},{"location":"api/#properties_1","title":"Properties","text":"<ul> <li><code>environment()</code> - Returns Environment object</li> <li><code>robot()</code> - Returns RobotController object</li> <li><code>robot_in_motion()</code> - Returns bool</li> <li><code>verbose()</code> - Returns bool</li> </ul>"},{"location":"api/#objects-detection","title":"Objects &amp; Detection","text":""},{"location":"api/#object-class","title":"Object Class","text":"<p>Represents a detected object with spatial information.</p>"},{"location":"api/#properties_2","title":"Properties","text":"<pre><code># Identity\nobj.label()                    # str - Object label\nobj.workspace()                # Workspace - Associated workspace\n\n# Position (center of mass)\nobj.x_com()                    # float - X coordinate (meters)\nobj.y_com()                    # float - Y coordinate (meters)\nobj.xy_com()                   # Tuple[float, float] - (x, y)\n\n# Dimensions\nobj.width_m()                  # float - Width in meters\nobj.height_m()                 # float - Height in meters\nobj.shape_m()                  # Tuple[float, float] - (width, height)\nobj.size_m2()                  # float - Area in square meters\n\n# Pixel coordinates\nobj.u_min()                    # int - Left pixel\nobj.u_max()                    # int - Right pixel\nobj.v_min()                    # int - Top pixel\nobj.v_max()                    # int - Bottom pixel\nobj.width_px()                 # int - Width in pixels\nobj.height_px()                # int - Height in pixels\n\n# Orientation\nobj.gripper_rotation()         # float - Optimal gripper angle (radians)\nobj.rotation_rad()             # float - Object rotation (radians)\n\n# Pose\nobj.pose_com()                 # PoseObjectPNP - Center of mass pose\nobj.pose_center()              # PoseObjectPNP - Geometric center pose\n\n# Segmentation\nobj.mask()                     # np.ndarray or None - Binary mask\nobj.has_mask()                 # bool - Has segmentation mask\n</code></pre>"},{"location":"api/#methods","title":"Methods","text":"<pre><code># Serialization\nobj_dict = obj.to_dict()                    # Convert to dictionary\nobj_json = obj.to_json()                    # Convert to JSON string\nobj = Object.from_dict(obj_dict, workspace) # Reconstruct from dict\n\n# String representations\nobj.as_string_for_llm()                     # Detailed description\nobj.as_string_for_llm_lbl()                 # Compact description\n</code></pre> <p>Example:</p> <pre><code>obj = objects[0]\nprint(f\"Label: {obj.label()}\")\nprint(f\"Position: ({obj.x_com():.3f}, {obj.y_com():.3f})\")\nprint(f\"Size: {obj.width_m():.3f}m x {obj.height_m():.3f}m\")\nprint(f\"Area: {obj.size_m2() * 10000:.2f} cm\u00b2\")\nprint(f\"Rotation: {obj.gripper_rotation():.2f} rad\")\n\n# Serialize\nobj_dict = obj.to_dict()\nobj_json = obj.to_json()\n</code></pre>"},{"location":"api/#objects-class","title":"Objects Class","text":"<p>Collection of Object instances with query methods.</p>"},{"location":"api/#query-methods","title":"Query Methods","text":"<pre><code># Find by location\nobj = objects.get_detected_object(\n    coordinate=[0.2, 0.05],\n    label=\"cube\"\n)\n\n# Find nearest\nobj, distance = objects.get_nearest_detected_object(\n    coordinate=[0.25, 0.05],\n    label=\"pencil\"  # Optional: filter by label\n)\n\n# Find by size\nlargest, size = objects.get_largest_detected_object()\nsmallest, size = objects.get_smallest_detected_object()\n\n# Get sorted list\nsorted_objects = objects.get_detected_objects_sorted(ascending=True)\n\n# Spatial filtering\nfiltered = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.2, 0.0],\n    label=\"cube\"\n)\n</code></pre>"},{"location":"api/#location-filters","title":"Location Filters","text":"<pre><code>from robot_workspace import Location\n\n# Spatial relationships\nLocation.LEFT_NEXT_TO      # y &gt; coordinate[1]\nLocation.RIGHT_NEXT_TO     # y &lt; coordinate[1]\nLocation.ABOVE             # x &gt; coordinate[0]\nLocation.BELOW             # x &lt; coordinate[0]\nLocation.CLOSE_TO          # distance &lt;= 2cm\nLocation.ON_TOP_OF         # On top of reference object\nLocation.INSIDE            # Inside reference object\nLocation.NONE              # No spatial relationship\n</code></pre> <p>Example:</p> <pre><code>objects = env.get_detected_objects()\n\n# Get all cubes\ncubes = [obj for obj in objects if \"cube\" in obj.label().lower()]\n\n# Find nearest pencil to coordinate\npencil, dist = objects.get_nearest_detected_object(\n    coordinate=[0.25, 0.05],\n    label=\"pencil\"\n)\nprint(f\"Nearest pencil at {dist*100:.1f}cm distance\")\n\n# Get objects to the left\nleft_objects = objects.get_detected_objects(\n    location=Location.LEFT_NEXT_TO,\n    coordinate=[0.2, 0.0]\n)\n\n# Get largest object\nlargest, size = objects.get_largest_detected_object()\nprint(f\"Largest: {largest.label()} ({size*10000:.2f} cm\u00b2)\")\n\n# String representation\nprint(objects.get_detected_objects_as_comma_separated_string())\n# \"cube, cylinder, pencil\"\n</code></pre>"},{"location":"api/#workspaces","title":"Workspaces","text":""},{"location":"api/#workspace-class","title":"Workspace Class","text":"<p>Abstract base class for workspace definitions.</p>"},{"location":"api/#properties_3","title":"Properties","text":"<pre><code>workspace.id()                          # str - Workspace ID\nworkspace.width_m()                     # float - Width in meters\nworkspace.height_m()                    # float - Height in meters\nworkspace.img_shape()                   # Tuple - Image shape\n</code></pre>"},{"location":"api/#corner-methods","title":"Corner Methods","text":"<pre><code># World coordinates of corners\nworkspace.xy_ul_wc()                    # Upper left\nworkspace.xy_ur_wc()                    # Upper right\nworkspace.xy_ll_wc()                    # Lower left\nworkspace.xy_lr_wc()                    # Lower right\nworkspace.xy_center_wc()                # Center point\n</code></pre>"},{"location":"api/#coordinate-transformation","title":"Coordinate Transformation","text":"<pre><code>pose = workspace.transform_camera2world_coords(\n    workspace_id=\"niryo_ws\",\n    u_rel=0.5,          # Normalized [0, 1]\n    v_rel=0.5,\n    yaw=0.0\n)\n# Returns: PoseObjectPNP in world coordinates\n</code></pre> <p>Example:</p> <pre><code>workspace = env.get_workspace(0)\n\nprint(f\"ID: {workspace.id()}\")\nprint(f\"Size: {workspace.width_m():.3f}m x {workspace.height_m():.3f}m\")\n\n# Get corners\nul = workspace.xy_ul_wc()\nlr = workspace.xy_lr_wc()\nprint(f\"Upper left: ({ul.x:.3f}, {ul.y:.3f})\")\nprint(f\"Lower right: ({lr.x:.3f}, {lr.y:.3f})\")\n\n# Transform image coordinates to world\npose = workspace.transform_camera2world_coords(\n    workspace_id=\"niryo_ws\",\n    u_rel=0.5,  # Center of image\n    v_rel=0.5,\n    yaw=0.0\n)\n</code></pre>"},{"location":"api/#workspaces-collection","title":"Workspaces Collection","text":""},{"location":"api/#methods_1","title":"Methods","text":"<pre><code>workspaces = env.workspaces()\n\n# Get workspace by ID\nws = workspaces.get_workspace_by_id(\"niryo_ws\")\n\n# Get workspace by index\nws = workspaces.get_workspace(0)\n\n# Get home workspace\nhome_ws = workspaces.get_home_workspace()\nhome_id = workspaces.get_workspace_home_id()\n\n# Get visible workspace\nvisible_ws = workspaces.get_visible_workspace(camera_pose)\n\n# Get observation pose\nobs_pose = workspaces.get_observation_pose(\"niryo_ws\")\n</code></pre>"},{"location":"api/#camera","title":"Camera","text":""},{"location":"api/#framegrabber-class","title":"FrameGrabber Class","text":"<p>Abstract base class for camera interfaces.</p>"},{"location":"api/#methods_2","title":"Methods","text":"<pre><code>framegrabber = env.framegrabber()\n\n# Capture frame\nframe = framegrabber.get_current_frame()  # np.ndarray (BGR)\n\n# Get dimensions\nshape = framegrabber.get_current_frame_shape()\nwidth, height = framegrabber.get_current_frame_width_height()\n\n# Properties\ncurrent_frame = framegrabber.current_frame()\nenvironment = framegrabber.environment()\nverbose = framegrabber.verbose()\n</code></pre>"},{"location":"api/#niryoframegrabber","title":"NiryoFrameGrabber","text":"<p>Niryo-specific implementation.</p>"},{"location":"api/#additional-methods","title":"Additional Methods","text":"<pre><code># Camera intrinsics\nmtx = framegrabber.camera_matrix()\ndist = framegrabber.camera_dist_coeff()\n\n# Redis streaming\nstream_id = framegrabber.publish_workspace_image(\n    image,\n    workspace_id=\"niryo_ws\",\n    robot_pose={'x': 0.2, 'y': 0.0, 'z': 0.3}\n)\n\n# Visibility check\nis_visible = framegrabber.is_point_visible(\n    world_point=np.array([0.24, 0.01, 0.001]),\n    camera_to_gripper_transform=np.eye(4)\n)\n</code></pre>"},{"location":"api/#robot-controller","title":"Robot Controller","text":""},{"location":"api/#robotcontroller-class","title":"RobotController Class","text":"<p>Abstract base class for robot hardware control.</p>"},{"location":"api/#core-methods_1","title":"Core Methods","text":"<pre><code>controller = env.get_robot_controller()\n\n# Pose\npose = controller.get_pose()\n\n# Pick and place (low-level)\nsuccess = controller.robot_pick_object(pick_pose)\nsuccess = controller.robot_place_object(place_pose)\nsuccess = controller.robot_push_object(push_pose, direction, distance)\n\n# Movement\ncontroller.move2observation_pose(workspace_id)\n\n# Coordinate transformation\npose = controller.get_target_pose_from_rel(\n    workspace_id, u_rel, v_rel, yaw\n)\n\n# Calibration\nsuccess = controller.calibrate()\n\n# Thread safety\nwith controller.lock():\n    # Thread-safe robot operations\n    pose = controller.get_pose()\n</code></pre>"},{"location":"api/#niryorobotcontroller","title":"NiryoRobotController","text":"<p>Niryo Ned2 specific implementation.</p>"},{"location":"api/#additional-methods_1","title":"Additional Methods","text":"<pre><code># Camera\nmtx, dist = controller.get_camera_intrinsics()\nimg_compressed = controller.get_img_compressed()\n\n# Connection\ncontroller.reset_connection()\ncontroller.cleanup()\n</code></pre>"},{"location":"api/#text-to-speech","title":"Text-to-Speech","text":""},{"location":"api/#text2speech-class","title":"Text2Speech Class","text":"<p>Natural language feedback.</p>"},{"location":"api/#usage","title":"Usage","text":"<pre><code># Async speech\nthread = env.oralcom_call_text2speech_async(\n    \"I have picked up the cube\"\n)\n\n# Wait for completion (optional)\nthread.join()\n\n# Non-blocking\nthread = env.oralcom_call_text2speech_async(\"Task completed\")\n# Continue with other operations...\n</code></pre>"},{"location":"api/#data-types","title":"Data Types","text":""},{"location":"api/#poseobjectpnp","title":"PoseObjectPNP","text":"<p>6-DOF pose representation.</p> <pre><code>from robot_workspace import PoseObjectPNP\n\n# Create pose\npose = PoseObjectPNP(\n    x=0.25,        # meters\n    y=0.05,        # meters\n    z=0.02,        # meters\n    roll=0.0,      # radians\n    pitch=1.57,    # radians (\u03c0/2)\n    yaw=0.0        # radians\n)\n\n# Copy with offsets\nnew_pose = pose.copy_with_offsets(\n    x_offset=0.01,\n    y_offset=0.0,\n    z_offset=0.05\n)\n\n# Arithmetic\npose3 = pose1 + pose2\npose4 = pose1 - pose2\n\n# Transformation matrix\nmatrix = pose.to_transformation_matrix()  # 4x4 homogeneous\n\n# Coordinate access\ncoords = pose.xy_coordinate()  # [x, y]\n</code></pre>"},{"location":"api/#location-enum","title":"Location Enum","text":"<pre><code>from robot_workspace import Location\n\n# Spatial relationships\nLocation.LEFT_NEXT_TO\nLocation.RIGHT_NEXT_TO\nLocation.ABOVE\nLocation.BELOW\nLocation.ON_TOP_OF\nLocation.INSIDE\nLocation.CLOSE_TO\nLocation.NONE\n\n# Convert from string\nlocation = Location.convert_str2location(\"left next to\")\n</code></pre>"},{"location":"api/#complete-example","title":"Complete Example","text":"<pre><code>from robot_environment import Environment\nfrom robot_workspace import Location\nimport time\n\n# Initialize\nenv = Environment(\n    el_api_key=\"your_key\",\n    use_simulation=False,\n    robot_id=\"niryo\",\n    verbose=True\n)\n\n# Start camera updates\ncamera_thread = env.start_camera_updates(visualize=False)\n\n# Move to observation pose\nenv.robot_move2observation_pose(env.get_workspace_home_id())\ntime.sleep(2)  # Wait for detection\n\n# Get robot interface\nrobot = env.robot()\n\n# Get detected objects\nobjects = env.get_detected_objects()\nprint(f\"Detected {len(objects)} objects\")\n\n# Find nearest cube\ncube, dist = objects.get_nearest_detected_object(\n    coordinate=[0.2, 0.0],\n    label=\"cube\"\n)\n\nif cube:\n    # Find largest free space\n    area, center_x, center_y = env.get_largest_free_space_with_center()\n\n    # Pick and place\n    success = robot.pick_place_object(\n        object_name=cube.label(),\n        pick_coordinate=[cube.x_com(), cube.y_com()],\n        place_coordinate=[center_x, center_y],\n        location=Location.NONE\n    )\n\n    if success:\n        print(\"Task completed successfully\")\n\n# Cleanup\nenv.cleanup()\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # Robot operations\n    success = robot.pick_place_object(\n        object_name='cube',\n        pick_coordinate=[0.2, 0.05],\n        place_coordinate=[0.3, 0.1],\n        location=Location.RIGHT_NEXT_TO\n    )\n\n    if not success:\n        print(\"Operation failed\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nfinally:\n    env.cleanup()\n</code></pre>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>All robot operations are thread-safe:</p> <pre><code># Automatic locking\npose = env.get_robot_pose()\n\n# Manual locking for multiple operations\nwith env.get_robot_controller().lock():\n    pose1 = robot.get_pose()\n    pose2 = robot.get_target_pose_from_rel(\"niryo_ws\", 0.5, 0.5, 0.0)\n</code></pre>"},{"location":"api/#see-also","title":"See Also","text":"<ul> <li>README.md - Getting started</li> <li>Architecture Documentation - System design</li> <li>Multi-Workspace Guide - Advanced usage</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"architecture/","title":"Robot Environment Architecture Documentation","text":"<p>This document describes the architectural design of the <code>robot_environment</code> package, detailing component interactions, data flows, and integration with external packages.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>The <code>robot_environment</code> package provides a comprehensive framework for robotic pick-and-place operations with vision-based object detection. The architecture follows a layered design with clear separation of concerns:</p> <ol> <li>Environment Layer - Central orchestrator</li> <li>Robot Control Layer - Hardware abstraction and motion control</li> <li>Camera Layer - Image acquisition and streaming</li> <li>Workspace Layer - Coordinate transformation and workspace management</li> <li>Communication Layer - Redis-based data streaming</li> <li>Interaction Layer - Text-to-speech feedback</li> </ol> <p>Note: Object detection is handled by a separate process (<code>vision_detect_segment</code>) that communicates via Redis streams.</p>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Environment Layer                        \u2502\n\u2502  (Central orchestrator coordinating all subsystems)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Robot Control \u2502   \u2502   Camera    \u2502   \u2502    Workspace     \u2502\n\u2502     Layer      \u2502   \u2502    Layer    \u2502   \u2502      Layer       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 RobotController\u2502   \u2502FrameGrabber \u2502   \u2502    Workspace     \u2502\n\u2502   (Abstract)   \u2502   \u2502  (Abstract) \u2502   \u2502   (Abstract)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u2502                   \u2502                   \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 NiryoRobot     \u2502   \u2502 NiryoFrame  \u2502   \u2502 NiryoWorkspace   \u2502\n\u2502  Controller    \u2502   \u2502   Grabber   \u2502   \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 Redis Streams \u2502\n                    \u2502  (Images +    \u2502\n                    \u2502   Objects)    \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502 vision_detect \u2502\n                    \u2502   _segment    \u2502\n                    \u2502 (Separate     \u2502\n                    \u2502  Process)     \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#environment-layer","title":"Environment Layer","text":"<ul> <li><code>Environment</code> - Central orchestrator managing all subsystems</li> <li>Coordinates camera updates, object retrieval, and robot control</li> <li>Manages object memory with workspace-aware tracking</li> <li>Handles thread-safe operations with proper locking</li> </ul>"},{"location":"architecture/#robot-control-layer","title":"Robot Control Layer","text":"<ul> <li><code>Robot</code> - High-level robot API implementing pick-and-place operations</li> <li><code>RobotController</code> - Abstract base class for hardware control</li> <li><code>NiryoRobotController</code> - Niryo Ned2 implementation with pyniryo</li> <li><code>WidowXRobotController</code> - WidowX implementation with InterbotixManipulatorXS</li> </ul>"},{"location":"architecture/#camera-layer","title":"Camera Layer","text":"<ul> <li><code>FrameGrabber</code> - Abstract camera interface with Redis streaming</li> <li><code>NiryoFrameGrabber</code> - Niryo-mounted camera with undistortion</li> <li><code>WidowXFrameGrabber</code> - Intel RealSense integration (stub)</li> <li>External: Object detection via <code>vision_detect_segment</code> package (separate process)</li> </ul>"},{"location":"architecture/#workspace-layer","title":"Workspace Layer","text":"<ul> <li><code>Workspace</code> - Abstract workspace with coordinate transformation</li> <li><code>NiryoWorkspace</code> - Niryo-specific workspace implementation</li> <li><code>Workspaces</code> - Collection managing multiple workspaces</li> </ul>"},{"location":"architecture/#object-representation","title":"Object Representation","text":"<ul> <li><code>Object</code> - Detected object with full spatial information</li> <li><code>Objects</code> - Collection with spatial queries and filtering</li> <li><code>PoseObjectPNP</code> - 6-DOF pose representation</li> </ul>"},{"location":"architecture/#communication-layer-redis-based","title":"Communication Layer (Redis-based)","text":"<ul> <li><code>RedisImageStreamer</code> - Variable-size image streaming (from <code>redis_robot_comm</code>)</li> <li><code>RedisMessageBroker</code> - Object detection results consumption (from <code>redis_robot_comm</code>)</li> <li><code>RedisLabelManager</code> - Dynamic object label configuration (from <code>redis_robot_comm</code>)</li> </ul> <p>For detailed architecture documentation, see docs/README.md</p>"},{"location":"architecture/#data-flow-architecture","title":"Data Flow Architecture","text":""},{"location":"architecture/#complete-pick-and-place-workflow","title":"Complete Pick-and-Place Workflow","text":"<pre><code>1. INITIALIZATION\n   Environment (robot_environment process)\n      \u251c\u2500\u2192 Robot \u2192 RobotController (connects to hardware)\n      \u251c\u2500\u2192 FrameGrabber (initializes camera)\n      \u251c\u2500\u2192 Workspaces (loads workspace definitions)\n      \u2514\u2500\u2192 Redis Communication (RedisMessageBroker, RedisLabelManager)\n\n   Separate Process: vision_detect_segment\n      \u2514\u2500\u2192 VisualCortex (loads detection models)\n\n2. OBSERVATION\n   Robot.move2observation_pose(workspace_id)\n      \u2514\u2500\u2192 RobotController.move2observation_pose()\n          \u2514\u2500\u2192 Gripper moves to hover above workspace\n\n3. IMAGE CAPTURE &amp; STREAMING\n   FrameGrabber.get_current_frame()\n      \u251c\u2500\u2192 Capture image from camera\n      \u251c\u2500\u2192 Undistort image\n      \u251c\u2500\u2192 Extract workspace region\n      \u2514\u2500\u2192 RedisImageStreamer.publish_image()\n              \u2514\u2500\u2192 Redis: 'robot_camera' stream\n\n4. OBJECT DETECTION (SEPARATE PROCESS)\n   vision_detect_segment VisualCortex\n      \u251c\u2500\u2192 Read from Redis stream 'robot_camera'\n      \u251c\u2500\u2192 Run detection model (OWL-V2, YOLO-World, YOLOE, etc.)\n      \u251c\u2500\u2192 Optional: Track objects with persistent IDs\n      \u251c\u2500\u2192 Optional: Segment objects (SAM2, FastSAM)\n      \u2514\u2500\u2192 Publish results to Redis 'detected_objects' stream\n\n5. OBJECT RETRIEVAL\n   Environment.get_detected_objects()\n      \u251c\u2500\u2192 RedisMessageBroker.get_latest_objects()\n      \u251c\u2500\u2192 Read from Redis stream 'detected_objects'\n      \u251c\u2500\u2192 Deserialize JSON\n      \u2514\u2500\u2192 Convert to Object instances\n              \u251c\u2500\u2192 Calculate world coordinates\n              \u251c\u2500\u2192 Determine object orientation (for gripper rotation)\n              \u2514\u2500\u2192 Store in Objects collection\n\n6. PICK OPERATION\n   Robot.pick_object(label, coordinate)\n      \u251c\u2500\u2192 Find nearest object with label\n      \u251c\u2500\u2192 Text2Speech: \"Going to pick {label}\"\n      \u2514\u2500\u2192 RobotController.robot_pick_object(pose)\n              \u251c\u2500\u2192 Lock acquired\n              \u251c\u2500\u2192 Move to pre-grasp pose\n              \u251c\u2500\u2192 Move to grasp pose (z_offset applied)\n              \u251c\u2500\u2192 Close gripper\n              \u251c\u2500\u2192 Lift object\n              \u2514\u2500\u2192 Lock released\n\n7. PLACE OPERATION\n   Robot.place_object(coordinate, location)\n      \u251c\u2500\u2192 Calculate placement pose\n      \u2502   \u2514\u2500\u2192 Apply location offset (LEFT_NEXT_TO, ABOVE, etc.)\n      \u251c\u2500\u2192 Text2Speech: \"Going to place at {coordinate}\"\n      \u2514\u2500\u2192 RobotController.robot_place_object(pose)\n              \u251c\u2500\u2192 Lock acquired\n              \u251c\u2500\u2192 Move to pre-place pose\n              \u251c\u2500\u2192 Move to place pose (z_offset=0.005m)\n              \u251c\u2500\u2192 Open gripper\n              \u251c\u2500\u2192 Retract\n              \u2514\u2500\u2192 Lock released\n</code></pre>"},{"location":"architecture/#continuous-camera-update-loop","title":"Continuous Camera Update Loop","text":"<pre><code>Background Thread (daemon) in robot_environment:\n    Loop:\n        1. Move to observation pose (if not in motion)\n        2. Capture frame \u2192 Redis stream 'robot_camera'\n        3. Wait 0.1s\n        4. (vision_detect_segment detects in parallel)\n        5. Get detected objects from Redis 'detected_objects'\n        6. Update memory\n        7. Sleep (0.25s or 0.5s depending on robot motion)\n</code></pre> <p>Note: Object detection happens in a separate process (<code>vision_detect_segment</code>), which continuously monitors the <code>robot_camera</code> Redis stream and publishes results to <code>detected_objects</code>.</p>"},{"location":"architecture/#thread-safety","title":"Thread Safety","text":""},{"location":"architecture/#critical-sections","title":"Critical Sections","text":"<ol> <li>Robot Operations</li> <li>All <code>RobotController</code> methods use <code>self._lock</code></li> <li>Prevents concurrent hardware commands</li> <li> <p>Essential for Niryo robot (not thread-safe API)</p> </li> <li> <p>Camera Updates</p> </li> <li>Background thread for continuous updates</li> <li>Daemon thread (terminates with main program)</li> <li> <p>Uses <code>_stop_event</code> for clean shutdown</p> </li> <li> <p>Text-to-Speech</p> </li> <li>Asynchronous threads for non-blocking speech</li> <li>Thread returned for synchronization if needed</li> </ol>"},{"location":"architecture/#example-thread-safe-robot-access","title":"Example - Thread-Safe Robot Access","text":"<pre><code>def get_target_pose_from_rel(self, ws_id, u_rel, v_rel, yaw):\n    with self._lock:  # Thread-safe section\n        try:\n            obj_coords = self._robot_ctrl.get_target_pose_from_rel(\n                ws_id, 0.0, u_rel, v_rel, yaw\n            )\n        except (NiryoRobotException, UnicodeDecodeError) as e:\n            print(f\"Error: {e}\")\n            obj_coords = PoseObject(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n    return PoseObjectPNP.convert_niryo_pose_object2pose_object(obj_coords)\n</code></pre>"},{"location":"architecture/#external-package-integration","title":"External Package Integration","text":""},{"location":"architecture/#1-vision_detect_segment","title":"1. vision_detect_segment","text":"<p>Purpose: Object detection and segmentation (runs as separate process)</p> <p>Integration Points: - Reads images from Redis stream <code>robot_camera</code> (published by <code>FrameGrabber</code>) - Publishes detection results to Redis stream <code>detected_objects</code> - Subscribes to Redis channel <code>object_labels</code> for dynamic label configuration - Completely decoupled from <code>robot_environment</code> - communicates only via Redis</p> <p>How to Run: The vision system is configured and run independently as a separate process:</p> <pre><code># External process (vision_detect_segment)\nfrom vision_detect_segment import VisualCortex, get_default_config\n\nconfig = get_default_config(\"owlv2\")\ncortex = VisualCortex(\n    objdetect_model_id=\"owlv2\",  # or \"yolo-world\", \"yoloe-11l\", etc.\n    device=\"auto\",\n    verbose=True,\n    config=config\n)\n\n# Continuously detect from Redis\ncortex.detect_objects_from_redis(stream_name=\"robot_camera\")\n</code></pre> <p>Models Supported: - OWL-V2: Open-vocabulary detection - YOLO-World: Fast real-time detection - YOLOE-11L: Fast with built-in segmentation - Grounding-DINO: Text-guided detection</p> <p>Data Flow: 1. <code>robot_environment</code> publishes images \u2192 Redis stream <code>robot_camera</code> 2. <code>vision_detect_segment</code> reads from Redis \u2192 detects objects 3. <code>vision_detect_segment</code> publishes results \u2192 Redis stream <code>detected_objects</code> 4. <code>robot_environment</code> reads detections from Redis</p>"},{"location":"architecture/#2-redis_robot_comm","title":"2. redis_robot_comm","text":"<p>Purpose: Redis-based communication infrastructure</p> <p>Components Used: - <code>RedisImageStreamer</code> - Variable-size image streaming (used by <code>FrameGrabber</code>) - <code>RedisMessageBroker</code> - Object data consumption (used by <code>Environment</code>) - <code>RedisLabelManager</code> - Dynamic label configuration (used by <code>Environment</code>)</p> <p>Integration:</p> <pre><code># In FrameGrabber - publish images\nself.streamer = RedisImageStreamer(stream_name='robot_camera')\nself.streamer.publish_image(image, metadata={...}, compress_jpeg=True)\n\n# In Environment - consume detections\nself._object_broker = RedisMessageBroker()\nobjects_dict_list = self._object_broker.get_latest_objects(max_age_seconds=2.0)\n\n# In Environment - manage labels\nself._label_manager = RedisLabelManager()\nself._label_manager.add_label(\"new_object\")\nlabels = self._label_manager.get_latest_labels()\n</code></pre> <p>Data Streams: - <code>robot_camera</code> - Compressed images with metadata (published by <code>robot_environment</code>) - <code>detected_objects</code> - Object detection results (published by <code>vision_detect_segment</code>, consumed by <code>robot_environment</code>) - <code>object_labels</code> - Dynamic label configuration (published by <code>robot_environment</code> via <code>RedisLabelManager</code>)</p>"},{"location":"architecture/#3-text2speech","title":"3. text2speech","text":"<p>Purpose: Natural language feedback</p> <p>Integration:</p> <pre><code>self._oralcom = Text2Speech(el_api_key, verbose=verbose)\n\n# Asynchronous usage\nthread = self._oralcom.call_text2speech_async(\n    \"I have detected a pencil at position 0.25, 0.05\"\n)\nthread.join()  # Optional: wait for completion\n</code></pre> <p>TTS Engines: - ElevenLabs API (primary) - Kokoro TTS (local alternative)</p>"},{"location":"architecture/#4-pyniryo-pyniryo2","title":"4. pyniryo / pyniryo2","text":"<p>Purpose: Niryo robot hardware control</p> <p>Integration:</p> <pre><code># In NiryoRobotController\nself._robot_ctrl = NiryoRobot(robot_ip_address)\nself._robot_ctrl.calibrate_auto()\nself._robot_ctrl.update_tool()\n\n# Pick operation\nself._robot_ctrl.pick_from_pose(pick_pose)\n\n# Place operation\nself._robot_ctrl.place_from_pose(place_pose)\n</code></pre>"},{"location":"architecture/#coordinate-systems","title":"Coordinate Systems","text":""},{"location":"architecture/#three-coordinate-systems","title":"Three Coordinate Systems","text":"<ol> <li>Image Coordinates (Pixels)</li> <li>Origin: Top-left corner</li> <li>Units: Pixels</li> <li> <p>Range: <code>u \u2208 [0, width]</code>, <code>v \u2208 [0, height]</code></p> </li> <li> <p>Relative Coordinates</p> </li> <li>Origin: Top-left corner</li> <li>Units: Normalized [0, 1]</li> <li>Range: <code>u_rel, v_rel \u2208 [0, 1]</code></li> <li> <p>Used for workspace-independent calculations</p> </li> <li> <p>World Coordinates (Robot Base Frame)</p> </li> <li>Origin: Robot base</li> <li>Units: Meters</li> <li>Niryo axes:<ul> <li><code>x</code>: Forward (away from base)</li> <li><code>y</code>: Right (when facing robot)</li> <li><code>z</code>: Up</li> </ul> </li> </ol> <p>Camera Coordinate System:</p> <pre><code>  (u_min, v_min)\n         \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        \u2502            \u2502\n  v_rel \u2502            \u2502\n        \u2502            \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 (u_max, v_max)\n            \u2192 u_rel\n\n</code></pre> <p>World Coordinate System:</p> <pre><code>        Y (left)\n        \u2191\n        \u2502\n0.087 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Upper workspace boundary\n        \u2502\n    0 \u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  Center line (Y=0)\n        \u2502\n-0.087 \u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Lower workspace boundary\n        \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 X (forward)\n      0.163        0.337\n     (closer)     (farther)\n</code></pre>"},{"location":"architecture/#transformation-chain","title":"Transformation Chain","text":"<pre><code>Image (u, v)\n    \u2193 divide by image dimensions\nRelative (u_rel, v_rel)\n    \u2193 Workspace.transform_camera2world_coords()\n    \u2193 (uses Niryo's get_target_pose_from_rel())\nWorld (x, y, z) + orientation (roll, pitch, yaw)\n</code></pre>"},{"location":"architecture/#example-transformation","title":"Example Transformation","text":"<pre><code># Object at pixel (320, 240) in 640x480 image\nu, v = 320, 240\nu_rel = 320 / 640 = 0.5  # Center horizontally\nv_rel = 240 / 480 = 0.5  # Center vertically\n\n# Transform to world coordinates\npose = workspace.transform_camera2world_coords(\n    \"niryo_ws\", u_rel=0.5, v_rel=0.5, yaw=0.0\n)\n# Result: pose.x \u2248 0.25, pose.y \u2248 0.0, pose.z \u2248 0.01\n</code></pre>"},{"location":"architecture/#configuration","title":"Configuration","text":""},{"location":"architecture/#robot-selection","title":"Robot Selection","text":"<pre><code># Niryo Ned2 (real robot)\nenv = Environment(\n    el_api_key=\"key\",\n    use_simulation=False,\n    robot_id=\"niryo\"\n)\n\n# Niryo in Gazebo simulation\nenv = Environment(\n    el_api_key=\"key\",\n    use_simulation=True,\n    robot_id=\"niryo\"\n)\n</code></pre>"},{"location":"architecture/#workspace-configuration","title":"Workspace Configuration","text":"<p>Workspaces are defined in <code>niryo_workspace.py</code>:</p> <pre><code>def _set_observation_pose(self):\n    if self._id == \"niryo_ws\":\n        self._observation_pose = PoseObjectPNP(\n            x=0.173, y=-0.002, z=0.277,\n            roll=-3.042, pitch=1.327, yaw=-3.027\n        )\n    elif self._id == \"gazebo_1\":\n        self._observation_pose = PoseObjectPNP(\n            x=0.18, y=0, z=0.36,\n            roll=2.4, pitch=\u03c0/2, yaw=2.4\n        )\n</code></pre>"},{"location":"architecture/#error-handling","title":"Error Handling","text":""},{"location":"architecture/#robot-connection-issues","title":"Robot Connection Issues","text":"<pre><code>try:\n    success = robot.pick_place_object(\"cube\", [0.2, 0.0], [0.3, 0.0])\n    if not success:\n        print(\"Pick and place operation failed\")\nexcept (NiryoRobotException, UnicodeDecodeError) as e:\n    print(f\"Error: {e}\")\n    # Optionally: reset connection\n    robot_controller.reset_connection()\n</code></pre>"},{"location":"architecture/#object-not-found","title":"Object Not Found","text":"<pre><code>obj = objects.get_detected_object([0.2, 0.0], label=\"nonexistent\")\nif obj is None:\n    print(\"Object not found\")\n    # Handle missing object\n</code></pre>"},{"location":"architecture/#thread-cleanup","title":"Thread Cleanup","text":"<pre><code># Proper cleanup\nenv = Environment(...)\ntry:\n    # Use environment\n    pass\nfinally:\n    env.cleanup()  # Stops threads, closes connections\n</code></pre>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#detection-speed","title":"Detection Speed","text":"Model Detection Segmentation Total FPS YOLOE-11L 6-10ms Built-in 100-160 FPS YOLO-World 20-50ms 50-100ms (FastSAM) 10-25 FPS OWL-V2 100-200ms 200-500ms (SAM2) 1-3 FPS Grounding-DINO 200-400ms 200-500ms (SAM2) 1-2 FPS"},{"location":"architecture/#recommendations","title":"Recommendations","text":"<ul> <li>Real-time: Use YOLOE-11L or YOLO-World</li> <li>Accuracy: Use OWL-V2 or Grounding-DINO + SAM2</li> <li>Camera rate: 5-10 FPS sufficient for pick-and-place</li> </ul>"},{"location":"architecture/#memory-management","title":"Memory Management","text":"<ul> <li>Object memory stores detection history</li> <li>Background thread continuously updates</li> <li>Old detections persist until new scan</li> <li>Manual updates from pick/place operations override detections temporarily</li> </ul>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#adding-new-robot","title":"Adding New Robot","text":"<ol> <li>Create <code>MyRobotController(RobotController)</code></li> <li>Implement abstract methods</li> <li>Create <code>MyRobotWorkspace(Workspace)</code></li> <li>Add to <code>Robot.__init__()</code> selection</li> </ol>"},{"location":"architecture/#adding-new-workspace","title":"Adding New Workspace","text":"<ol> <li>Add ID to <code>NiryoWorkspace._set_observation_pose()</code></li> <li>Define observation pose</li> <li>No code changes needed elsewhere</li> </ol>"},{"location":"architecture/#custom-object-queries","title":"Custom Object Queries","text":"<pre><code>class MyObjects(Objects):\n    def get_objects_in_region(self, x_min, x_max, y_min, y_max):\n        return Objects(\n            obj for obj in self\n            if x_min &lt;= obj.x_com() &lt;= x_max\n            and y_min &lt;= obj.y_com() &lt;= y_max\n        )\n</code></pre>"},{"location":"architecture/#summary","title":"Summary","text":"<p>The <code>robot_environment</code> architecture provides:</p> <p>\u2705 Modular Design - Clear separation of concerns \u2705 Hardware Abstraction - Easy to add new robots \u2705 Thread Safety - Concurrent camera and control \u2705 External Integration - Clean package boundaries \u2705 Redis Communication - Decoupled data flow via separate processes \u2705 Flexible Workspaces - Multiple workspace support \u2705 Rich Object Representation - Full spatial information \u2705 Natural Interaction - Text-to-speech feedback</p> <p>This architecture enables robust pick-and-place operations with vision-based object detection while maintaining extensibility and clean code organization. The key innovation is the decoupled vision system running as a separate process, communicating via Redis streams, which allows for independent scaling and development of vision capabilities.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please ensure:</p> <ol> <li>Code follows the existing architecture patterns</li> <li>Thread safety is maintained</li> <li>Documentation is updated</li> <li>Type hints are included</li> <li>Tests pass successfully</li> </ol>"},{"location":"multi_workspace/","title":"Multi-Workspace Operations Guide","text":""},{"location":"multi_workspace/#overview","title":"Overview","text":"<p>The <code>robot_environment</code> package supports multi-workspace operations, allowing the robot to: - Detect objects in multiple workspaces simultaneously - Pick objects from one workspace and place them in another - Maintain separate object memory for each workspace - Coordinate movements between workspaces</p>"},{"location":"multi_workspace/#architecture","title":"Architecture","text":""},{"location":"multi_workspace/#workspace-memory-management","title":"Workspace Memory Management","text":"<p>Each workspace has its own object memory that tracks detected objects:</p> <pre><code># Per-workspace memory structure\n_workspace_memories: Dict[str, Objects] = {\n    \"niryo_ws_left\": Objects([...]),\n    \"niryo_ws_right\": Objects([...])\n}\n</code></pre>"},{"location":"multi_workspace/#workspace-configuration","title":"Workspace Configuration","text":"<p>Define multiple workspaces in <code>NiryoWorkspaces</code>:</p> <pre><code>from robot_workspace.workspaces.niryo_workspaces import NiryoWorkspaces\n\n# Initialize with multiple workspaces\nworkspaces = NiryoWorkspaces(environment, verbose=True)\n\n# Access individual workspaces\nleft_ws = workspaces.get_workspace_left()\nright_ws = workspaces.get_workspace_right()\n</code></pre>"},{"location":"multi_workspace/#basic-usage","title":"Basic Usage","text":""},{"location":"multi_workspace/#1-initialize-environment","title":"1. Initialize Environment","text":"<pre><code>from robot_environment.environment import Environment\n\nenv = Environment(\n    el_api_key=\"your_key\",\n    use_simulation=False,\n    robot_id=\"niryo\",\n    verbose=True\n)\n\n# Get workspace IDs\nleft_ws_id = env.workspaces().get_workspace_left_id()\nright_ws_id = env.workspaces().get_workspace_right_id()\n</code></pre>"},{"location":"multi_workspace/#2-observe-multiple-workspaces","title":"2. Observe Multiple Workspaces","text":"<pre><code># Observe left workspace\nenv.robot_move2observation_pose(left_ws_id)\nenv.set_current_workspace(left_ws_id)\ntime.sleep(2)  # Wait for detection\n\n# Observe right workspace\nenv.robot_move2observation_pose(right_ws_id)\nenv.set_current_workspace(right_ws_id)\ntime.sleep(2)  # Wait for detection\n</code></pre>"},{"location":"multi_workspace/#3-query-objects-by-workspace","title":"3. Query Objects by Workspace","text":"<pre><code># Get objects from specific workspace\nleft_objects = env.get_detected_objects_from_workspace(left_ws_id)\nright_objects = env.get_detected_objects_from_workspace(right_ws_id)\n\n# Get all objects from all workspaces\nall_objects = env.get_all_workspace_objects()\nfor ws_id, objects in all_objects.items():\n    print(f\"{ws_id}: {len(objects)} objects\")\n</code></pre>"},{"location":"multi_workspace/#4-transfer-objects-between-workspaces","title":"4. Transfer Objects Between Workspaces","text":"<pre><code>robot = env.robot()\n\n# Simple transfer\nrobot.pick_place_object_across_workspaces(\n    object_name='cube',\n    pick_workspace_id=left_ws_id,\n    pick_coordinate=[0.2, 0.05],\n    place_workspace_id=right_ws_id,\n    place_coordinate=[0.25, -0.05],\n    location=Location.NONE\n)\n</code></pre>"},{"location":"multi_workspace/#advanced-operations","title":"Advanced Operations","text":""},{"location":"multi_workspace/#organized-placement","title":"Organized Placement","text":"<p>Place objects in a grid pattern:</p> <pre><code>placement_grid = [\n    [0.20, -0.08], [0.25, -0.08], [0.30, -0.08],\n    [0.20, -0.04], [0.25, -0.04], [0.30, -0.04]\n]\n\nfor i, obj in enumerate(objects):\n    robot.pick_place_object_across_workspaces(\n        object_name=obj.label(),\n        pick_workspace_id=source_ws_id,\n        pick_coordinate=[obj.x_com(), obj.y_com()],\n        place_workspace_id=target_ws_id,\n        place_coordinate=placement_grid[i],\n        location=Location.NONE\n    )\n</code></pre>"},{"location":"multi_workspace/#object-sorting","title":"Object Sorting","text":"<p>Sort objects by type into different workspaces:</p> <pre><code>for obj in all_detected_objects:\n    if 'cube' in obj.label().lower():\n        target_ws = left_ws_id\n    elif 'cylinder' in obj.label().lower():\n        target_ws = right_ws_id\n    else:\n        continue\n\n    robot.pick_place_object_across_workspaces(\n        object_name=obj.label(),\n        pick_workspace_id=obj.workspace().id(),\n        pick_coordinate=[obj.x_com(), obj.y_com()],\n        place_workspace_id=target_ws,\n        place_coordinate=[0.25, 0.0],\n        location=Location.CLOSE_TO\n    )\n</code></pre>"},{"location":"multi_workspace/#find-free-space","title":"Find Free Space","text":"<p>Find the largest free space in a workspace:</p> <pre><code># Get largest free space in target workspace\nenv.robot_move2observation_pose(target_ws_id)\nenv.set_current_workspace(target_ws_id)\n\nlargest_area, center_x, center_y = env.get_largest_free_space_with_center()\nprint(f\"Free space: {largest_area*10000:.2f} cm\u00b2 at [{center_x:.2f}, {center_y:.2f}]\")\n\n# Place object at center of free space\nrobot.pick_place_object_across_workspaces(\n    object_name='box',\n    pick_workspace_id=source_ws_id,\n    pick_coordinate=[0.2, 0.0],\n    place_workspace_id=target_ws_id,\n    place_coordinate=[center_x, center_y],\n    location=Location.NONE\n)\n</code></pre>"},{"location":"multi_workspace/#memory-management","title":"Memory Management","text":""},{"location":"multi_workspace/#per-workspace-memory","title":"Per-Workspace Memory","text":"<p>Each workspace maintains its own object memory:</p> <pre><code># Clear specific workspace memory\nenv.clear_workspace_memory(workspace_id)\n\n# Remove object from workspace\nenv.remove_object_from_workspace(\n    workspace_id='niryo_ws_left',\n    object_label='cube',\n    coordinate=[0.2, 0.05]\n)\n\n# Update object position across workspaces\nenv.update_object_in_workspace(\n    source_workspace_id='niryo_ws_left',\n    target_workspace_id='niryo_ws_right',\n    object_label='cube',\n    old_coordinate=[0.2, 0.05],\n    new_coordinate=[0.25, -0.05]\n)\n</code></pre>"},{"location":"multi_workspace/#memory-synchronization","title":"Memory Synchronization","text":"<p>Memory is automatically updated during: - Object detection at observation pose - Pick operations (removes from source workspace) - Place operations (adds to target workspace) - Cross-workspace transfers (moves between workspace memories)</p>"},{"location":"multi_workspace/#workspace-coordinate-systems","title":"Workspace Coordinate Systems","text":"<p>Each workspace has its own coordinate system:</p> <pre><code>Left Workspace (niryo_ws_left):\n  - Origin: Upper-left corner\n  - X-axis: Forward (0.163 to 0.337 m)\n  - Y-axis: Right to Left (0.087 to -0.087 m)\n  - Observation pose: x=0.173, y=0.10, z=0.277\n\nRight Workspace (niryo_ws_right):\n  - Origin: Upper-left corner\n  - X-axis: Forward (0.163 to 0.337 m)\n  - Y-axis: Right to Left (0.087 to -0.087 m)\n  - Observation pose: x=0.173, y=-0.10, z=0.277\n</code></pre>"},{"location":"multi_workspace/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # Attempt transfer\n    success = robot.pick_place_object_across_workspaces(\n        object_name='cube',\n        pick_workspace_id=left_ws_id,\n        pick_coordinate=[0.2, 0.05],\n        place_workspace_id=right_ws_id,\n        place_coordinate=[0.25, -0.05],\n        location=Location.NONE\n    )\n\n    if not success:\n        print(\"Transfer failed\")\n        # Handle failure\n\nexcept Exception as e:\n    print(f\"Error during transfer: {e}\")\n    # Cleanup or recovery\n</code></pre>"},{"location":"multi_workspace/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always observe workspaces before operations:    <code>python    env.robot_move2observation_pose(workspace_id)    env.set_current_workspace(workspace_id)    time.sleep(2)  # Wait for detection</code></p> </li> <li> <p>Check for objects before picking:    <code>python    objects = env.get_detected_objects_from_workspace(workspace_id)    if len(objects) == 0:        print(\"No objects found\")        return</code></p> </li> <li> <p>Verify workspace IDs:    <code>python    workspace_ids = env.workspaces().get_workspace_ids()    if target_ws_id not in workspace_ids:        print(f\"Invalid workspace: {target_ws_id}\")        return</code></p> </li> <li> <p>Use appropriate wait times:    ```python    # After moving to observation pose    time.sleep(2)  # Allow detection to complete</p> </li> </ol> <p># Between transfers    time.sleep(0.5)  # Brief pause    ```</p>"},{"location":"multi_workspace/#examples","title":"Examples","text":"<p>See <code>examples/multi_workspace_example.py</code> for complete examples:</p> <pre><code>cd robot_environment/examples\npython multi_workspace_example.py\n</code></pre> <p>Available examples: 1. Simple Transfer: Move one object between workspaces 2. Organized Transfer: Arrange multiple objects in a grid 3. Object Sorting: Sort objects by type 4. Workspace Cleanup: Clear one workspace into another</p>"},{"location":"multi_workspace/#troubleshooting","title":"Troubleshooting","text":""},{"location":"multi_workspace/#objects-not-detected","title":"Objects not detected","text":"<pre><code># Ensure workspace is visible\nis_visible = env.is_any_workspace_visible()\nif not is_visible:\n    env.robot_move2observation_pose(workspace_id)\n\n# Clear and refresh memory\nenv.clear_workspace_memory(workspace_id)\nenv.set_current_workspace(workspace_id)\ntime.sleep(2)\n</code></pre>"},{"location":"multi_workspace/#transfer-failures","title":"Transfer failures","text":"<pre><code># Check object still exists\nobjects = env.get_detected_objects_from_workspace(source_ws_id)\nobj = objects.get_detected_object(coordinate, label)\nif obj is None:\n    print(\"Object no longer detected\")\n</code></pre>"},{"location":"multi_workspace/#memory-inconsistencies","title":"Memory inconsistencies","text":"<pre><code># Force memory refresh\nenv.clear_memory()  # Clear all workspaces\nenv.robot_move2observation_pose(workspace_id)\ntime.sleep(2)  # Re-detect all objects\n</code></pre>"},{"location":"multi_workspace/#performance-tips","title":"Performance Tips","text":"<ol> <li>Minimize workspace switches: Group operations by workspace</li> <li>Use memory queries: Avoid unnecessary observations</li> <li>Batch transfers: Transfer multiple objects in sequence</li> <li>Pre-calculate positions: Determine all target positions before starting</li> </ol>"},{"location":"multi_workspace/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Automatic workspace detection</li> <li>Collision avoidance between workspaces</li> <li>Parallel processing for multiple robots</li> <li>Dynamic workspace reconfiguration</li> </ul>"},{"location":"troubleshooting/","title":"Robot Environment - Troubleshooting","text":"<p>Common issues and solutions for the Robot Environment system.</p>"},{"location":"troubleshooting/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Object Detection Problems</li> <li>Robot Movement Issues</li> <li>Hardware Problems</li> <li>Frequently Asked Questions (FAQ)</li> <li>Getting Help</li> </ul>"},{"location":"troubleshooting/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"troubleshooting/#1-why-do-i-get-a-modulenotfounderror-no-module-named-text2speech","title":"1. Why do I get a <code>ModuleNotFoundError: No module named 'text2speech'</code>?","text":"<p>This usually means the <code>text2speech</code> package was not installed correctly or is not in your Python path. Ensure you ran:</p> <pre><code>pip install git+https://github.com/dgaida/text2speech.git\n</code></pre> <p>If you are running in a virtual environment, make sure it is activated.</p>"},{"location":"troubleshooting/#2-the-robot-moves-to-the-wrong-place-or-misses-the-object-what-should-i-do","title":"2. The robot moves to the wrong place or misses the object. What should I do?","text":"<p>First, check if the workspace is correctly calibrated. Use <code>env.get_workspace_by_id(\"your_ws_id\").get_bounds()</code> to see the world coordinates the system is using. Second, ensure you are using fresh detections. Always move to an observation pose and wait a second before calling <code>get_detected_objects()</code>.</p>"},{"location":"troubleshooting/#3-how-do-i-switch-between-simulation-and-real-robot","title":"3. How do I switch between simulation and real robot?","text":"<p>When initializing the <code>Environment</code> class, set <code>use_simulation=True</code> for Gazebo and <code>use_simulation=False</code> for the real hardware. Note that the real Niryo robot requires a specific IP address (default is <code>192.168.0.140</code>).</p>"},{"location":"troubleshooting/#4-can-i-use-this-without-a-gpu","title":"4. Can I use this without a GPU?","text":"<p>Yes! While object detection is faster on a GPU, models like <code>yolo-world</code> or <code>yoloe-11s</code> can run on a standard CPU with reasonable performance for pick-and-place tasks.</p>"},{"location":"troubleshooting/#5-why-is-the-camera-feed-delayed","title":"5. Why is the camera feed delayed?","text":"<p>The camera feed is streamed via Redis. If you notice a lag, it might be due to network congestion (if using a real robot over Wi-Fi) or high CPU usage by the vision models. Try a lighter model or reduce the camera update frequency in the configuration.</p>"},{"location":"troubleshooting/#object-detection-problems","title":"Object Detection Problems","text":""},{"location":"troubleshooting/#no-objects-detected","title":"No Objects Detected","text":"<p>Symptoms: - <code>get_detected_objects()</code> returns empty list - Camera shows black screen - \"No objects detected\" messages</p> <p>Solutions:</p> <ol> <li>Verify camera is working:</li> </ol> <pre><code>from redis_robot_comm import RedisImageStreamer\nstreamer = RedisImageStreamer(stream_name=\"robot_camera\")\nimg, metadata = streamer.get_latest_image()\nprint(f\"Image shape: {img.shape}\")  # Should be (480, 640, 3)\n</code></pre> <ol> <li>Check Redis is running:</li> </ol> <pre><code>docker ps | grep redis\n\n# If not running:\ndocker run -p 6379:6379 redis:alpine\n</code></pre> <ol> <li>Verify camera thread is started:</li> </ol> <pre><code># In server initialization\nenv = Environment(\n    ...\n    start_camera_thread=True  # Must be True!\n)\n</code></pre> <ol> <li>Check lighting conditions:</li> <li>Ensure workspace is well-lit</li> <li>Avoid shadows and glare</li> <li> <p>Use consistent lighting</p> </li> <li> <p>Verify object labels:</p> </li> </ol> <pre><code>labels = env.get_object_labels_as_string()\nprint(f\"Recognizable objects: {labels}\")\n\n# Add custom labels if needed\nenv.add_object_name2object_labels(\"your_object\")\n</code></pre>"},{"location":"troubleshooting/#objects-detected-at-wrong-positions","title":"Objects Detected at Wrong Positions","text":"<p>Symptoms: - Robot misses objects when picking - Coordinates don't match visual position - Objects appear shifted in camera view</p> <p>Solutions:</p> <ol> <li>Recalibrate camera transformation:</li> </ol> <pre><code># Check workspace calibration\nworkspace = env.get_workspace_by_id(\"niryo_ws\")\nprint(f\"Workspace bounds: {workspace.get_bounds()}\")\n\n# Verify transformation parameters\n# May need to recalibrate camera-to-world transform\n</code></pre> <ol> <li>Check workspace is level:</li> <li>Ensure robot base is stable</li> <li>Workspace surface should be flat</li> <li> <p>Check for tilting or movement</p> </li> <li> <p>Update detection immediately before pick:</p> </li> </ol> <pre><code># \u2705 Good: Fresh detection\nobjects = get_detected_objects()\nobj = objects[0]\npick_object(obj['label'], [obj['x'], obj['y']])\n\n# \u274c Bad: Stale coordinates\npick_object(\"pencil\", [0.15, -0.05])  # May have moved!\n</code></pre> <ol> <li>Verify coordinate system understanding:</li> </ol> <pre><code>Niryo workspace (top view):\n    Y-axis \u2192\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\nX \u2193 \u2502 Center  \u2502\n    \u2502  (0,0)  \u2502\n    \u2502         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"troubleshooting/#detection-is-too-slow","title":"Detection is Too Slow","text":"<p>Symptoms: - Long delays before robot responds - Camera updates lag behind - Low FPS (&lt; 1 frame/second)</p> <p>Solutions:</p> <ol> <li>Use faster detection model:</li> </ol> <pre><code># In Environment initialization\nvisual_cortex = VisualCortex(\n    objdetect_model_id=\"yoloworld\",  # Faster than owlv2\n    device=\"cuda\"  # Use GPU if available\n)\n</code></pre> <ol> <li>Reduce camera update rate:</li> </ol> <pre><code># In camera thread\ntime.sleep(0.5)  # Update every 0.5s instead of 0.1s\n</code></pre> <ol> <li>Check GPU availability:</li> </ol> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# If no GPU:\n# - Use CPU with yoloworld model\n# - Or add GPU to system\n</code></pre> <ol> <li>Optimize detection parameters:</li> </ol> <pre><code>config = {\n    'confidence_threshold': 0.20,  # Higher = fewer false positives\n    'iou_threshold': 0.5,\n    'max_detections': 50  # Lower = faster\n}\n</code></pre>"},{"location":"troubleshooting/#robot-movement-issues","title":"Robot Movement Issues","text":""},{"location":"troubleshooting/#robot-wont-move","title":"Robot Won't Move","text":"<p>Symptoms: - Commands accepted but no movement - Robot stays in same position - \"Movement failed\" errors</p> <p>Solutions:</p> <ol> <li>Check robot connection:</li> </ol> <pre><code># For Niryo\nrobot = env.robot()\nstatus = robot.robot_ctrl().get_hardware_status()\nprint(f\"Robot connected: {status}\")\n</code></pre> <ol> <li>Verify simulation vs. real mode:</li> </ol> <pre><code># TODO\n\n# --no-simulation flag for real robot\n# Without flag = simulation mode\n</code></pre> <ol> <li>Check robot power and calibration:</li> <li>Ensure robot is powered on</li> <li>Run calibration routine if needed</li> <li> <p>Check for error LEDs on robot</p> </li> <li> <p>Verify coordinates are reachable:</p> </li> </ol> <pre><code># Check workspace bounds\nupper_left = get_workspace_coordinate_from_point(\"niryo_ws\", \"upper left corner\")\nlower_right = get_workspace_coordinate_from_point(\"niryo_ws\", \"lower right corner\")\n\nprint(f\"Valid X range: [{lower_right[0]}, {upper_left[0]}]\")\nprint(f\"Valid Y range: [{lower_right[1]}, {upper_left[1]}]\")\n\n# Niryo: X=[0.163, 0.337], Y=[-0.087, 0.087]\n</code></pre>"},{"location":"troubleshooting/#collision-detection-triggered","title":"Collision Detection Triggered","text":"<p>Symptoms: - Robot stops suddenly - \"Collision detected\" messages - Robot needs reset before continuing</p> <p>Solutions:</p> <ol> <li>Clear collision flag:</li> </ol> <pre><code>clear_collision_detected()\n</code></pre> <ol> <li>Check workspace for obstacles:</li> <li>Remove objects outside workspace</li> <li>Ensure cables aren't blocking movement</li> <li> <p>Check gripper clearance</p> </li> <li> <p>Adjust movement parameters:</p> </li> </ol> <pre><code># In robot controller (if accessible)\nrobot_ctrl.set_collision_threshold(higher_value)\n</code></pre> <ol> <li>Move to safe observation pose:</li> </ol> <pre><code>move2observation_pose(\"niryo_ws\")\nclear_collision_detected()\n</code></pre>"},{"location":"troubleshooting/#gripper-problems","title":"Gripper Problems","text":"<p>Symptoms: - Objects slip out of gripper - Gripper doesn't close/open - \"Failed to grasp\" errors</p> <p>Solutions:</p> <ol> <li>Check object size:</li> </ol> <pre><code>obj = get_detected_object([x, y])\nif obj['width_m'] &gt; 0.05:\n    print(\"Object too large for gripper!\")\n    # Use push_object() instead\n</code></pre> <ol> <li>Verify gripper calibration:</li> </ol> <pre><code># Test gripper\nrobot.robot_ctrl().open_gripper()\ntime.sleep(2)\nrobot.robot_ctrl().close_gripper()\n</code></pre> <ol> <li>Check object graspability:</li> <li>Objects should have flat surfaces</li> <li>Avoid round or irregular shapes</li> <li> <p>Ensure objects aren't too heavy (&lt; 500g)</p> </li> <li> <p>Adjust grasp approach angle:</p> </li> </ol> <pre><code># Object rotation affects grasp success\nobj = get_detected_object([x, y])\nprint(f\"Object rotation: {obj['rotation_rad']} rad\")\n\n# Robot adjusts approach automatically\n</code></pre>"},{"location":"troubleshooting/#hardware-problems","title":"Hardware Problems","text":""},{"location":"troubleshooting/#niryo-robot-specific","title":"Niryo Robot Specific","text":"<p>Issue: Robot not responding</p> <pre><code># Check Niryo connection\nping &lt;robot_ip&gt;\n\n# Default: 192.168.1.xxx\n</code></pre> <p>Issue: Calibration needed</p> <pre><code># Run calibration\nrobot.robot_ctrl().calibrate()\n</code></pre> <p>Issue: Learning mode activated - Manually disable learning mode on robot - Robot will be stiff when learning mode is off</p>"},{"location":"troubleshooting/#widowx-robot-specific","title":"WidowX Robot Specific","text":"<p>Issue: Joint limits</p> <pre><code># WidowX has different workspace\n# Adjust coordinates accordingly\n</code></pre> <p>Issue: Power supply - Ensure adequate power (12V) - Check for voltage drops during operation</p>"},{"location":"troubleshooting/#camera-issues","title":"Camera Issues","text":"<p>Issue: Poor image quality</p> <pre><code># Adjust camera settings\ncamera.set(cv2.CAP_PROP_EXPOSURE, -7)\ncamera.set(cv2.CAP_PROP_BRIGHTNESS, 130)\n</code></pre> <p>Issue: Wrong camera selected</p> <pre><code># List available cameras\nfor i in range(4):\n    cap = cv2.VideoCapture(i)\n    if cap.isOpened():\n        print(f\"Camera {i} available\")\n    cap.release()\n</code></pre>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"troubleshooting/#resources","title":"Resources","text":"<ul> <li>GitHub Issues: https://github.com/dgaida/robot_environment/issues</li> <li>Documentation: README.md</li> </ul>"},{"location":"troubleshooting/#quick-diagnostic-checklist","title":"Quick Diagnostic Checklist","text":"<p>Before opening an issue, check:</p> <ul> <li>[ ] Redis is running</li> <li>[ ] Robot is powered on (if using real robot)</li> <li>[ ] Camera is working (check Redis stream)</li> <li>[ ] Object detection is running (check for detections)</li> <li>[ ] Coordinates are within workspace bounds</li> <li>[ ] Object names match detected labels exactly</li> <li>[ ] All dependencies are installed</li> <li>[ ] Log files checked for errors</li> </ul> <p>If all checked and still having issues, please open a GitHub issue with the information above!</p>"},{"location":"api/camera/","title":"Camera","text":""},{"location":"api/camera/#framegrabber","title":"FrameGrabber","text":""},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber","title":"<code>robot_environment.camera.framegrabber.FrameGrabber</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract class that provides the abstract method get_current_frame() to be implemented by classes inheriting from this one. The FrameGrabber grabs frames from the camera and provides them.</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>class FrameGrabber(ABC):\n    \"\"\"\n    An abstract class that provides the abstract method get_current_frame() to be implemented by classes\n    inheriting from this one. The FrameGrabber grabs frames from the camera and provides them.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(self, environment: \"Environment\", verbose: bool = False):\n        \"\"\"\n\n        Args:\n            environment: Environment object this FrameGrabber is installed in\n            verbose:\n        \"\"\"\n        self._current_frame = None\n        self._environment = environment\n        self._verbose = verbose\n\n    # *** PUBLIC GET methods ***\n\n    def get_current_frame_shape(self) -&gt; tuple[int, ...]:\n        \"\"\"\n\n        Returns:\n\n        \"\"\"\n        return self._current_frame.shape\n\n    def get_current_frame_width_height(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Returns width and height of current frame in pixels.\n\n        Returns:\n            width and height of current frame in pixels.\n        \"\"\"\n        return self._current_frame.shape[0], self._current_frame.shape[1]\n\n    # *** PUBLIC methods ***\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    @abstractmethod\n    def get_current_frame(self) -&gt; np.ndarray:\n        \"\"\"\n        Captures an image of the robot's workspace, ensuring proper undistortion in RGB.\n\n        Returns:\n            numpy.ndarray: Raw image captured from the robot's camera.\n        \"\"\"\n        return self._current_frame\n\n    # *** PRIVATE methods ***\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    # *** PUBLIC properties ***\n\n    def current_frame(self) -&gt; np.ndarray:\n        \"\"\"\n        Returns current frame.\n\n        Returns:\n            numpy.ndarray: Current frame.\n        \"\"\"\n        return self._current_frame\n\n    def environment(self) -&gt; \"Environment\":\n        return self._environment\n\n    def verbose(self) -&gt; bool:\n        \"\"\"\n\n        Returns: True, if verbose is on, else False\n\n        \"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    # current frame.\n    _current_frame = None\n\n    _environment = None\n\n    _verbose = False\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber-functions","title":"Functions","text":""},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.__init__","title":"<code>__init__(environment, verbose=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>Environment object this FrameGrabber is installed in</p> required <code>verbose</code> <code>bool</code> <code>False</code> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>def __init__(self, environment: \"Environment\", verbose: bool = False):\n    \"\"\"\n\n    Args:\n        environment: Environment object this FrameGrabber is installed in\n        verbose:\n    \"\"\"\n    self._current_frame = None\n    self._environment = environment\n    self._verbose = verbose\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.current_frame","title":"<code>current_frame()</code>","text":"<p>Returns current frame.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Current frame.</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>def current_frame(self) -&gt; np.ndarray:\n    \"\"\"\n    Returns current frame.\n\n    Returns:\n        numpy.ndarray: Current frame.\n    \"\"\"\n    return self._current_frame\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.get_current_frame","title":"<code>get_current_frame()</code>  <code>abstractmethod</code>","text":"<p>Captures an image of the robot's workspace, ensuring proper undistortion in RGB.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Raw image captured from the robot's camera.</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>@abstractmethod\ndef get_current_frame(self) -&gt; np.ndarray:\n    \"\"\"\n    Captures an image of the robot's workspace, ensuring proper undistortion in RGB.\n\n    Returns:\n        numpy.ndarray: Raw image captured from the robot's camera.\n    \"\"\"\n    return self._current_frame\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.get_current_frame_shape","title":"<code>get_current_frame_shape()</code>","text":"<p>Returns:</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>def get_current_frame_shape(self) -&gt; tuple[int, ...]:\n    \"\"\"\n\n    Returns:\n\n    \"\"\"\n    return self._current_frame.shape\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.get_current_frame_width_height","title":"<code>get_current_frame_width_height()</code>","text":"<p>Returns width and height of current frame in pixels.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>width and height of current frame in pixels.</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>def get_current_frame_width_height(self) -&gt; tuple[int, int]:\n    \"\"\"\n    Returns width and height of current frame in pixels.\n\n    Returns:\n        width and height of current frame in pixels.\n    \"\"\"\n    return self._current_frame.shape[0], self._current_frame.shape[1]\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.framegrabber.FrameGrabber.verbose","title":"<code>verbose()</code>","text":"<p>Returns: True, if verbose is on, else False</p> Source code in <code>robot_environment/camera/framegrabber.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"\n\n    Returns: True, if verbose is on, else False\n\n    \"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/camera/#niryoframegrabber","title":"NiryoFrameGrabber","text":""},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber","title":"<code>robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber</code>","text":"<p>               Bases: <code>FrameGrabber</code></p> <p>A class implementing a framegrabber for Niryo Ned2 robot arm</p> Source code in <code>robot_environment/camera/niryo_framegrabber.py</code> <pre><code>class NiryoFrameGrabber(FrameGrabber):\n    \"\"\"\n    A class implementing a framegrabber for Niryo Ned2 robot arm\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(self, environment: \"Environment\", stream_name=\"robot_camera\", verbose: bool = False):\n        \"\"\"\n        Args:\n            environment: Environment object this FrameGrabber is installed in\n            verbose:\n\n        Returns:\n            object:\n\n        Raises:\n            TypeError: robot must be an instance of NiryoRobotController.\n        \"\"\"\n        super().__init__(environment, verbose)\n\n        self._logger = get_package_logger(__name__, verbose)\n\n        robot = environment.get_robot_controller()\n\n        if not isinstance(robot, NiryoRobotController):\n            raise TypeError(\"robot must be an instance of NiryoRobotController.\")\n\n        self._robot = robot\n\n        self._mtx, self._dist = self._robot.get_camera_intrinsics()\n\n        self.streamer = RedisImageStreamer(stream_name=stream_name)\n        self.frame_counter = 0\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    # @log_start_end_cls()\n    def get_current_frame(self) -&gt; np.ndarray:\n        \"\"\"\n        Captures an image of the robot's workspace, ensuring proper undistortion in BGR.\n\n        Returns:\n            numpy.ndarray: Raw image captured from the robot's camera.\n        \"\"\"\n        try:\n            img_compressed = self._robot.get_img_compressed()\n        except UnicodeDecodeError as e:\n            self._logger.error(f\"Error getting compressed image: {e}\", exc_info=True)\n            return self._current_frame\n\n        img_raw = uncompress_image(img_compressed)\n        img = undistort_image(img_raw, self._mtx, self._dist)\n\n        img_work = extract_img_workspace(img, workspace_ratio=1)\n\n        if img_work is not None:\n            gripper_pose = self._robot.get_pose()\n\n            # TODO: try to get transformation between camera and gripper\n            camera_pose = gripper_pose\n\n            self._logger.debug(f\"camera_pose: {camera_pose}\")\n\n            current_frame = img_work\n            myworkspace = self._environment.get_visible_workspace(camera_pose)\n\n            if myworkspace is not None:\n                myworkspace.set_img_shape(img_work.shape)\n            else:\n                self._logger.debug(f\"No visible workspace: {myworkspace}\")\n        else:\n            current_frame = img\n\n        self._current_frame = current_frame\n\n        # TODO: ben\u00f6tige ich die Methode? f\u00fcr was? falls ja, wo muss die aufgerufen werden? funktioniert hat die noch\n        #  nicht. sollte in Methode get_visible_workspace() auf den Mittelpunkt des workspaces angewandt werden\n        # is_visible = self.is_point_visible(np.array([0.24, 0.01, 0.001]))\n        # print(is_visible)\n\n        # cv2.imshow(\"Camera View\", self._current_frame)\n        # Break the loop if ESC key is pressed\n        # cv2.waitKey(0)\n\n        self.publish_workspace_image(self._current_frame, \"id\")\n\n        return self._current_frame\n\n    def publish_workspace_image(self, image: np.ndarray, workspace_id: str, robot_pose: Dict[str, float] = None):\n        \"\"\"\n        Publish workspace image with robot context\n        Image size can vary based on workspace and robot position\n        \"\"\"\n        metadata = {\n            \"workspace_id\": workspace_id,\n            \"frame_id\": self.frame_counter,\n            \"robot_pose\": robot_pose or {},\n            \"image_source\": \"robot_mounted_camera\",\n        }\n\n        stream_id = self.streamer.publish_image(image, metadata=metadata, compress_jpeg=True, quality=85)\n\n        self.frame_counter += 1\n        return stream_id\n\n    def is_point_visible(self, world_point: np.array, camera_to_gripper_transform=np.eye(4)) -&gt; bool:\n        \"\"\"\n        Determines whether a given world point is visible in the camera's field of view.\n\n        Args:\n            world_point (np.array): A 3D point in world coordinates as a NumPy array of shape (3,).\n            camera_to_gripper_transform (np.array): A 4x4 transformation matrix that defines\n                the relationship between the camera and the gripper frame. Defaults to the identity matrix.\n\n        Returns:\n            bool: True if the point is visible in the camera's view, False otherwise.\n\n        Explanation:\n            1. Transforms the `world_point` from world coordinates to the gripper frame using the\n               gripper's pose transformation matrix.\n            2. Transforms the point to the camera frame using the provided camera-to-gripper transformation.\n            3. Projects the point onto the image plane using the camera's intrinsic matrix.\n            4. Checks if the projected point lies within the camera's image bounds (640x480 pixels).\n            5. Optionally undistorts the projected point to account for lens distortion.\n\n        Notes:\n            - This method assumes the camera intrinsic matrix (`self._mtx`) and distortion coefficients (`self._dist`)\n              are precomputed and valid for a resolution of 640x480 pixels.\n            - Points behind the camera (with z &lt;= 0 in the camera frame) are not visible.\n        \"\"\"\n        # print(self._mtx, self._dist)\n        # Unpack camera intrinsics\n        K = np.array(self._mtx)\n        distortion = np.array(self._dist)\n\n        # img_width, img_height = super().get_current_frame_width_height()\n        # ich muss hier mit 640 x 480 pixel arbeiten, da camera intrinsics nur daf\u00fcr gilt: uo, vo\n        img_width, img_height = 640, 480\n\n        gripper_pose = self._robot.get_pose()\n        gripper_pose = PoseObjectPNP.convert_niryo_pose_object2pose_object(gripper_pose)\n\n        # Transform the world point to the gripper frame\n        world_to_gripper_transform = gripper_pose.to_transformation_matrix()\n        # Convert `world_point` to homogeneous coordinates by appending a 1\n        world_point_homogeneous = np.append(world_point, 1)\n\n        # print(world_to_gripper_transform)\n        # print(gripper_pose)\n\n        # Transform `world_point` to the gripper frame\n        # gripper_frame_point = np.linalg.inv(world_to_gripper_transform) @ world_point_homogeneous\n        gripper_frame_point = np.dot(world_to_gripper_transform, world_point_homogeneous)\n\n        # Transform the point to the camera frame\n        camera_point = np.dot(camera_to_gripper_transform, gripper_frame_point)\n\n        # print(camera_point)\n\n        # Check if the point is in front of the camera\n        if camera_point[2] &lt;= 0:\n            return False\n\n        # Project the point to the image plane\n        pixel_coords = np.dot(K, camera_point[:3] / camera_point[2])\n\n        # Normalize homogeneous coordinates\n        u, v = pixel_coords[:2] / pixel_coords[2]\n\n        # print(u, v)\n\n        # Check if the point is within the image bounds\n        if 0 &lt;= u &lt; img_width and 0 &lt;= v &lt; img_height:\n            # Optional: Account for lens distortion\n            undistorted_points = cv2.undistortPoints(np.array([[u, v]], dtype=np.float32), K, distortion)\n            u, v = undistorted_points[0][0]\n            return 0 &lt;= u &lt; img_width and 0 &lt;= v &lt; img_height\n\n        return False\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    # *** PRIVATE STATIC/CLASS methods ***\n\n    # *** PUBLIC properties ***\n\n    def camera_matrix(self):\n        return self._mtx\n\n    def camera_dist_coeff(self):\n        return self._dist\n\n    # *** PRIVATE variables ***\n\n    # NiryoRobotController object\n    _robot = None\n\n    # camera intrinsic transformation matrix\n    _mtx = None\n\n    # camera distortion coefficients\n    _dist = None\n    _logger = None\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber-functions","title":"Functions","text":""},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber.__init__","title":"<code>__init__(environment, stream_name='robot_camera', verbose=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>environment</code> <code>Environment</code> <p>Environment object this FrameGrabber is installed in</p> required <code>verbose</code> <code>bool</code> <code>False</code> <p>Returns:</p> Name Type Description <code>object</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>robot must be an instance of NiryoRobotController.</p> Source code in <code>robot_environment/camera/niryo_framegrabber.py</code> <pre><code>@log_start_end_cls()\ndef __init__(self, environment: \"Environment\", stream_name=\"robot_camera\", verbose: bool = False):\n    \"\"\"\n    Args:\n        environment: Environment object this FrameGrabber is installed in\n        verbose:\n\n    Returns:\n        object:\n\n    Raises:\n        TypeError: robot must be an instance of NiryoRobotController.\n    \"\"\"\n    super().__init__(environment, verbose)\n\n    self._logger = get_package_logger(__name__, verbose)\n\n    robot = environment.get_robot_controller()\n\n    if not isinstance(robot, NiryoRobotController):\n        raise TypeError(\"robot must be an instance of NiryoRobotController.\")\n\n    self._robot = robot\n\n    self._mtx, self._dist = self._robot.get_camera_intrinsics()\n\n    self.streamer = RedisImageStreamer(stream_name=stream_name)\n    self.frame_counter = 0\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber.get_current_frame","title":"<code>get_current_frame()</code>","text":"<p>Captures an image of the robot's workspace, ensuring proper undistortion in BGR.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Raw image captured from the robot's camera.</p> Source code in <code>robot_environment/camera/niryo_framegrabber.py</code> <pre><code>def get_current_frame(self) -&gt; np.ndarray:\n    \"\"\"\n    Captures an image of the robot's workspace, ensuring proper undistortion in BGR.\n\n    Returns:\n        numpy.ndarray: Raw image captured from the robot's camera.\n    \"\"\"\n    try:\n        img_compressed = self._robot.get_img_compressed()\n    except UnicodeDecodeError as e:\n        self._logger.error(f\"Error getting compressed image: {e}\", exc_info=True)\n        return self._current_frame\n\n    img_raw = uncompress_image(img_compressed)\n    img = undistort_image(img_raw, self._mtx, self._dist)\n\n    img_work = extract_img_workspace(img, workspace_ratio=1)\n\n    if img_work is not None:\n        gripper_pose = self._robot.get_pose()\n\n        # TODO: try to get transformation between camera and gripper\n        camera_pose = gripper_pose\n\n        self._logger.debug(f\"camera_pose: {camera_pose}\")\n\n        current_frame = img_work\n        myworkspace = self._environment.get_visible_workspace(camera_pose)\n\n        if myworkspace is not None:\n            myworkspace.set_img_shape(img_work.shape)\n        else:\n            self._logger.debug(f\"No visible workspace: {myworkspace}\")\n    else:\n        current_frame = img\n\n    self._current_frame = current_frame\n\n    # TODO: ben\u00f6tige ich die Methode? f\u00fcr was? falls ja, wo muss die aufgerufen werden? funktioniert hat die noch\n    #  nicht. sollte in Methode get_visible_workspace() auf den Mittelpunkt des workspaces angewandt werden\n    # is_visible = self.is_point_visible(np.array([0.24, 0.01, 0.001]))\n    # print(is_visible)\n\n    # cv2.imshow(\"Camera View\", self._current_frame)\n    # Break the loop if ESC key is pressed\n    # cv2.waitKey(0)\n\n    self.publish_workspace_image(self._current_frame, \"id\")\n\n    return self._current_frame\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber.is_point_visible","title":"<code>is_point_visible(world_point, camera_to_gripper_transform=np.eye(4))</code>","text":"<p>Determines whether a given world point is visible in the camera's field of view.</p> <p>Parameters:</p> Name Type Description Default <code>world_point</code> <code>array</code> <p>A 3D point in world coordinates as a NumPy array of shape (3,).</p> required <code>camera_to_gripper_transform</code> <code>array</code> <p>A 4x4 transformation matrix that defines the relationship between the camera and the gripper frame. Defaults to the identity matrix.</p> <code>eye(4)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the point is visible in the camera's view, False otherwise.</p> Explanation <ol> <li>Transforms the <code>world_point</code> from world coordinates to the gripper frame using the    gripper's pose transformation matrix.</li> <li>Transforms the point to the camera frame using the provided camera-to-gripper transformation.</li> <li>Projects the point onto the image plane using the camera's intrinsic matrix.</li> <li>Checks if the projected point lies within the camera's image bounds (640x480 pixels).</li> <li>Optionally undistorts the projected point to account for lens distortion.</li> </ol> Notes <ul> <li>This method assumes the camera intrinsic matrix (<code>self._mtx</code>) and distortion coefficients (<code>self._dist</code>)   are precomputed and valid for a resolution of 640x480 pixels.</li> <li>Points behind the camera (with z &lt;= 0 in the camera frame) are not visible.</li> </ul> Source code in <code>robot_environment/camera/niryo_framegrabber.py</code> <pre><code>def is_point_visible(self, world_point: np.array, camera_to_gripper_transform=np.eye(4)) -&gt; bool:\n    \"\"\"\n    Determines whether a given world point is visible in the camera's field of view.\n\n    Args:\n        world_point (np.array): A 3D point in world coordinates as a NumPy array of shape (3,).\n        camera_to_gripper_transform (np.array): A 4x4 transformation matrix that defines\n            the relationship between the camera and the gripper frame. Defaults to the identity matrix.\n\n    Returns:\n        bool: True if the point is visible in the camera's view, False otherwise.\n\n    Explanation:\n        1. Transforms the `world_point` from world coordinates to the gripper frame using the\n           gripper's pose transformation matrix.\n        2. Transforms the point to the camera frame using the provided camera-to-gripper transformation.\n        3. Projects the point onto the image plane using the camera's intrinsic matrix.\n        4. Checks if the projected point lies within the camera's image bounds (640x480 pixels).\n        5. Optionally undistorts the projected point to account for lens distortion.\n\n    Notes:\n        - This method assumes the camera intrinsic matrix (`self._mtx`) and distortion coefficients (`self._dist`)\n          are precomputed and valid for a resolution of 640x480 pixels.\n        - Points behind the camera (with z &lt;= 0 in the camera frame) are not visible.\n    \"\"\"\n    # print(self._mtx, self._dist)\n    # Unpack camera intrinsics\n    K = np.array(self._mtx)\n    distortion = np.array(self._dist)\n\n    # img_width, img_height = super().get_current_frame_width_height()\n    # ich muss hier mit 640 x 480 pixel arbeiten, da camera intrinsics nur daf\u00fcr gilt: uo, vo\n    img_width, img_height = 640, 480\n\n    gripper_pose = self._robot.get_pose()\n    gripper_pose = PoseObjectPNP.convert_niryo_pose_object2pose_object(gripper_pose)\n\n    # Transform the world point to the gripper frame\n    world_to_gripper_transform = gripper_pose.to_transformation_matrix()\n    # Convert `world_point` to homogeneous coordinates by appending a 1\n    world_point_homogeneous = np.append(world_point, 1)\n\n    # print(world_to_gripper_transform)\n    # print(gripper_pose)\n\n    # Transform `world_point` to the gripper frame\n    # gripper_frame_point = np.linalg.inv(world_to_gripper_transform) @ world_point_homogeneous\n    gripper_frame_point = np.dot(world_to_gripper_transform, world_point_homogeneous)\n\n    # Transform the point to the camera frame\n    camera_point = np.dot(camera_to_gripper_transform, gripper_frame_point)\n\n    # print(camera_point)\n\n    # Check if the point is in front of the camera\n    if camera_point[2] &lt;= 0:\n        return False\n\n    # Project the point to the image plane\n    pixel_coords = np.dot(K, camera_point[:3] / camera_point[2])\n\n    # Normalize homogeneous coordinates\n    u, v = pixel_coords[:2] / pixel_coords[2]\n\n    # print(u, v)\n\n    # Check if the point is within the image bounds\n    if 0 &lt;= u &lt; img_width and 0 &lt;= v &lt; img_height:\n        # Optional: Account for lens distortion\n        undistorted_points = cv2.undistortPoints(np.array([[u, v]], dtype=np.float32), K, distortion)\n        u, v = undistorted_points[0][0]\n        return 0 &lt;= u &lt; img_width and 0 &lt;= v &lt; img_height\n\n    return False\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.niryo_framegrabber.NiryoFrameGrabber.publish_workspace_image","title":"<code>publish_workspace_image(image, workspace_id, robot_pose=None)</code>","text":"<p>Publish workspace image with robot context Image size can vary based on workspace and robot position</p> Source code in <code>robot_environment/camera/niryo_framegrabber.py</code> <pre><code>def publish_workspace_image(self, image: np.ndarray, workspace_id: str, robot_pose: Dict[str, float] = None):\n    \"\"\"\n    Publish workspace image with robot context\n    Image size can vary based on workspace and robot position\n    \"\"\"\n    metadata = {\n        \"workspace_id\": workspace_id,\n        \"frame_id\": self.frame_counter,\n        \"robot_pose\": robot_pose or {},\n        \"image_source\": \"robot_mounted_camera\",\n    }\n\n    stream_id = self.streamer.publish_image(image, metadata=metadata, compress_jpeg=True, quality=85)\n\n    self.frame_counter += 1\n    return stream_id\n</code></pre>"},{"location":"api/camera/#widowxframegrabber","title":"WidowXFrameGrabber","text":""},{"location":"api/camera/#robot_environment.camera.widowx_framegrabber.WidowXFrameGrabber","title":"<code>robot_environment.camera.widowx_framegrabber.WidowXFrameGrabber</code>","text":"<p>               Bases: <code>FrameGrabber</code></p> <p>A class implementing a framegrabber for WidowX robot arm - here Intel RealSense camera as third person camera</p> Source code in <code>robot_environment/camera/widowx_framegrabber.py</code> <pre><code>class WidowXFrameGrabber(FrameGrabber):\n    \"\"\"\n    A class implementing a framegrabber for WidowX robot arm - here Intel RealSense camera as third person camera\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(self, environment: \"Environment\", verbose: bool = False):\n        super().__init__(environment, verbose)\n\n    # *** PUBLIC GET methods ***\n\n    # *** PUBLIC methods ***\n\n    def get_current_frame(self) -&gt; np.ndarray:\n        \"\"\"\n        Captures an image of the robot's workspace, ensuring proper undistortion in BGR.\n\n        Args:\n\n        Returns:\n            numpy.ndarray: Raw image captured from the robot's camera.\n        \"\"\"\n        # TODO: capture a frame from the camera and set _current_frame\n\n        return self._current_frame\n</code></pre>"},{"location":"api/camera/#robot_environment.camera.widowx_framegrabber.WidowXFrameGrabber-functions","title":"Functions","text":""},{"location":"api/camera/#robot_environment.camera.widowx_framegrabber.WidowXFrameGrabber.get_current_frame","title":"<code>get_current_frame()</code>","text":"<p>Captures an image of the robot's workspace, ensuring proper undistortion in BGR.</p> <p>Args:</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Raw image captured from the robot's camera.</p> Source code in <code>robot_environment/camera/widowx_framegrabber.py</code> <pre><code>def get_current_frame(self) -&gt; np.ndarray:\n    \"\"\"\n    Captures an image of the robot's workspace, ensuring proper undistortion in BGR.\n\n    Args:\n\n    Returns:\n        numpy.ndarray: Raw image captured from the robot's camera.\n    \"\"\"\n    # TODO: capture a frame from the camera and set _current_frame\n\n    return self._current_frame\n</code></pre>"},{"location":"api/environment/","title":"Environment","text":""},{"location":"api/environment/#robot_environment.environment.Environment","title":"<code>robot_environment.environment.Environment</code>","text":"<p>Environment class for robotic pick-and-place operations.</p> <p>Coordinates between: - Robot control - Vision system - Workspace management - Object memory tracking</p> Source code in <code>robot_environment/environment.py</code> <pre><code>class Environment:\n    \"\"\"\n    Environment class for robotic pick-and-place operations.\n\n    Coordinates between:\n    - Robot control\n    - Vision system\n    - Workspace management\n    - Object memory tracking\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(\n        self,\n        el_api_key: str,\n        use_simulation: bool,\n        robot_id: str,\n        verbose: bool = False,\n        start_camera_thread: bool = True,\n        enable_performance_monitoring: bool = True,\n        performance_log_interval: float = 60.0,\n    ):\n        \"\"\"\n        Creates environment object.\n\n        Args:\n            el_api_key: ElevenLabs API Key for text-to-speech\n            use_simulation: If True, simulate the robot, else use real robot\n            robot_id: Robot identifier (\"niryo\" or \"widowx\")\n            verbose: Enable verbose logging\n            start_camera_thread: If True, start camera update thread\n                                Set to False for MCP server!\n            enable_performance_monitoring: Enable performance metrics tracking\n            performance_log_interval: Interval in seconds for performance logging\n        \"\"\"\n        self._use_simulation = use_simulation\n        self._verbose = verbose\n        self._logger = get_package_logger(__name__, verbose)\n\n        self._logger.info(\"Initializing Environment\")\n        self._logger.debug(\n            f\"Configuration: simulation={use_simulation}, \"\n            f\"robot_id={robot_id}, camera_thread={start_camera_thread}, \"\n            f\"metrics={enable_performance_monitoring}\"\n        )\n\n        # Initialize performance metrics\n        self._metrics = PerformanceMetrics(history_size=100, verbose=verbose) if enable_performance_monitoring else None\n\n        self._performance_monitor: Optional[PerformanceMonitor] = None\n        if enable_performance_monitoring:\n            self._performance_monitor = PerformanceMonitor(\n                self._metrics, interval_seconds=performance_log_interval, verbose=verbose\n            )\n\n        # Initialize robot (must come before framegrabber and workspaces)\n        self._robot = Robot(self, use_simulation, robot_id, verbose)\n\n        # Initialize robot-specific components\n        if isinstance(self.get_robot_controller(), NiryoRobotController):\n            self._framegrabber = NiryoFrameGrabber(self, verbose=verbose)\n            self._workspaces = NiryoWorkspaces(self, verbose)\n            self._logger.debug(f\"Home workspace: {self._workspaces.get_home_workspace()}\")\n        elif isinstance(self.get_robot_controller(), WidowXRobotController):\n            self._framegrabber = WidowXFrameGrabber(self, verbose=verbose)\n            self._workspaces = WidowXWorkspaces(self, verbose)\n        else:\n            self._logger.error(f\"Unknown robot controller type: {self.get_robot_controller()}\")\n\n        # Initialize text-to-speech\n        self._oralcom = Text2Speech(\n            el_api_key,\n            verbose=verbose,\n            enable_queue=True,  # Enable built-in audio queue\n            max_queue_size=50,\n            duplicate_timeout=2.0,\n        )\n\n        # Thread control\n        self._stop_event = threading.Event()\n\n        # Initialize ObjectMemoryManager\n        self._memory_manager = ObjectMemoryManager(manual_update_timeout=5.0, position_tolerance=0.05, verbose=verbose)\n\n        # Initialize workspaces in memory manager\n        if hasattr(self._workspaces, \"__iter__\"):\n            try:\n                for workspace in self._workspaces:\n                    workspace_id = workspace.id()\n                    self._memory_manager.initialize_workspace(workspace_id)\n                    self._logger.debug(f\"Initialized memory for workspace: {workspace_id}\")\n            except Exception as e:\n                self._logger.warning(f\"Could not iterate workspaces: {e}\")\n                if hasattr(self._workspaces, \"get_workspace_home_id\"):\n                    default_ws_id = self._workspaces.get_workspace_home_id()\n                    self._memory_manager.initialize_workspace(default_ws_id)\n\n        # Current workspace tracking\n        # self._current_workspace_id: Optional[str] = None\n        self._current_workspace_id = self._workspaces.get_workspace_home_id()\n        self._logger.debug(f\"Set initial workspace to: {self._current_workspace_id}\")\n\n        # Redis-based communication\n        self._object_broker = RedisMessageBroker()\n        self._label_manager = RedisLabelManager()\n\n        # Start performance monitor if enabled\n        if self._performance_monitor:\n            self._performance_monitor.start()\n            self._logger.info(\"Performance monitoring started\")\n\n        # Start camera thread if requested\n        if start_camera_thread:\n            self._logger.info(\"Starting camera update thread...\")\n            self.start_camera_updates(visualize=False)\n        else:\n            self._logger.info(\"Camera thread disabled (manual control)\")\n\n    def __del__(self):\n        \"\"\"Destructor.\"\"\"\n        if hasattr(self, \"_stop_event\"):\n            self._logger.debug(\"Shutting down environment in destructor...\")\n            self._stop_event.set()\n\n        if hasattr(self, \"_performance_monitor\") and self._performance_monitor:\n            self._performance_monitor.stop()\n\n    def cleanup(self):\n        \"\"\"\n        Explicit cleanup method - call when done with the object.\n        More reliable than relying on __del__.\n        \"\"\"\n        if hasattr(self, \"_stop_event\"):\n            self._logger.info(\"Shutting down environment...\")\n            self._stop_event.set()\n\n        if hasattr(self, \"_performance_monitor\") and self._performance_monitor:\n            self._performance_monitor.stop()\n\n        if hasattr(self, \"_oralcom\"):\n            self._oralcom.shutdown(timeout=5.0)\n\n    # *** PUBLIC METHODS ***\n\n    def start_camera_updates(self, visualize: bool = False) -&gt; threading.Thread:\n        \"\"\"\n        Start the background camera update thread.\n\n        Args:\n            visualize: If True, show the camera feed (requires GUI).\n\n        Returns:\n            The started threading.Thread object.\n        \"\"\"\n\n        def loop():\n            for _ in self.update_camera_and_objects(visualize=visualize):\n                pass\n\n        t = threading.Thread(target=loop, daemon=True)\n        t.start()\n        return t\n\n    def update_camera_and_objects(self, visualize: bool = False):\n        \"\"\"\n        Continuously updates the camera and detected objects.\n\n        Args:\n            visualize: If True, displays the updated camera feed\n        \"\"\"\n        # FIX: Get home workspace ID and set it as current\n        home_workspace_id = self._workspaces.get_workspace_home_id()\n\n        if self._current_workspace_id is None:\n            self._current_workspace_id = home_workspace_id  # Set before moving\n\n        self.robot_move2observation_pose(home_workspace_id)\n\n        while not self._stop_event.is_set():\n            loop_start = time.perf_counter()\n\n            # Get current frame with timing\n            if self._metrics:\n                with self._metrics.timer(\"frame_capture\"):\n                    img = self.get_current_frame()\n            else:\n                img = self.get_current_frame()\n\n            time.sleep(0.1)\n\n            # Get detected objects from Redis\n            # Get detected objects from Redis with timing\n            if self._metrics:\n                with self._metrics.timer(\"object_fetch_redis\"):\n                    detected_objects = self.get_detected_objects()\n\n                # Record object count\n                self._metrics.increment_counter(\"objects_detected\", len(detected_objects))\n            else:\n                detected_objects = self.get_detected_objects()\n\n            # Update memory using ObjectMemoryManager\n            if self._current_workspace_id:  # This should now always be True\n                at_observation = self.is_any_workspace_visible()\n                robot_moving = self.get_robot_in_motion()\n\n                if self._metrics:\n                    mem_start = time.perf_counter()\n\n                objects_added, objects_updated = self._memory_manager.update(\n                    workspace_id=self._current_workspace_id,\n                    detected_objects=detected_objects,\n                    at_observation_pose=at_observation,\n                    robot_in_motion=robot_moving,\n                )\n\n                if self._metrics:\n                    mem_duration = (time.perf_counter() - mem_start) * 1000\n                    self._metrics.record_memory_update(mem_duration, objects_added, objects_updated)\n\n                self._logger.debug(\n                    f\"Memory update for '{self._current_workspace_id}': \" f\"added={objects_added}, updated={objects_updated}\"\n                )\n            else:\n                # This should never happen now, but log it if it does\n                self._logger.error(\"Current workspace ID is None - memory not updated!\")\n\n            # Record loop iteration time\n            if self._metrics:\n                loop_duration = (time.perf_counter() - loop_start) * 1000\n                self._metrics.record_timing(\"camera_loop_iteration\", loop_duration)\n\n            # Log memory stats\n            if self._verbose:\n                stats = self._memory_manager.get_memory_stats()\n                if self._current_workspace_id in stats:\n                    ws_stats = stats[self._current_workspace_id]\n                    self._logger.debug(\n                        f\"Memory: {ws_stats['object_count']} objects, \"\n                        f\"manual_updates={ws_stats['manual_updates']}, \"\n                        f\"visible={ws_stats['visible']}\\n\"\n                    )\n\n            yield img\n\n            if self.get_robot_in_motion():\n                time.sleep(0.05)\n            else:\n                time.sleep(0.05)\n\n    def clear_memory(self) -&gt; None:\n        \"\"\"\n        Manually clear all objects from memory.\n        Useful when workspace has changed significantly.\n        \"\"\"\n        self._logger.warning(\"Clearing memory of all objects\")\n\n        if self._metrics:\n            with self._metrics.timer(\"memory_clear\"):\n                self._memory_manager.clear()\n            self._metrics.increment_counter(\"memory_clears\")\n        else:\n            self._memory_manager.clear()\n\n    def get_detected_objects_from_memory(self) -&gt; Objects:\n        \"\"\"\n        Get a copy of the object memory for current workspace.\n\n        Returns:\n            Objects: Copy of objects currently in memory\n        \"\"\"\n        if self._metrics:\n            with self._metrics.timer(\"memory_get\"):\n                result = self._get_memory_internal()\n            return result\n        else:\n            return self._get_memory_internal()\n\n    def _get_memory_internal(self) -&gt; Objects:\n        \"\"\"Internal method for getting memory.\"\"\"\n        if self._current_workspace_id:\n            return self._memory_manager.get(self._current_workspace_id)\n\n        home_ws_id = self._workspaces.get_workspace_home_id()\n        return self._memory_manager.get(home_ws_id)\n\n    def remove_object_from_memory(self, object_label: str, coordinate: List[float]) -&gt; None:\n        \"\"\"\n        Remove an object from memory after manipulation.\n\n        Args:\n            object_label: Label of the object to remove\n            coordinate: Last known coordinate [x, y]\n        \"\"\"\n        if not self._current_workspace_id:\n            self._logger.warning(\"No current workspace set\")\n            return\n\n        self._memory_manager.remove_object(\n            workspace_id=self._current_workspace_id, object_label=object_label, coordinate=coordinate\n        )\n\n    def update_object_in_memory(self, object_label: str, old_coordinate: List[float], new_pose: \"PoseObjectPNP\") -&gt; None:\n        \"\"\"\n        Update an object's position in memory after movement.\n\n        Args:\n            object_label: Label of the object\n            old_coordinate: Previous coordinate [x, y]\n            new_pose: New pose after movement\n        \"\"\"\n        if not self._current_workspace_id:\n            self._logger.warning(\"No current workspace set\")\n            return\n\n        self._memory_manager.mark_manual_update(\n            workspace_id=self._current_workspace_id,\n            object_label=object_label,\n            old_coordinate=old_coordinate,\n            new_pose=new_pose,\n        )\n\n    # Performance metrics methods\n\n    def get_performance_metrics(self) -&gt; Optional[PerformanceMetrics]:\n        \"\"\"\n        Get the performance metrics tracker.\n\n        Returns:\n            PerformanceMetrics instance or None if disabled\n        \"\"\"\n        return self._metrics\n\n    def get_performance_stats(self) -&gt; Optional[Dict]:\n        \"\"\"\n        Get current performance statistics.\n\n        Returns:\n            Dictionary with performance stats or None if disabled\n        \"\"\"\n        if self._metrics:\n            return self._metrics.get_stats()\n        return None\n\n    def print_performance_summary(self) -&gt; None:\n        \"\"\"Print a human-readable performance summary.\"\"\"\n        if self._metrics:\n            print(self._metrics.get_summary())\n        else:\n            print(\"Performance monitoring is disabled\")\n\n    def export_performance_metrics(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export performance metrics to JSON file.\n\n        Args:\n            filepath: Path to output file\n        \"\"\"\n        if self._metrics:\n            self._metrics.export_json(filepath)\n            self._logger.info(f\"Performance metrics exported to {filepath}\")\n        else:\n            self._logger.warning(\"Performance monitoring is disabled\")\n\n    def reset_performance_metrics(self) -&gt; None:\n        \"\"\"Reset all performance metrics.\"\"\"\n        if self._metrics:\n            self._metrics.reset()\n            self._logger.info(\"Performance metrics reset\")\n\n    # Multi-workspace memory methods\n\n    def get_current_workspace_id(self) -&gt; Optional[str]:\n        \"\"\"Get the ID of the currently observed workspace.\"\"\"\n        return self._current_workspace_id\n\n    def set_current_workspace(self, workspace_id: str) -&gt; None:\n        \"\"\"Set the current workspace being observed.\"\"\"\n        self._current_workspace_id = workspace_id\n        self._logger.debug(f\"Current workspace set to: {workspace_id}\")\n\n    def get_detected_objects_from_workspace(self, workspace_id: str) -&gt; Objects:\n        \"\"\"\n        Get objects from a specific workspace memory.\n\n        Args:\n            workspace_id: ID of the workspace\n\n        Returns:\n            Objects: Copy of objects in that workspace's memory\n        \"\"\"\n        return self._memory_manager.get(workspace_id)\n\n    def get_all_workspace_objects(self) -&gt; Dict[str, Objects]:\n        \"\"\"\n        Get objects from all workspaces.\n\n        Returns:\n            Dict mapping workspace_id to Objects collection\n        \"\"\"\n        return self._memory_manager.get_all()\n\n    def clear_workspace_memory(self, workspace_id: str) -&gt; None:\n        \"\"\"Clear memory for a specific workspace.\"\"\"\n        self._memory_manager.clear(workspace_id)\n\n    def remove_object_from_workspace(self, workspace_id: str, object_label: str, coordinate: list) -&gt; None:\n        \"\"\"Remove an object from a specific workspace's memory.\"\"\"\n        self._memory_manager.remove_object(workspace_id=workspace_id, object_label=object_label, coordinate=coordinate)\n\n    def update_object_in_workspace(\n        self, source_workspace_id: str, target_workspace_id: str, object_label: str, old_coordinate: list, new_coordinate: list\n    ) -&gt; None:\n        \"\"\"\n        Move an object from one workspace to another in memory.\n\n        Args:\n            source_workspace_id: ID of workspace where object currently is\n            target_workspace_id: ID of workspace where object will be placed\n            object_label: Label of the object\n            old_coordinate: Current coordinate in source workspace\n            new_coordinate: New coordinate in target workspace\n        \"\"\"\n        self._memory_manager.move_object(\n            source_workspace_id=source_workspace_id,\n            target_workspace_id=target_workspace_id,\n            object_label=object_label,\n            old_coordinate=old_coordinate,\n            new_coordinate=new_coordinate,\n        )\n\n    # *** PUBLIC SET METHODS ***\n\n    def add_object_name2object_labels(self, object_name: str) -&gt; str:\n        \"\"\"\n        Add a new object to the list of recognizable objects via Redis.\n\n        Args:\n            object_name: Name of the object to add\n\n        Returns:\n            str: Status message\n        \"\"\"\n        # Add label to Redis stream\n        success = self._label_manager.add_label(object_name)\n\n        if success:\n            mymessage = f\"Added {object_name} to the list of recognizable objects.\"\n        else:\n            mymessage = f\"{object_name} is already in the list of recognizable objects.\"\n\n        # Provide audio feedback\n        thread_oral = self._oralcom.call_text2speech_async(mymessage)\n        thread_oral.join()\n\n        return mymessage\n\n    def stop_camera_updates(self) -&gt; None:\n        \"\"\"Stop camera update thread.\"\"\"\n        self._stop_event.set()\n\n    def oralcom_call_text2speech_async(self, text: str, priority: int = 0) -&gt; bool:\n        \"\"\"\n        Asynchronously call text-to-speech API.\n\n        Args:\n            text: Message for text-to-speech\n            priority: Priority (0-10, higher = more urgent)\n\n        Returns:\n            True if queued successfully (or dummy thread for compatibility)\n        \"\"\"\n        return self._oralcom.speak(text, priority=priority, blocking=False)\n\n    # *** PUBLIC GET METHODS ***\n\n    def get_largest_free_space_with_center(self, workspace_id: Optional[str] = None) -&gt; Tuple[float, float, float]:\n        \"\"\"\n        Determines the largest free space in the workspace in square metres and its center coordinate in metres.\n        This method can be used to determine at which location an object can be placed safely.\n\n        Args:\n            workspace_id: Optional ID of the workspace to analyze. If None, the home workspace is used.\n\n        Returns:\n            tuple: (largest_free_area_m2, center_x, center_y) where:\n                - largest_area_m2 (float): Largest free area in square meters.\n                - center_x (float): X-coordinate of the center of the largest free area in meters.\n                - center_y (float): Y-coordinate of the center of the largest free area in meters.\n        \"\"\"\n        if workspace_id:\n            workspace = self.get_workspace_by_id(workspace_id)\n        else:\n            workspace = self.get_workspace(0)\n\n        if workspace is None:\n            self._logger.error(f\"Workspace not found: {workspace_id}\")\n            return 0.0, 0.0, 0.0\n\n        detected_objects = self.get_detected_objects()\n\n        return calculate_largest_free_space(\n            workspace=workspace, detected_objects=detected_objects, visualize=self.verbose, logger=self._logger\n        )\n\n    def get_workspace_coordinate_from_point(self, workspace_id: str, point: str) -&gt; Optional[List[float]]:\n        \"\"\"\n        Get the world coordinate of a special point of the given workspace.\n\n        Args:\n            workspace_id (str): ID of workspace.\n            point (str): description of point. Possible values are:\n            - 'upper left corner': Returns the world coordinate of the upper left corner of the workspace.\n            - 'upper right corner': Returns the world coordinate of the upper right corner of the workspace.\n            - 'lower left corner': Returns the world coordinate of the lower left corner of the workspace.\n            - 'lower right corner': Returns the world coordinate of the lower right corner of the workspace.\n            - 'center point': Returns the world coordinate of the center of the workspace.\n\n        Returns:\n            List[float]: (x,y) world coordinate of the point on the workspace that was specified by the argument point.\n        \"\"\"\n        if point == \"upper left corner\":\n            return self.get_workspace_by_id(workspace_id).xy_ul_wc().xy_coordinate()\n        elif point == \"upper right corner\":\n            return self.get_workspace_by_id(workspace_id).xy_ur_wc().xy_coordinate()\n        elif point == \"lower left corner\":\n            return self.get_workspace_by_id(workspace_id).xy_ll_wc().xy_coordinate()\n        elif point == \"lower right corner\":\n            return self.get_workspace_by_id(workspace_id).xy_lr_wc().xy_coordinate()\n        elif point == \"center point\":\n            return self.get_workspace_by_id(workspace_id).xy_center_wc().xy_coordinate()\n        else:\n            self._logger.error(f\"Unknown point type: {point}\")\n            return None\n\n    # GET methods from Workspaces\n\n    def get_workspace(self, index: int = 0) -&gt; Workspace:\n        \"\"\"\n        Return the workspace at the given position index in the list of workspaces.\n\n        Args:\n            index: 0-based index in the list of workspaces.\n\n        Returns:\n\n        \"\"\"\n        return self._workspaces.get_workspace(index)\n\n    def get_workspace_by_id(self, workspace_id: str) -&gt; Optional[Workspace]:\n        \"\"\"\n        Return the Workspace object with the given id, if existent, else None is returned.\n\n        Args:\n            id: workspace ID\n\n        Returns:\n            Workspace or None, if no workspace with the given id exists.\n        \"\"\"\n        return self._workspaces.get_workspace_by_id(workspace_id)\n\n    def get_workspace_home_id(self) -&gt; str:\n        \"\"\"\n        Returns the ID of the workspace at index 0.\n\n        Returns:\n            the ID of the workspace at index 0.\n        \"\"\"\n        return self._workspaces.get_workspace_home_id()\n\n    def get_workspace_id(self, index: int) -&gt; str:\n        \"\"\"\n        Return the id of the workspace at the given position index in the list of workspaces.\n\n        Args:\n            index: 0-based index in the list of workspaces.\n\n        Returns:\n            str: id of the workspace at the given position index in the list of workspaces.\n        \"\"\"\n        return self._workspaces.get_workspace_id(index)\n\n    @log_start_end_cls()\n    def get_visible_workspace(self, camera_pose: PoseObjectPNP) -&gt; Workspace:\n        \"\"\"Get visible workspace from camera pose.\"\"\"\n        return self._workspaces.get_visible_workspace(camera_pose)\n\n    def is_any_workspace_visible(self) -&gt; bool:\n        \"\"\"Check if any workspace is currently visible.\"\"\"\n        pose = self.get_robot_pose()\n        return self.get_visible_workspace(pose) is not None\n\n    def get_observation_pose(self, workspace_id: str) -&gt; PoseObjectPNP:\n        \"\"\"\n        Return the observation pose of the given workspace id\n\n        Args:\n            workspace_id: id of the workspace\n\n        Returns:\n            PoseObjectPNP: observation pose of the gripper where it can observe the workspace given by workspace_id\n        \"\"\"\n        return self._workspaces.get_observation_pose(workspace_id)\n\n    # Camera-related GET methods\n\n    def get_current_frame(self) -&gt; np.ndarray:\n        \"\"\"\n        Capture image from robot's camera.\n\n        Returns:\n            numpy.ndarray: Camera image\n        \"\"\"\n        frame = self._framegrabber.get_current_frame()\n\n        if self._metrics and frame is not None:\n            self._metrics.increment_counter(\"frames_captured\")\n\n        return frame\n\n    def get_current_frame_width_height(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Returns width and height of current frame in pixels.\n\n        Returns:\n            width and height of current frame in pixels.\n        \"\"\"\n        return self._framegrabber.get_current_frame_width_height()\n\n    # Robot-related GET methods\n\n    def get_robot_controller(self) -&gt; RobotController:\n        \"\"\"\n\n        Returns:\n            RobotController: object that controls the robot.\n        \"\"\"\n        return self._robot.robot()\n\n    @log_start_end_cls()\n    def get_robot_in_motion(self) -&gt; bool:\n        \"\"\"\n        Check if robot is in motion.\n\n        Returns:\n            bool: True if robot is moving, False otherwise\n        \"\"\"\n        return self._robot.robot_in_motion()\n\n    def get_robot_pose(self) -&gt; PoseObjectPNP:\n        \"\"\"\n        Get current pose of gripper of robot.\n\n        Returns:\n            current pose of gripper of robot.\n        \"\"\"\n        if self._metrics:\n            with self._metrics.timer(\"robot_get_pose\"):\n                return self._robot.get_pose()\n        else:\n            return self._robot.get_pose()\n\n    @log_start_end_cls()\n    def get_robot_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; PoseObjectPNP:\n        \"\"\"\n        Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n        calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n        given coordinate an object that has the given orientation. For this method to work, it is important that\n        only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n        this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n        Args:\n            workspace_id: id of the workspace\n            u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n            v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n            yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n        Returns:\n            PoseObjectPNP: Pose in world coordinates\n        \"\"\"\n        return self._robot.get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n\n    # Vision-related GET methods\n\n    def get_object_labels_as_string(self) -&gt; str:\n        \"\"\"\n        Return detectable object labels as comma-separated string.\n\n        Returns:\n            str: Comma-separated list of objects\n        \"\"\"\n        object_labels = self.get_object_labels()\n\n        if not object_labels or not object_labels[0]:\n            return \"No detectable objects configured.\"\n\n        return f\"I can recognize these objects: {', '.join(object_labels[0])}\"\n\n    def get_detected_objects(self) -&gt; Objects:\n        \"\"\"\n        Get detected objects from Redis stream.\n\n        Returns:\n            Objects: Collection of detected objects\n        \"\"\"\n        # Get latest objects from Redis (published by vision_detect_segment)\n        objects_dict_list = self._object_broker.get_latest_objects(max_age_seconds=2.0)\n\n        if not objects_dict_list:\n            if self.verbose:\n                print(\"No fresh object detections from Redis\")\n            return Objects()\n\n        # Convert dictionaries to Object instances\n        return Objects.dict_list_to_objects(objects_dict_list, self.get_workspace(0))\n\n    def get_object_labels(self) -&gt; List[List[str]]:\n        \"\"\"\n        Get list of detectable object labels from Redis.\n\n        Returns:\n            List of lists of detectable strings\n        \"\"\"\n        # Get latest labels from Redis (published by vision_detect_segment)\n        labels = self._label_manager.get_latest_labels(timeout_seconds=60.0)\n\n        if labels is None:\n            if self.verbose:\n                print(\"No labels from Redis, using empty list\")\n            return [[]]\n\n        return [labels]\n\n    # Robot control methods\n\n    def robot_move2home_observation_pose(self) -&gt; None:\n        \"\"\"Move robot to home workspace observation pose.\"\"\"\n        workspace_id = self.get_workspace_home_id()\n        self.robot_move2observation_pose(workspace_id)\n\n    def robot_move2observation_pose(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        Move robot to observation pose for given workspace.\n\n        Args:\n            workspace_id: ID of workspace\n        \"\"\"\n        if self._metrics:\n            with self._metrics.timer(\"robot_move_observation\"):\n                self._robot.move2observation_pose(workspace_id)\n        else:\n            self._robot.move2observation_pose(workspace_id)\n\n        self._current_workspace_id = workspace_id\n        self._logger.debug(f\"Set current workspace to: {workspace_id}\")\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    # *** PUBLIC properties ***\n\n    def workspaces(self) -&gt; Workspaces:\n        \"\"\"Return workspaces object.\"\"\"\n        return self._workspaces\n\n    def framegrabber(self) -&gt; FrameGrabber:\n        \"\"\"Return framegrabber object.\"\"\"\n        return self._framegrabber\n\n    def robot(self) -&gt; Robot:\n        \"\"\"Return robot object.\"\"\"\n        return self._robot\n\n    def use_simulation(self) -&gt; bool:\n        \"\"\"Check if using simulation.\"\"\"\n        return self._use_simulation\n\n    def metrics(self) -&gt; Optional[PerformanceMetrics]:\n        \"\"\"Get performance metrics tracker.\"\"\"\n        return self._metrics\n\n    @property\n    def verbose(self) -&gt; bool:\n        \"\"\"Check if verbose logging enabled.\"\"\"\n        return self._logger.isEnabledFor(logging.DEBUG)\n\n    @verbose.setter\n    def verbose(self, value: bool):\n        \"\"\"Set verbose logging on/off.\"\"\"\n        from .common.logger_config import set_verbose\n\n        set_verbose(self._logger, value)\n\n    # *** PRIVATE VARIABLES ***\n\n    # Workspaces object\n    _workspaces = None\n\n    # FrameGraber object\n    _framegrabber = None\n\n    # Robot object\n    _robot = None\n    _use_simulation = False\n    _verbose = False\n    _memory_manager: Optional[ObjectMemoryManager] = None\n    _current_workspace_id: Optional[str] = None\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment-attributes","title":"Attributes","text":""},{"location":"api/environment/#robot_environment.environment.Environment.verbose","title":"<code>verbose</code>  <code>property</code> <code>writable</code>","text":"<p>Check if verbose logging enabled.</p>"},{"location":"api/environment/#robot_environment.environment.Environment-functions","title":"Functions","text":""},{"location":"api/environment/#robot_environment.environment.Environment.__del__","title":"<code>__del__()</code>","text":"<p>Destructor.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def __del__(self):\n    \"\"\"Destructor.\"\"\"\n    if hasattr(self, \"_stop_event\"):\n        self._logger.debug(\"Shutting down environment in destructor...\")\n        self._stop_event.set()\n\n    if hasattr(self, \"_performance_monitor\") and self._performance_monitor:\n        self._performance_monitor.stop()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.__init__","title":"<code>__init__(el_api_key, use_simulation, robot_id, verbose=False, start_camera_thread=True, enable_performance_monitoring=True, performance_log_interval=60.0)</code>","text":"<p>Creates environment object.</p> <p>Parameters:</p> Name Type Description Default <code>el_api_key</code> <code>str</code> <p>ElevenLabs API Key for text-to-speech</p> required <code>use_simulation</code> <code>bool</code> <p>If True, simulate the robot, else use real robot</p> required <code>robot_id</code> <code>str</code> <p>Robot identifier (\"niryo\" or \"widowx\")</p> required <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>False</code> <code>start_camera_thread</code> <code>bool</code> <p>If True, start camera update thread                 Set to False for MCP server!</p> <code>True</code> <code>enable_performance_monitoring</code> <code>bool</code> <p>Enable performance metrics tracking</p> <code>True</code> <code>performance_log_interval</code> <code>float</code> <p>Interval in seconds for performance logging</p> <code>60.0</code> Source code in <code>robot_environment/environment.py</code> <pre><code>def __init__(\n    self,\n    el_api_key: str,\n    use_simulation: bool,\n    robot_id: str,\n    verbose: bool = False,\n    start_camera_thread: bool = True,\n    enable_performance_monitoring: bool = True,\n    performance_log_interval: float = 60.0,\n):\n    \"\"\"\n    Creates environment object.\n\n    Args:\n        el_api_key: ElevenLabs API Key for text-to-speech\n        use_simulation: If True, simulate the robot, else use real robot\n        robot_id: Robot identifier (\"niryo\" or \"widowx\")\n        verbose: Enable verbose logging\n        start_camera_thread: If True, start camera update thread\n                            Set to False for MCP server!\n        enable_performance_monitoring: Enable performance metrics tracking\n        performance_log_interval: Interval in seconds for performance logging\n    \"\"\"\n    self._use_simulation = use_simulation\n    self._verbose = verbose\n    self._logger = get_package_logger(__name__, verbose)\n\n    self._logger.info(\"Initializing Environment\")\n    self._logger.debug(\n        f\"Configuration: simulation={use_simulation}, \"\n        f\"robot_id={robot_id}, camera_thread={start_camera_thread}, \"\n        f\"metrics={enable_performance_monitoring}\"\n    )\n\n    # Initialize performance metrics\n    self._metrics = PerformanceMetrics(history_size=100, verbose=verbose) if enable_performance_monitoring else None\n\n    self._performance_monitor: Optional[PerformanceMonitor] = None\n    if enable_performance_monitoring:\n        self._performance_monitor = PerformanceMonitor(\n            self._metrics, interval_seconds=performance_log_interval, verbose=verbose\n        )\n\n    # Initialize robot (must come before framegrabber and workspaces)\n    self._robot = Robot(self, use_simulation, robot_id, verbose)\n\n    # Initialize robot-specific components\n    if isinstance(self.get_robot_controller(), NiryoRobotController):\n        self._framegrabber = NiryoFrameGrabber(self, verbose=verbose)\n        self._workspaces = NiryoWorkspaces(self, verbose)\n        self._logger.debug(f\"Home workspace: {self._workspaces.get_home_workspace()}\")\n    elif isinstance(self.get_robot_controller(), WidowXRobotController):\n        self._framegrabber = WidowXFrameGrabber(self, verbose=verbose)\n        self._workspaces = WidowXWorkspaces(self, verbose)\n    else:\n        self._logger.error(f\"Unknown robot controller type: {self.get_robot_controller()}\")\n\n    # Initialize text-to-speech\n    self._oralcom = Text2Speech(\n        el_api_key,\n        verbose=verbose,\n        enable_queue=True,  # Enable built-in audio queue\n        max_queue_size=50,\n        duplicate_timeout=2.0,\n    )\n\n    # Thread control\n    self._stop_event = threading.Event()\n\n    # Initialize ObjectMemoryManager\n    self._memory_manager = ObjectMemoryManager(manual_update_timeout=5.0, position_tolerance=0.05, verbose=verbose)\n\n    # Initialize workspaces in memory manager\n    if hasattr(self._workspaces, \"__iter__\"):\n        try:\n            for workspace in self._workspaces:\n                workspace_id = workspace.id()\n                self._memory_manager.initialize_workspace(workspace_id)\n                self._logger.debug(f\"Initialized memory for workspace: {workspace_id}\")\n        except Exception as e:\n            self._logger.warning(f\"Could not iterate workspaces: {e}\")\n            if hasattr(self._workspaces, \"get_workspace_home_id\"):\n                default_ws_id = self._workspaces.get_workspace_home_id()\n                self._memory_manager.initialize_workspace(default_ws_id)\n\n    # Current workspace tracking\n    # self._current_workspace_id: Optional[str] = None\n    self._current_workspace_id = self._workspaces.get_workspace_home_id()\n    self._logger.debug(f\"Set initial workspace to: {self._current_workspace_id}\")\n\n    # Redis-based communication\n    self._object_broker = RedisMessageBroker()\n    self._label_manager = RedisLabelManager()\n\n    # Start performance monitor if enabled\n    if self._performance_monitor:\n        self._performance_monitor.start()\n        self._logger.info(\"Performance monitoring started\")\n\n    # Start camera thread if requested\n    if start_camera_thread:\n        self._logger.info(\"Starting camera update thread...\")\n        self.start_camera_updates(visualize=False)\n    else:\n        self._logger.info(\"Camera thread disabled (manual control)\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.add_object_name2object_labels","title":"<code>add_object_name2object_labels(object_name)</code>","text":"<p>Add a new object to the list of recognizable objects via Redis.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>Name of the object to add</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Status message</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def add_object_name2object_labels(self, object_name: str) -&gt; str:\n    \"\"\"\n    Add a new object to the list of recognizable objects via Redis.\n\n    Args:\n        object_name: Name of the object to add\n\n    Returns:\n        str: Status message\n    \"\"\"\n    # Add label to Redis stream\n    success = self._label_manager.add_label(object_name)\n\n    if success:\n        mymessage = f\"Added {object_name} to the list of recognizable objects.\"\n    else:\n        mymessage = f\"{object_name} is already in the list of recognizable objects.\"\n\n    # Provide audio feedback\n    thread_oral = self._oralcom.call_text2speech_async(mymessage)\n    thread_oral.join()\n\n    return mymessage\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.cleanup","title":"<code>cleanup()</code>","text":"<p>Explicit cleanup method - call when done with the object. More reliable than relying on del.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Explicit cleanup method - call when done with the object.\n    More reliable than relying on __del__.\n    \"\"\"\n    if hasattr(self, \"_stop_event\"):\n        self._logger.info(\"Shutting down environment...\")\n        self._stop_event.set()\n\n    if hasattr(self, \"_performance_monitor\") and self._performance_monitor:\n        self._performance_monitor.stop()\n\n    if hasattr(self, \"_oralcom\"):\n        self._oralcom.shutdown(timeout=5.0)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.clear_memory","title":"<code>clear_memory()</code>","text":"<p>Manually clear all objects from memory. Useful when workspace has changed significantly.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def clear_memory(self) -&gt; None:\n    \"\"\"\n    Manually clear all objects from memory.\n    Useful when workspace has changed significantly.\n    \"\"\"\n    self._logger.warning(\"Clearing memory of all objects\")\n\n    if self._metrics:\n        with self._metrics.timer(\"memory_clear\"):\n            self._memory_manager.clear()\n        self._metrics.increment_counter(\"memory_clears\")\n    else:\n        self._memory_manager.clear()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.clear_workspace_memory","title":"<code>clear_workspace_memory(workspace_id)</code>","text":"<p>Clear memory for a specific workspace.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def clear_workspace_memory(self, workspace_id: str) -&gt; None:\n    \"\"\"Clear memory for a specific workspace.\"\"\"\n    self._memory_manager.clear(workspace_id)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.export_performance_metrics","title":"<code>export_performance_metrics(filepath)</code>","text":"<p>Export performance metrics to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output file</p> required Source code in <code>robot_environment/environment.py</code> <pre><code>def export_performance_metrics(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export performance metrics to JSON file.\n\n    Args:\n        filepath: Path to output file\n    \"\"\"\n    if self._metrics:\n        self._metrics.export_json(filepath)\n        self._logger.info(f\"Performance metrics exported to {filepath}\")\n    else:\n        self._logger.warning(\"Performance monitoring is disabled\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.framegrabber","title":"<code>framegrabber()</code>","text":"<p>Return framegrabber object.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def framegrabber(self) -&gt; FrameGrabber:\n    \"\"\"Return framegrabber object.\"\"\"\n    return self._framegrabber\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_all_workspace_objects","title":"<code>get_all_workspace_objects()</code>","text":"<p>Get objects from all workspaces.</p> <p>Returns:</p> Type Description <code>Dict[str, Objects]</code> <p>Dict mapping workspace_id to Objects collection</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_all_workspace_objects(self) -&gt; Dict[str, Objects]:\n    \"\"\"\n    Get objects from all workspaces.\n\n    Returns:\n        Dict mapping workspace_id to Objects collection\n    \"\"\"\n    return self._memory_manager.get_all()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_current_frame","title":"<code>get_current_frame()</code>","text":"<p>Capture image from robot's camera.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Camera image</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_current_frame(self) -&gt; np.ndarray:\n    \"\"\"\n    Capture image from robot's camera.\n\n    Returns:\n        numpy.ndarray: Camera image\n    \"\"\"\n    frame = self._framegrabber.get_current_frame()\n\n    if self._metrics and frame is not None:\n        self._metrics.increment_counter(\"frames_captured\")\n\n    return frame\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_current_frame_width_height","title":"<code>get_current_frame_width_height()</code>","text":"<p>Returns width and height of current frame in pixels.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>width and height of current frame in pixels.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_current_frame_width_height(self) -&gt; tuple[int, int]:\n    \"\"\"\n    Returns width and height of current frame in pixels.\n\n    Returns:\n        width and height of current frame in pixels.\n    \"\"\"\n    return self._framegrabber.get_current_frame_width_height()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_current_workspace_id","title":"<code>get_current_workspace_id()</code>","text":"<p>Get the ID of the currently observed workspace.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_current_workspace_id(self) -&gt; Optional[str]:\n    \"\"\"Get the ID of the currently observed workspace.\"\"\"\n    return self._current_workspace_id\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_detected_objects","title":"<code>get_detected_objects()</code>","text":"<p>Get detected objects from Redis stream.</p> <p>Returns:</p> Name Type Description <code>Objects</code> <code>Objects</code> <p>Collection of detected objects</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_detected_objects(self) -&gt; Objects:\n    \"\"\"\n    Get detected objects from Redis stream.\n\n    Returns:\n        Objects: Collection of detected objects\n    \"\"\"\n    # Get latest objects from Redis (published by vision_detect_segment)\n    objects_dict_list = self._object_broker.get_latest_objects(max_age_seconds=2.0)\n\n    if not objects_dict_list:\n        if self.verbose:\n            print(\"No fresh object detections from Redis\")\n        return Objects()\n\n    # Convert dictionaries to Object instances\n    return Objects.dict_list_to_objects(objects_dict_list, self.get_workspace(0))\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_detected_objects_from_memory","title":"<code>get_detected_objects_from_memory()</code>","text":"<p>Get a copy of the object memory for current workspace.</p> <p>Returns:</p> Name Type Description <code>Objects</code> <code>Objects</code> <p>Copy of objects currently in memory</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_detected_objects_from_memory(self) -&gt; Objects:\n    \"\"\"\n    Get a copy of the object memory for current workspace.\n\n    Returns:\n        Objects: Copy of objects currently in memory\n    \"\"\"\n    if self._metrics:\n        with self._metrics.timer(\"memory_get\"):\n            result = self._get_memory_internal()\n        return result\n    else:\n        return self._get_memory_internal()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_detected_objects_from_workspace","title":"<code>get_detected_objects_from_workspace(workspace_id)</code>","text":"<p>Get objects from a specific workspace memory.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <p>Returns:</p> Name Type Description <code>Objects</code> <code>Objects</code> <p>Copy of objects in that workspace's memory</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_detected_objects_from_workspace(self, workspace_id: str) -&gt; Objects:\n    \"\"\"\n    Get objects from a specific workspace memory.\n\n    Args:\n        workspace_id: ID of the workspace\n\n    Returns:\n        Objects: Copy of objects in that workspace's memory\n    \"\"\"\n    return self._memory_manager.get(workspace_id)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_largest_free_space_with_center","title":"<code>get_largest_free_space_with_center(workspace_id=None)</code>","text":"<p>Determines the largest free space in the workspace in square metres and its center coordinate in metres. This method can be used to determine at which location an object can be placed safely.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>Optional[str]</code> <p>Optional ID of the workspace to analyze. If None, the home workspace is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float, float]</code> <p>(largest_free_area_m2, center_x, center_y) where: - largest_area_m2 (float): Largest free area in square meters. - center_x (float): X-coordinate of the center of the largest free area in meters. - center_y (float): Y-coordinate of the center of the largest free area in meters.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_largest_free_space_with_center(self, workspace_id: Optional[str] = None) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Determines the largest free space in the workspace in square metres and its center coordinate in metres.\n    This method can be used to determine at which location an object can be placed safely.\n\n    Args:\n        workspace_id: Optional ID of the workspace to analyze. If None, the home workspace is used.\n\n    Returns:\n        tuple: (largest_free_area_m2, center_x, center_y) where:\n            - largest_area_m2 (float): Largest free area in square meters.\n            - center_x (float): X-coordinate of the center of the largest free area in meters.\n            - center_y (float): Y-coordinate of the center of the largest free area in meters.\n    \"\"\"\n    if workspace_id:\n        workspace = self.get_workspace_by_id(workspace_id)\n    else:\n        workspace = self.get_workspace(0)\n\n    if workspace is None:\n        self._logger.error(f\"Workspace not found: {workspace_id}\")\n        return 0.0, 0.0, 0.0\n\n    detected_objects = self.get_detected_objects()\n\n    return calculate_largest_free_space(\n        workspace=workspace, detected_objects=detected_objects, visualize=self.verbose, logger=self._logger\n    )\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_object_labels","title":"<code>get_object_labels()</code>","text":"<p>Get list of detectable object labels from Redis.</p> <p>Returns:</p> Type Description <code>List[List[str]]</code> <p>List of lists of detectable strings</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_object_labels(self) -&gt; List[List[str]]:\n    \"\"\"\n    Get list of detectable object labels from Redis.\n\n    Returns:\n        List of lists of detectable strings\n    \"\"\"\n    # Get latest labels from Redis (published by vision_detect_segment)\n    labels = self._label_manager.get_latest_labels(timeout_seconds=60.0)\n\n    if labels is None:\n        if self.verbose:\n            print(\"No labels from Redis, using empty list\")\n        return [[]]\n\n    return [labels]\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_object_labels_as_string","title":"<code>get_object_labels_as_string()</code>","text":"<p>Return detectable object labels as comma-separated string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Comma-separated list of objects</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_object_labels_as_string(self) -&gt; str:\n    \"\"\"\n    Return detectable object labels as comma-separated string.\n\n    Returns:\n        str: Comma-separated list of objects\n    \"\"\"\n    object_labels = self.get_object_labels()\n\n    if not object_labels or not object_labels[0]:\n        return \"No detectable objects configured.\"\n\n    return f\"I can recognize these objects: {', '.join(object_labels[0])}\"\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_observation_pose","title":"<code>get_observation_pose(workspace_id)</code>","text":"<p>Return the observation pose of the given workspace id</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>observation pose of the gripper where it can observe the workspace given by workspace_id</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_observation_pose(self, workspace_id: str) -&gt; PoseObjectPNP:\n    \"\"\"\n    Return the observation pose of the given workspace id\n\n    Args:\n        workspace_id: id of the workspace\n\n    Returns:\n        PoseObjectPNP: observation pose of the gripper where it can observe the workspace given by workspace_id\n    \"\"\"\n    return self._workspaces.get_observation_pose(workspace_id)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_performance_metrics","title":"<code>get_performance_metrics()</code>","text":"<p>Get the performance metrics tracker.</p> <p>Returns:</p> Type Description <code>Optional[PerformanceMetrics]</code> <p>PerformanceMetrics instance or None if disabled</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_performance_metrics(self) -&gt; Optional[PerformanceMetrics]:\n    \"\"\"\n    Get the performance metrics tracker.\n\n    Returns:\n        PerformanceMetrics instance or None if disabled\n    \"\"\"\n    return self._metrics\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_performance_stats","title":"<code>get_performance_stats()</code>","text":"<p>Get current performance statistics.</p> <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>Dictionary with performance stats or None if disabled</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_performance_stats(self) -&gt; Optional[Dict]:\n    \"\"\"\n    Get current performance statistics.\n\n    Returns:\n        Dictionary with performance stats or None if disabled\n    \"\"\"\n    if self._metrics:\n        return self._metrics.get_stats()\n    return None\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_robot_controller","title":"<code>get_robot_controller()</code>","text":"<p>Returns:</p> Name Type Description <code>RobotController</code> <code>RobotController</code> <p>object that controls the robot.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_robot_controller(self) -&gt; RobotController:\n    \"\"\"\n\n    Returns:\n        RobotController: object that controls the robot.\n    \"\"\"\n    return self._robot.robot()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_robot_in_motion","title":"<code>get_robot_in_motion()</code>","text":"<p>Check if robot is in motion.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if robot is moving, False otherwise</p> Source code in <code>robot_environment/environment.py</code> <pre><code>@log_start_end_cls()\ndef get_robot_in_motion(self) -&gt; bool:\n    \"\"\"\n    Check if robot is in motion.\n\n    Returns:\n        bool: True if robot is moving, False otherwise\n    \"\"\"\n    return self._robot.robot_in_motion()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_robot_pose","title":"<code>get_robot_pose()</code>","text":"<p>Get current pose of gripper of robot.</p> <p>Returns:</p> Type Description <code>PoseObjectPNP</code> <p>current pose of gripper of robot.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_robot_pose(self) -&gt; PoseObjectPNP:\n    \"\"\"\n    Get current pose of gripper of robot.\n\n    Returns:\n        current pose of gripper of robot.\n    \"\"\"\n    if self._metrics:\n        with self._metrics.timer(\"robot_get_pose\"):\n            return self._robot.get_pose()\n    else:\n        return self._robot.get_pose()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_robot_target_pose_from_rel","title":"<code>get_robot_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code>","text":"<p>Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw), calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the given coordinate an object that has the given orientation. For this method to work, it is important that only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <code>u_rel</code> <code>float</code> <p>horizontal coordinate in image of workspace, normalized between 0 and 1</p> required <code>v_rel</code> <code>float</code> <p>vertical coordinate in image of workspace, normalized between 0 and 1</p> required <code>yaw</code> <code>float</code> <p>orientation of an object at the pixel coordinates [u_rel, v_rel].</p> required <p>Returns:</p> Name Type Description <code>PoseObjectPNP</code> <code>PoseObjectPNP</code> <p>Pose in world coordinates</p> Source code in <code>robot_environment/environment.py</code> <pre><code>@log_start_end_cls()\ndef get_robot_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; PoseObjectPNP:\n    \"\"\"\n    Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n    calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n    given coordinate an object that has the given orientation. For this method to work, it is important that\n    only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n    this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n    Args:\n        workspace_id: id of the workspace\n        u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n        v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n        yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n    Returns:\n        PoseObjectPNP: Pose in world coordinates\n    \"\"\"\n    return self._robot.get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_visible_workspace","title":"<code>get_visible_workspace(camera_pose)</code>","text":"<p>Get visible workspace from camera pose.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>@log_start_end_cls()\ndef get_visible_workspace(self, camera_pose: PoseObjectPNP) -&gt; Workspace:\n    \"\"\"Get visible workspace from camera pose.\"\"\"\n    return self._workspaces.get_visible_workspace(camera_pose)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_workspace","title":"<code>get_workspace(index=0)</code>","text":"<p>Return the workspace at the given position index in the list of workspaces.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>0-based index in the list of workspaces.</p> <code>0</code> <p>Returns:</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_workspace(self, index: int = 0) -&gt; Workspace:\n    \"\"\"\n    Return the workspace at the given position index in the list of workspaces.\n\n    Args:\n        index: 0-based index in the list of workspaces.\n\n    Returns:\n\n    \"\"\"\n    return self._workspaces.get_workspace(index)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_workspace_by_id","title":"<code>get_workspace_by_id(workspace_id)</code>","text":"<p>Return the Workspace object with the given id, if existent, else None is returned.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <p>workspace ID</p> required <p>Returns:</p> Type Description <code>Optional[Workspace]</code> <p>Workspace or None, if no workspace with the given id exists.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_workspace_by_id(self, workspace_id: str) -&gt; Optional[Workspace]:\n    \"\"\"\n    Return the Workspace object with the given id, if existent, else None is returned.\n\n    Args:\n        id: workspace ID\n\n    Returns:\n        Workspace or None, if no workspace with the given id exists.\n    \"\"\"\n    return self._workspaces.get_workspace_by_id(workspace_id)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_workspace_coordinate_from_point","title":"<code>get_workspace_coordinate_from_point(workspace_id, point)</code>","text":"<p>Get the world coordinate of a special point of the given workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of workspace.</p> required <code>point</code> <code>str</code> <p>description of point. Possible values are:</p> required <code>- 'upper left corner'</code> <p>Returns the world coordinate of the upper left corner of the workspace.</p> required <code>- 'upper right corner'</code> <p>Returns the world coordinate of the upper right corner of the workspace.</p> required <code>- 'lower left corner'</code> <p>Returns the world coordinate of the lower left corner of the workspace.</p> required <code>- 'lower right corner'</code> <p>Returns the world coordinate of the lower right corner of the workspace.</p> required <code>- 'center point'</code> <p>Returns the world coordinate of the center of the workspace.</p> required <p>Returns:</p> Type Description <code>Optional[List[float]]</code> <p>List[float]: (x,y) world coordinate of the point on the workspace that was specified by the argument point.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_workspace_coordinate_from_point(self, workspace_id: str, point: str) -&gt; Optional[List[float]]:\n    \"\"\"\n    Get the world coordinate of a special point of the given workspace.\n\n    Args:\n        workspace_id (str): ID of workspace.\n        point (str): description of point. Possible values are:\n        - 'upper left corner': Returns the world coordinate of the upper left corner of the workspace.\n        - 'upper right corner': Returns the world coordinate of the upper right corner of the workspace.\n        - 'lower left corner': Returns the world coordinate of the lower left corner of the workspace.\n        - 'lower right corner': Returns the world coordinate of the lower right corner of the workspace.\n        - 'center point': Returns the world coordinate of the center of the workspace.\n\n    Returns:\n        List[float]: (x,y) world coordinate of the point on the workspace that was specified by the argument point.\n    \"\"\"\n    if point == \"upper left corner\":\n        return self.get_workspace_by_id(workspace_id).xy_ul_wc().xy_coordinate()\n    elif point == \"upper right corner\":\n        return self.get_workspace_by_id(workspace_id).xy_ur_wc().xy_coordinate()\n    elif point == \"lower left corner\":\n        return self.get_workspace_by_id(workspace_id).xy_ll_wc().xy_coordinate()\n    elif point == \"lower right corner\":\n        return self.get_workspace_by_id(workspace_id).xy_lr_wc().xy_coordinate()\n    elif point == \"center point\":\n        return self.get_workspace_by_id(workspace_id).xy_center_wc().xy_coordinate()\n    else:\n        self._logger.error(f\"Unknown point type: {point}\")\n        return None\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_workspace_home_id","title":"<code>get_workspace_home_id()</code>","text":"<p>Returns the ID of the workspace at index 0.</p> <p>Returns:</p> Type Description <code>str</code> <p>the ID of the workspace at index 0.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_workspace_home_id(self) -&gt; str:\n    \"\"\"\n    Returns the ID of the workspace at index 0.\n\n    Returns:\n        the ID of the workspace at index 0.\n    \"\"\"\n    return self._workspaces.get_workspace_home_id()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.get_workspace_id","title":"<code>get_workspace_id(index)</code>","text":"<p>Return the id of the workspace at the given position index in the list of workspaces.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>0-based index in the list of workspaces.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>id of the workspace at the given position index in the list of workspaces.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def get_workspace_id(self, index: int) -&gt; str:\n    \"\"\"\n    Return the id of the workspace at the given position index in the list of workspaces.\n\n    Args:\n        index: 0-based index in the list of workspaces.\n\n    Returns:\n        str: id of the workspace at the given position index in the list of workspaces.\n    \"\"\"\n    return self._workspaces.get_workspace_id(index)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.is_any_workspace_visible","title":"<code>is_any_workspace_visible()</code>","text":"<p>Check if any workspace is currently visible.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def is_any_workspace_visible(self) -&gt; bool:\n    \"\"\"Check if any workspace is currently visible.\"\"\"\n    pose = self.get_robot_pose()\n    return self.get_visible_workspace(pose) is not None\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.metrics","title":"<code>metrics()</code>","text":"<p>Get performance metrics tracker.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def metrics(self) -&gt; Optional[PerformanceMetrics]:\n    \"\"\"Get performance metrics tracker.\"\"\"\n    return self._metrics\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.oralcom_call_text2speech_async","title":"<code>oralcom_call_text2speech_async(text, priority=0)</code>","text":"<p>Asynchronously call text-to-speech API.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Message for text-to-speech</p> required <code>priority</code> <code>int</code> <p>Priority (0-10, higher = more urgent)</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if queued successfully (or dummy thread for compatibility)</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def oralcom_call_text2speech_async(self, text: str, priority: int = 0) -&gt; bool:\n    \"\"\"\n    Asynchronously call text-to-speech API.\n\n    Args:\n        text: Message for text-to-speech\n        priority: Priority (0-10, higher = more urgent)\n\n    Returns:\n        True if queued successfully (or dummy thread for compatibility)\n    \"\"\"\n    return self._oralcom.speak(text, priority=priority, blocking=False)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.print_performance_summary","title":"<code>print_performance_summary()</code>","text":"<p>Print a human-readable performance summary.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def print_performance_summary(self) -&gt; None:\n    \"\"\"Print a human-readable performance summary.\"\"\"\n    if self._metrics:\n        print(self._metrics.get_summary())\n    else:\n        print(\"Performance monitoring is disabled\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.remove_object_from_memory","title":"<code>remove_object_from_memory(object_label, coordinate)</code>","text":"<p>Remove an object from memory after manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>object_label</code> <code>str</code> <p>Label of the object to remove</p> required <code>coordinate</code> <code>List[float]</code> <p>Last known coordinate [x, y]</p> required Source code in <code>robot_environment/environment.py</code> <pre><code>def remove_object_from_memory(self, object_label: str, coordinate: List[float]) -&gt; None:\n    \"\"\"\n    Remove an object from memory after manipulation.\n\n    Args:\n        object_label: Label of the object to remove\n        coordinate: Last known coordinate [x, y]\n    \"\"\"\n    if not self._current_workspace_id:\n        self._logger.warning(\"No current workspace set\")\n        return\n\n    self._memory_manager.remove_object(\n        workspace_id=self._current_workspace_id, object_label=object_label, coordinate=coordinate\n    )\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.remove_object_from_workspace","title":"<code>remove_object_from_workspace(workspace_id, object_label, coordinate)</code>","text":"<p>Remove an object from a specific workspace's memory.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def remove_object_from_workspace(self, workspace_id: str, object_label: str, coordinate: list) -&gt; None:\n    \"\"\"Remove an object from a specific workspace's memory.\"\"\"\n    self._memory_manager.remove_object(workspace_id=workspace_id, object_label=object_label, coordinate=coordinate)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.reset_performance_metrics","title":"<code>reset_performance_metrics()</code>","text":"<p>Reset all performance metrics.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def reset_performance_metrics(self) -&gt; None:\n    \"\"\"Reset all performance metrics.\"\"\"\n    if self._metrics:\n        self._metrics.reset()\n        self._logger.info(\"Performance metrics reset\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.robot","title":"<code>robot()</code>","text":"<p>Return robot object.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def robot(self) -&gt; Robot:\n    \"\"\"Return robot object.\"\"\"\n    return self._robot\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.robot_move2home_observation_pose","title":"<code>robot_move2home_observation_pose()</code>","text":"<p>Move robot to home workspace observation pose.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def robot_move2home_observation_pose(self) -&gt; None:\n    \"\"\"Move robot to home workspace observation pose.\"\"\"\n    workspace_id = self.get_workspace_home_id()\n    self.robot_move2observation_pose(workspace_id)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.robot_move2observation_pose","title":"<code>robot_move2observation_pose(workspace_id)</code>","text":"<p>Move robot to observation pose for given workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of workspace</p> required Source code in <code>robot_environment/environment.py</code> <pre><code>def robot_move2observation_pose(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    Move robot to observation pose for given workspace.\n\n    Args:\n        workspace_id: ID of workspace\n    \"\"\"\n    if self._metrics:\n        with self._metrics.timer(\"robot_move_observation\"):\n            self._robot.move2observation_pose(workspace_id)\n    else:\n        self._robot.move2observation_pose(workspace_id)\n\n    self._current_workspace_id = workspace_id\n    self._logger.debug(f\"Set current workspace to: {workspace_id}\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.set_current_workspace","title":"<code>set_current_workspace(workspace_id)</code>","text":"<p>Set the current workspace being observed.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def set_current_workspace(self, workspace_id: str) -&gt; None:\n    \"\"\"Set the current workspace being observed.\"\"\"\n    self._current_workspace_id = workspace_id\n    self._logger.debug(f\"Current workspace set to: {workspace_id}\")\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.start_camera_updates","title":"<code>start_camera_updates(visualize=False)</code>","text":"<p>Start the background camera update thread.</p> <p>Parameters:</p> Name Type Description Default <code>visualize</code> <code>bool</code> <p>If True, show the camera feed (requires GUI).</p> <code>False</code> <p>Returns:</p> Type Description <code>Thread</code> <p>The started threading.Thread object.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def start_camera_updates(self, visualize: bool = False) -&gt; threading.Thread:\n    \"\"\"\n    Start the background camera update thread.\n\n    Args:\n        visualize: If True, show the camera feed (requires GUI).\n\n    Returns:\n        The started threading.Thread object.\n    \"\"\"\n\n    def loop():\n        for _ in self.update_camera_and_objects(visualize=visualize):\n            pass\n\n    t = threading.Thread(target=loop, daemon=True)\n    t.start()\n    return t\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.stop_camera_updates","title":"<code>stop_camera_updates()</code>","text":"<p>Stop camera update thread.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def stop_camera_updates(self) -&gt; None:\n    \"\"\"Stop camera update thread.\"\"\"\n    self._stop_event.set()\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.update_camera_and_objects","title":"<code>update_camera_and_objects(visualize=False)</code>","text":"<p>Continuously updates the camera and detected objects.</p> <p>Parameters:</p> Name Type Description Default <code>visualize</code> <code>bool</code> <p>If True, displays the updated camera feed</p> <code>False</code> Source code in <code>robot_environment/environment.py</code> <pre><code>def update_camera_and_objects(self, visualize: bool = False):\n    \"\"\"\n    Continuously updates the camera and detected objects.\n\n    Args:\n        visualize: If True, displays the updated camera feed\n    \"\"\"\n    # FIX: Get home workspace ID and set it as current\n    home_workspace_id = self._workspaces.get_workspace_home_id()\n\n    if self._current_workspace_id is None:\n        self._current_workspace_id = home_workspace_id  # Set before moving\n\n    self.robot_move2observation_pose(home_workspace_id)\n\n    while not self._stop_event.is_set():\n        loop_start = time.perf_counter()\n\n        # Get current frame with timing\n        if self._metrics:\n            with self._metrics.timer(\"frame_capture\"):\n                img = self.get_current_frame()\n        else:\n            img = self.get_current_frame()\n\n        time.sleep(0.1)\n\n        # Get detected objects from Redis\n        # Get detected objects from Redis with timing\n        if self._metrics:\n            with self._metrics.timer(\"object_fetch_redis\"):\n                detected_objects = self.get_detected_objects()\n\n            # Record object count\n            self._metrics.increment_counter(\"objects_detected\", len(detected_objects))\n        else:\n            detected_objects = self.get_detected_objects()\n\n        # Update memory using ObjectMemoryManager\n        if self._current_workspace_id:  # This should now always be True\n            at_observation = self.is_any_workspace_visible()\n            robot_moving = self.get_robot_in_motion()\n\n            if self._metrics:\n                mem_start = time.perf_counter()\n\n            objects_added, objects_updated = self._memory_manager.update(\n                workspace_id=self._current_workspace_id,\n                detected_objects=detected_objects,\n                at_observation_pose=at_observation,\n                robot_in_motion=robot_moving,\n            )\n\n            if self._metrics:\n                mem_duration = (time.perf_counter() - mem_start) * 1000\n                self._metrics.record_memory_update(mem_duration, objects_added, objects_updated)\n\n            self._logger.debug(\n                f\"Memory update for '{self._current_workspace_id}': \" f\"added={objects_added}, updated={objects_updated}\"\n            )\n        else:\n            # This should never happen now, but log it if it does\n            self._logger.error(\"Current workspace ID is None - memory not updated!\")\n\n        # Record loop iteration time\n        if self._metrics:\n            loop_duration = (time.perf_counter() - loop_start) * 1000\n            self._metrics.record_timing(\"camera_loop_iteration\", loop_duration)\n\n        # Log memory stats\n        if self._verbose:\n            stats = self._memory_manager.get_memory_stats()\n            if self._current_workspace_id in stats:\n                ws_stats = stats[self._current_workspace_id]\n                self._logger.debug(\n                    f\"Memory: {ws_stats['object_count']} objects, \"\n                    f\"manual_updates={ws_stats['manual_updates']}, \"\n                    f\"visible={ws_stats['visible']}\\n\"\n                )\n\n        yield img\n\n        if self.get_robot_in_motion():\n            time.sleep(0.05)\n        else:\n            time.sleep(0.05)\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.update_object_in_memory","title":"<code>update_object_in_memory(object_label, old_coordinate, new_pose)</code>","text":"<p>Update an object's position in memory after movement.</p> <p>Parameters:</p> Name Type Description Default <code>object_label</code> <code>str</code> <p>Label of the object</p> required <code>old_coordinate</code> <code>List[float]</code> <p>Previous coordinate [x, y]</p> required <code>new_pose</code> <code>'PoseObjectPNP'</code> <p>New pose after movement</p> required Source code in <code>robot_environment/environment.py</code> <pre><code>def update_object_in_memory(self, object_label: str, old_coordinate: List[float], new_pose: \"PoseObjectPNP\") -&gt; None:\n    \"\"\"\n    Update an object's position in memory after movement.\n\n    Args:\n        object_label: Label of the object\n        old_coordinate: Previous coordinate [x, y]\n        new_pose: New pose after movement\n    \"\"\"\n    if not self._current_workspace_id:\n        self._logger.warning(\"No current workspace set\")\n        return\n\n    self._memory_manager.mark_manual_update(\n        workspace_id=self._current_workspace_id,\n        object_label=object_label,\n        old_coordinate=old_coordinate,\n        new_pose=new_pose,\n    )\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.update_object_in_workspace","title":"<code>update_object_in_workspace(source_workspace_id, target_workspace_id, object_label, old_coordinate, new_coordinate)</code>","text":"<p>Move an object from one workspace to another in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_workspace_id</code> <code>str</code> <p>ID of workspace where object currently is</p> required <code>target_workspace_id</code> <code>str</code> <p>ID of workspace where object will be placed</p> required <code>object_label</code> <code>str</code> <p>Label of the object</p> required <code>old_coordinate</code> <code>list</code> <p>Current coordinate in source workspace</p> required <code>new_coordinate</code> <code>list</code> <p>New coordinate in target workspace</p> required Source code in <code>robot_environment/environment.py</code> <pre><code>def update_object_in_workspace(\n    self, source_workspace_id: str, target_workspace_id: str, object_label: str, old_coordinate: list, new_coordinate: list\n) -&gt; None:\n    \"\"\"\n    Move an object from one workspace to another in memory.\n\n    Args:\n        source_workspace_id: ID of workspace where object currently is\n        target_workspace_id: ID of workspace where object will be placed\n        object_label: Label of the object\n        old_coordinate: Current coordinate in source workspace\n        new_coordinate: New coordinate in target workspace\n    \"\"\"\n    self._memory_manager.move_object(\n        source_workspace_id=source_workspace_id,\n        target_workspace_id=target_workspace_id,\n        object_label=object_label,\n        old_coordinate=old_coordinate,\n        new_coordinate=new_coordinate,\n    )\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.use_simulation","title":"<code>use_simulation()</code>","text":"<p>Check if using simulation.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def use_simulation(self) -&gt; bool:\n    \"\"\"Check if using simulation.\"\"\"\n    return self._use_simulation\n</code></pre>"},{"location":"api/environment/#robot_environment.environment.Environment.workspaces","title":"<code>workspaces()</code>","text":"<p>Return workspaces object.</p> Source code in <code>robot_environment/environment.py</code> <pre><code>def workspaces(self) -&gt; Workspaces:\n    \"\"\"Return workspaces object.\"\"\"\n    return self._workspaces\n</code></pre>"},{"location":"api/robot/","title":"Robot","text":""},{"location":"api/robot/#robot_environment.robot.robot.Robot","title":"<code>robot_environment.robot.robot.Robot</code>","text":"<p>               Bases: <code>RobotAPI</code></p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>class Robot(RobotAPI):\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(\n        self, environment: \"Environment\", use_simulation: bool = False, robot_id: str = \"niryo\", verbose: bool = False\n    ):\n        \"\"\"\n        Creates robot object. Creates these objects:\n        - RobotController\n\n        Args:\n            environment:\n            use_simulation: if True, then simulate the robot, else the real robot is used\n            robot_id: string defining the robot. can be \"niryo\" or \"widowx\"\n            verbose:\n        \"\"\"\n        super().__init__()\n\n        self._environment = environment\n        self._verbose = verbose\n        self._object_last_picked = None\n\n        self._logger = get_package_logger(__name__, verbose)\n        self._logger.info(f\"Initializing robot: {robot_id}\")\n\n        if robot_id == \"niryo\":\n            self._robot = NiryoRobotController(self, use_simulation, verbose)\n        else:\n            self._robot = None\n\n    def handle_object_detection(self, objects_dict_list: List[Dict[str, Any]]) -&gt; None:\n        \"\"\"Process incoming object detections from Redis\"\"\"\n        # Convert dictionaries back to Object instances\n        objects = Objects.dict_list_to_objects(objects_dict_list, self.environment().get_workspace(0))\n\n        # Now work with Object instances as before\n        for obj in objects:\n            self._logger.debug(f\"Received object: {obj.label()} at {obj.xy_com()}\")\n\n    def get_pose(self) -&gt; PoseObjectPNP:\n        \"\"\"\n        Get current pose of gripper of robot.\n\n        Returns:\n            current pose of gripper of robot.\n        \"\"\"\n        return self._robot.get_pose()\n\n    @log_start_end_cls()\n    def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; PoseObjectPNP:\n        \"\"\"\n        Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n        calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n        given coordinate an object that has the given orientation. For this method to work, it is important that\n        only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n        this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n        Args:\n            workspace_id: id of the workspace\n            u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n            v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n            yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n        Returns:\n            pose_object: Pose of the point in world coordinates of the robot.\n        \"\"\"\n        self._logger.debug(f\"robot::get_target_pose_from_rel {workspace_id}, {u_rel}, {v_rel}, {yaw}\")\n\n        return self._robot.get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n\n    # *** PUBLIC methods ***\n\n    def calibrate(self) -&gt; bool:\n        \"\"\"\n        Calibrates the Robot.\n\n        Returns:\n            True, if calibration was successful, else False\n        \"\"\"\n        return self._robot.calibrate()\n\n    @log_start_end_cls()\n    def move2observation_pose(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        The robot will move to a pose where it can observe (the gripper hovers over) the workspace given by workspace_id.\n        Before a robot can pick up or place an object in a workspace, it must first move to this observation pose of the corresponding workspace.\n\n        Args:\n            workspace_id: id of the workspace\n\n        Returns:\n            None\n        \"\"\"\n        self._robot.move2observation_pose(workspace_id)\n\n    # TODO: the documentation of these pick methods is more upto date as teh one in robot_api\n\n    @log_start_end_cls()\n    def pick_place_object(\n        self,\n        object_name: str,\n        pick_coordinate: List[float],\n        place_coordinate: List[float],\n        location: Union[\"Location\", str, None] = None,\n        z_offset: float = 0.001,\n    ) -&gt; bool:\n        \"\"\"\n        Instructs the pick-and-place robot arm to pick a specific object and place it using its gripper.\n        The gripper will move to the specified 'pick_coordinate' and pick the named object. It will then move to the\n        specified 'place_coordinate' and place the object there. If you need to pick-and-place an object, call this\n        function and not robot_pick_object() followed by robot_place_object().\n\n        Example call:\n\n        robot.pick_place_object(\n            object_name='chocolate bar',\n            pick_coordinate=[-0.1, 0.01],\n            place_coordinate=[0.1, 0.11],\n            location=Location.RIGHT_NEXT_TO\n        )\n        --&gt; Picks the chocolate bar that is located at world coordinates [-0.1, 0.01] and places it right next to an\n        object that exists at world coordinate [0.1, 0.11].\n\n        robot.pick_place_object(\n            object_name='cube',\n            pick_coordinate=[0.2, 0.05],\n            place_coordinate=[0.3, 0.1],\n            location=Location.ON_TOP_OF,\n            z_offset=0.02\n        )\n        --&gt; Picks the cube with a 2cm z-offset (useful if it's on top of another object).\n\n        Args:\n            object_name (str): The name of the object to be picked up. Ensure this name matches an object visible in\n            the robot's workspace.\n            pick_coordinate (List): The world coordinates [x, y] where the object should be picked up. Use these\n            coordinates to identify the object's exact position.\n            place_coordinate (List): The world coordinates [x, y] where the object should be placed at.\n            location (Location): Specifies the relative placement position of the picked object with respect to an object\n            being at the 'place_coordinate'. Possible values are defined in the `Location` Enum:\n                - `Location.LEFT_NEXT_TO`: Left of the reference object.\n                - `Location.RIGHT_NEXT_TO`: Right of the reference object.\n                - `Location.ABOVE`: Above the reference object.\n                - `Location.BELOW`: Below the reference object.\n                - `Location.ON_TOP_OF`: On top of the reference object.\n                - `Location.INSIDE`: Inside the reference object.\n                - `Location.NONE`: No specific location relative to another object.\n            z_offset (float): Additional height offset in meters to apply when picking (default: 0.001).\n            Useful for picking objects that are stacked on top of other objects.\n\n        Returns:\n            bool: True if successful\n        \"\"\"\n        success = self.pick_object(object_name, pick_coordinate, z_offset=z_offset)\n\n        if success:\n            place_success = self.place_object(place_coordinate, location)\n            return place_success\n        else:\n            return False\n\n    @log_start_end_cls()\n    def pick_object(self, object_name: str, pick_coordinate: List[float], z_offset: float = 0.001) -&gt; bool:\n        \"\"\"\n        Command the pick-and-place robot arm to pick up a specific object using its gripper. The gripper will move to\n        the specified 'pick_coordinate' and pick the named object.\n\n        Example call:\n\n        robot.pick_object(\"pen\", [0.01, -0.15])\n        --&gt; Picks the pen that is located at world coordinates [0.01, -0.15].\n\n        robot.pick_object(\"pen\", [0.01, -0.15], z_offset=0.02)\n        --&gt; Picks the pen with a 2cm offset above its detected position (useful for stacked objects).\n\n        Args:\n            object_name (str): The name of the object to be picked up. Ensure this name matches an object visible in\n            the robot's workspace.\n            pick_coordinate (List): The world coordinates [x, y] where the object should be picked up. Use these\n            coordinates to identify the object's exact position.\n            z_offset (float): Additional height offset in meters to apply when picking (default: 0.001).\n            Useful for picking objects that are stacked on top of other objects.\n        Returns:\n            bool: True\n        \"\"\"\n        coords_str = \"[\" + \", \".join(f\"{x:.2f}\" for x in pick_coordinate) + \"]\"\n        message = f\"Going to pick {object_name} at coordinate {coords_str}.\"\n        self._logger.info(message)\n\n        self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n        obj_to_pick = self._get_nearest_object(object_name, pick_coordinate)\n\n        if obj_to_pick:\n            self._object_last_picked = obj_to_pick\n\n            # Apply z_offset to the pick pose\n            pick_pose = obj_to_pick.pose_com()\n            pick_pose = pick_pose.copy_with_offsets(z_offset=z_offset)\n\n            success = self._robot.robot_pick_object(pick_pose)\n        else:\n            success = False\n\n        # thread_oral.join()\n\n        return success\n\n    @log_start_end_cls()\n    def place_object(self, place_coordinate: List[float], location: Union[\"Location\", str, None] = None) -&gt; bool:\n        \"\"\"\n        Instruct the pick-and-place robot arm to place a picked object at the specified 'place_coordinate'. The\n        function moves the gripper to the specified 'place_coordinate' and calculates the exact placement position from\n        the given 'location'. Before calling this function you have to call robot_pick_object() to pick an object.\n\n        Example call:\n\n        robot.place_object([0.2, 0.0], \"left next to\")\n        --&gt; Places the already gripped object left next to the world coordinate [0.2, 0.0].\n\n        Args:\n            place_coordinate: The world coordinates [x, y] of the target object.\n            location (str): Specifies the relative placement position of the picked object in relation to an object\n            being at the 'place_coordinate'. Possible positions: 'left next to', 'right next to', 'above', 'below',\n            'on top of', 'inside', or None. Set to None, if there is no location given in the task.\n        Returns:\n            bool: True\n        \"\"\"\n        location = Location.convert_str2location(location)\n\n        if self._object_last_picked:\n            old_coordinate = [self._object_last_picked.x_com(), self._object_last_picked.y_com()]\n            message = (\n                f\"Going to place {self._object_last_picked.label()} {location} coordinate [\"\n                f\"{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n            )\n        else:\n            old_coordinate = None\n            message = f\"Going to place it {location} coordinate [{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n\n        self._logger.info(message)\n\n        self.environment().oralcom_call_text2speech_async(message, priority=8)\n        obj_where_to_place = None\n\n        if location is not None and location is not Location.NONE:\n            obj_where_to_place = self._get_nearest_object(None, place_coordinate)\n            if obj_where_to_place is None:\n                place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n            else:\n                place_pose = obj_where_to_place.pose_center()\n        else:\n            place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n            self._logger.debug(f\"place_object: {place_pose}\")\n\n        x_off = 0.02\n        y_off = 0.02\n\n        if self._object_last_picked:\n            x_off += self._object_last_picked.height_m() / 2\n            y_off += self._object_last_picked.width_m() / 2\n\n        if place_pose:\n            # TODO: use height of object instead\n            if location == Location.ON_TOP_OF:\n                place_pose.z += 0.02\n            elif location == Location.INSIDE:\n                place_pose.z += 0.01\n            elif location == Location.RIGHT_NEXT_TO:\n                place_pose.y -= obj_where_to_place.width_m() / 2 + y_off\n            elif location == Location.LEFT_NEXT_TO:\n                place_pose.y += obj_where_to_place.width_m() / 2 + y_off\n            elif location == Location.BELOW:\n                # print(obj_where_to_place.height_m(), self._object_last_picked.width_m(), x_off)\n                # TODO: nutze hier auch width, da width immer die gr\u00f6\u00dfere gr\u00f6\u00dfe ist und nicht eine koordinatenrichtugn hat\n                #  ich muss anstatt width und height eine gr\u00f6\u00dfe haben dim_x und dim_y, die a x und y koordinate gebunden sind\n                #  ich habe das in object klasse repariert, width geht immer entlang y-achse jetzt. pr\u00fcfen hier\n                place_pose.x -= obj_where_to_place.height_m() / 2 + x_off\n            elif location == Location.ABOVE:\n                # TODO: nutze hier auch width, da width immer die gr\u00f6\u00dfere gr\u00f6\u00dfe ist und nicht eine koordinatenrichtugn hat\n                #  ich habe das in object klasse repariert, width geht immer entlang y-achse jetzt. pr\u00fcfen hier\n                print(obj_where_to_place.height_m(), self._object_last_picked.width_m(), x_off)\n                place_pose.x += obj_where_to_place.height_m() / 2 + x_off\n                self._logger.debug(f\"{place_pose}\")\n            elif location is Location.NONE or location is None:\n                pass  # I do not have to do anything as the given location is where to place the object\n            else:\n                self._logger.error(f\"Unknown location: {location} (type: {type(location)})\")\n\n            success = self._robot.robot_place_object(place_pose)\n\n            # update position of placed object to the new position\n            # Update memory after successful placement\n            if success and self._object_last_picked and old_coordinate:\n                final_coordinate = [place_pose.x, place_pose.y]\n                self._logger.debug(f\"final_coordinate: {final_coordinate}\")\n\n                self.environment().update_object_in_memory(\n                    self._object_last_picked.label(), old_coordinate, new_pose=place_pose\n                )\n\n                # Give the memory system a moment to register the update\n                import time\n\n                time.sleep(0.1)\n        else:\n            success = False\n\n        self._object_last_picked = None\n\n        # thread_oral.join()\n\n        return success\n\n    @log_start_end_cls()\n    def push_object(self, object_name: str, push_coordinate: List[float], direction: str, distance: float) -&gt; bool:\n        \"\"\"\n        Instruct the pick-and-place robot arm to push a specific object to a new position.\n        This function should only be called if it is not possible to pick the object.\n        An object cannot be picked if its shorter side is larger than the gripper.\n\n        Args:\n            object_name (str): The name of the object to be pushed.\n            Ensure the name matches an object in the robot's environment.\n            push_coordinate: The world coordinates [x, y] where the object to push is located.\n            These coordinates indicate the initial position of the object.\n            direction (str): The direction in which to push the object.\n            Valid options are: \"up\", \"down\", \"left\", \"right\".\n            distance: The distance (in millimeters) to push the object in the specified direction.\n            Ensure the value is within the robot's operating range.\n\n        Returns:\n            bool: True\n        \"\"\"\n        message = f\"Calling push with {object_name} and {direction}\"\n        self._logger.info(message)\n\n        self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n        obj_to_push = self._get_nearest_object(object_name, push_coordinate)\n\n        push_pose = obj_to_push.pose_com()\n\n        # it is certainly better when pushing up to move under the object with a closed gripper so we can\n        #  actually push up. same for the other directions.\n        if direction == \"up\":\n            push_pose.x -= obj_to_push.height_m() / 2.0\n            # gripper 90\u00b0 rotated. TODO: I have to test these orientations\n            push_pose.yaw = math.pi / 2.0\n        elif direction == \"down\":\n            push_pose.x += obj_to_push.height_m() / 2.0\n            # gripper 90\u00b0 rotated. TODO: I have to test these orientations\n            push_pose.yaw = math.pi / 2.0\n        elif direction == \"left\":\n            push_pose.y += obj_to_push.width_m() / 2.0\n            # gripper 0\u00b0 rotated. TODO: I have to test these orientations\n            push_pose.yaw = 0.0\n        elif direction == \"right\":\n            push_pose.y -= obj_to_push.width_m() / 2.0\n            # gripper 0\u00b0 rotated. TODO: I have to test these orientations\n            push_pose.yaw = 0.0\n        else:\n            self._logger.error(f\"Unknown direction: {direction}\")\n\n        if obj_to_push is not None:\n            success = self._robot.robot_push_object(push_pose, direction, distance)\n        else:\n            success = False\n\n        # thread_oral.join()\n\n        return success\n\n    def pick_place_object_across_workspaces(\n        self,\n        object_name: str,\n        pick_workspace_id: str,\n        pick_coordinate: List[float],\n        place_workspace_id: str,\n        place_coordinate: List[float],\n        location: Union[\"Location\", str, None] = None,\n        z_offset: float = 0.001,\n    ) -&gt; bool:\n        \"\"\"\n        Pick an object from one workspace and place it in another workspace.\n\n        Args:\n            object_name: Name of the object to pick\n            pick_workspace_id: ID of the workspace to pick from\n            pick_coordinate: [x, y] coordinate in pick workspace\n            place_workspace_id: ID of the workspace to place in\n            place_coordinate: [x, y] coordinate in place workspace\n            location: Relative placement location (Location enum or string)\n            z_offset: Additional height offset in meters when picking (default: 0.001)\n\n        Returns:\n            bool: True if successful, False otherwise\n\n        Example:\n            robot.pick_place_object_across_workspaces(\n                object_name='cube',\n                pick_workspace_id='niryo_ws_left',\n                pick_coordinate=[0.2, 0.05],\n                place_workspace_id='niryo_ws_right',\n                place_coordinate=[0.25, -0.05],\n                location=Location.RIGHT_NEXT_TO,\n                z_offset=0.02\n            )\n        \"\"\"\n        self._logger.debug(f\"Multi-workspace operation: {object_name}\")\n        self._logger.debug(f\"  Pick from: {pick_workspace_id} at {pick_coordinate}\")\n        self._logger.debug(f\"  Place in: {place_workspace_id} at {place_coordinate}\")\n\n        # Step 1: Move to pick workspace observation pose\n        self.move2observation_pose(pick_workspace_id)\n        self.environment()._current_workspace_id = pick_workspace_id\n\n        # Step 2: Pick the object\n        success = self.pick_object_from_workspace(object_name, pick_workspace_id, pick_coordinate, z_offset=z_offset)\n\n        if not success:\n            self._logger.error(f\"Failed to pick {object_name} from {pick_workspace_id}\")\n            return False\n\n        # Step 3: Move to place workspace observation pose\n        self.move2observation_pose(place_workspace_id)\n        self.environment()._current_workspace_id = place_workspace_id\n\n        # Step 4: Place the object\n        place_success = self.place_object_in_workspace(place_workspace_id, place_coordinate, location)\n\n        if place_success:\n            # Update memory: remove from source, add to target\n            self.environment().update_object_in_workspace(\n                source_workspace_id=pick_workspace_id,\n                target_workspace_id=place_workspace_id,\n                object_label=object_name,\n                old_coordinate=pick_coordinate,\n                new_coordinate=place_coordinate,\n            )\n\n            self._logger.info(f\"Successfully moved {object_name} from {pick_workspace_id} to {place_workspace_id}\")\n\n        return place_success\n\n    def pick_object_from_workspace(\n        self, object_name: str, workspace_id: str, pick_coordinate: List[float], z_offset: float = 0.001\n    ) -&gt; bool:\n        \"\"\"\n        Pick an object from a specific workspace.\n\n        Args:\n            object_name: Name of the object to pick\n            workspace_id: ID of the workspace\n            pick_coordinate: [x, y] coordinate in workspace\n            z_offset: Additional height offset in meters (default: 0.001)\n\n        Returns:\n            bool: True if successful\n        \"\"\"\n        coords_str = \"[\" + \", \".join(f\"{x:.2f}\" for x in pick_coordinate) + \"]\"\n        message = f\"Picking {object_name} from workspace {workspace_id} at {coords_str}.\"\n        self._logger.info(message)\n\n        self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n        # Get object from specific workspace memory\n        obj_to_pick = self._get_nearest_object_in_workspace(object_name, workspace_id, pick_coordinate)\n\n        if obj_to_pick:\n            self._object_last_picked = obj_to_pick\n            self._object_source_workspace = workspace_id\n\n            # Apply z_offset to the pick pose\n            pick_pose = obj_to_pick.pose_com()\n            pick_pose = pick_pose.copy_with_offsets(z_offset=z_offset)\n\n            success = self._robot.robot_pick_object(pick_pose)\n        else:\n            success = False\n\n        # thread_oral.join()\n        return success\n\n    def place_object_in_workspace(\n        self, workspace_id: str, place_coordinate: List[float], location: Union[\"Location\", str, None] = None\n    ) -&gt; bool:\n        \"\"\"\n        Place a picked object in a specific workspace.\n\n        Args:\n            workspace_id: ID of the target workspace\n            place_coordinate: [x, y] coordinate in workspace\n            location: Relative placement location\n\n        Returns:\n            bool: True if successful\n        \"\"\"\n        location = Location.convert_str2location(location)\n\n        if self._object_last_picked:\n            message = (\n                f\"Placing {self._object_last_picked.label()} in workspace \"\n                f\"{workspace_id} {location} coordinate \"\n                f\"[{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n            )\n        else:\n            message = (\n                f\"Placing object in workspace {workspace_id} {location} \"\n                f\"coordinate [{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n            )\n\n        self._logger.info(message)\n        self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n        # Get workspace for coordinate transformation\n        workspace = self.environment().get_workspace_by_id(workspace_id)\n        if workspace is None:\n            self._logger.error(f\"Workspace {workspace_id} not found\")\n            # thread_oral.join()\n            return False\n\n        # Find reference object in target workspace if location specified\n        obj_where_to_place = None\n        if location is not None and location is not Location.NONE:\n            obj_where_to_place = self._get_nearest_object_in_workspace(None, workspace_id, place_coordinate)\n\n            if obj_where_to_place is None:\n                place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n            else:\n                place_pose = obj_where_to_place.pose_center()\n        else:\n            place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n\n        # Calculate placement offset based on location\n        if place_pose and obj_where_to_place:\n            x_off = 0.02\n            y_off = 0.02\n\n            if self._object_last_picked:\n                x_off += self._object_last_picked.height_m() / 2\n                y_off += self._object_last_picked.width_m() / 2\n\n            if location == Location.ON_TOP_OF:\n                place_pose.z += 0.02\n            elif location == Location.INSIDE:\n                place_pose.z += 0.01\n            elif location == Location.RIGHT_NEXT_TO:\n                place_pose.y -= obj_where_to_place.width_m() / 2 + y_off\n            elif location == Location.LEFT_NEXT_TO:\n                place_pose.y += obj_where_to_place.width_m() / 2 + y_off\n            elif location == Location.BELOW:\n                place_pose.x -= obj_where_to_place.height_m() / 2 + x_off\n            elif location == Location.ABOVE:\n                place_pose.x += obj_where_to_place.height_m() / 2 + x_off\n\n        success = self._robot.robot_place_object(place_pose)\n\n        # Clear last picked object\n        self._object_last_picked = None\n        if \"_object_source_workspace\" in self.__dict__:\n            del self._object_source_workspace\n\n        # thread_oral.join()\n        return success\n\n    def _get_nearest_object_in_workspace(\n        self, label: Union[str, None], workspace_id: str, target_coords: List[float]\n    ) -&gt; Optional[Object]:\n        \"\"\"\n        Find the nearest object in a specific workspace.\n\n        Args:\n            label: Object label to search for (None for any object)\n            workspace_id: ID of the workspace\n            target_coords: Target coordinates [x, y]\n\n        Returns:\n            Object or None\n        \"\"\"\n        # Get objects from specific workspace memory\n        detected_objects = self.environment().get_detected_objects_from_workspace(workspace_id)\n\n        self._logger.debug(f\"Objects in workspace {workspace_id}: {detected_objects}\")\n\n        if len(target_coords) == 0:\n            nearest_object = next((obj for obj in detected_objects if obj.label() == label), None)\n            min_distance = 0\n        else:\n            nearest_object, min_distance = detected_objects.get_nearest_detected_object(target_coords, label)\n\n        if nearest_object:\n            self._logger.debug(f\"Found {nearest_object.label()} at distance {min_distance:.3f}m\")\n        else:\n            self._logger.warning(f\"Object {label} not found in workspace {workspace_id}\")\n            self._logger.info(f\"Available objects: \" f\"{detected_objects.get_detected_objects_as_comma_separated_string()}\")\n\n        return nearest_object\n\n    @staticmethod\n    def _parse_command(line: str) -&gt; Tuple[Optional[str], Optional[str], List[Any], Dict[str, Any]]:\n        \"\"\"\n        Parse a single line of the input into the target object, method, positional arguments, and keyword arguments.\n        Deprecated: Use robot_environment.robot.command_processor.parse_robot_command instead.\n        \"\"\"\n        return parse_robot_command(line)\n\n    def get_detected_objects(self):\n        \"\"\"Get latest detected objects from memory.\"\"\"\n        latest_objects = self._environment.get_detected_objects_from_memory()\n        return latest_objects\n\n    def _get_nearest_object(self, label: Union[str, None], target_coords: List[float]) -&gt; Optional[Object]:\n        \"\"\"\n        Find the nearest object with the specified label.\n\n        Args:\n            label:\n            target_coords:\n\n        Returns:\n            object:\n        \"\"\"\n        detected_objects = self.get_detected_objects()\n\n        self._logger.debug(f\"detected_objects: {detected_objects}\")\n\n        if len(target_coords) == 0:  # then no target coords are given, true for push method\n            nearest_object = next((obj for obj in detected_objects if obj.label() == label), None)\n            min_distance = 0\n        else:\n            nearest_object, min_distance = detected_objects.get_nearest_detected_object(target_coords, label)\n\n        if nearest_object:\n            self._logger.debug(f\"Nearest object found: {nearest_object} with distance {min_distance}\")\n        else:\n            self._logger.warning(\n                f\"Object {label} does not exist: \" f\"{detected_objects.get_detected_objects_as_comma_separated_string()}\"\n            )\n\n            # add functionality that looks for the most similar object in self.get_detected_objects()\n            # and ask user whether this object should be used instead. if answer of user is yes, then set\n            # nearest_object to this new object\n            # TODO: get_most_similar_object wieder nutzen\n            #  nearest_object_name = self.environment().get_most_similar_object(label)\n            nearest_object_name = None\n\n            if nearest_object_name is not None:\n                self._logger.info(\n                    f\"I have detected the object {nearest_object_name}. Do you want to handle this object instead?\"\n                )\n\n                # TODO: auf antwort von user warten und diese pr\u00fcfen.\n                answer = \"yes\"\n\n                if answer != \"yes\":\n                    return None\n                else:\n                    nearest_object = next((obj for obj in detected_objects if obj.label() == nearest_object_name), None)\n\n        return nearest_object\n\n    # *** PUBLIC properties ***\n\n    def environment(self) -&gt; Environment:\n        return self._environment\n\n    def robot_in_motion(self) -&gt; bool:\n        \"\"\"\n        :return: value of _robot_in_motion:\n        False: robot is not in motion\n        True: robot is in motion and therefore maybe cannot see the workspace markers\n        \"\"\"\n        return self._robot.is_in_motion()\n\n    def robot(self) -&gt; RobotController:\n        \"\"\"\n        Returns:\n            RobotController: object that controls the robot.\n        \"\"\"\n        return self._robot\n\n    def verbose(self) -&gt; bool:\n        \"\"\"\n        Returns: True, if verbose is on, else False\n        \"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    _environment = None\n\n    # RobotController object\n    _robot = None\n\n    # True, if robot is in motion and therefore cannot see the workspace markers\n    # _robot_in_motion = False\n\n    _object_source_workspace: Optional[str] = None  # Track source workspace\n\n    _verbose = False\n    _logger = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot-functions","title":"Functions","text":""},{"location":"api/robot/#robot_environment.robot.robot.Robot.__init__","title":"<code>__init__(environment, use_simulation=False, robot_id='niryo', verbose=False)</code>","text":"<p>Creates robot object. Creates these objects: - RobotController</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>'Environment'</code> required <code>use_simulation</code> <code>bool</code> <p>if True, then simulate the robot, else the real robot is used</p> <code>False</code> <code>robot_id</code> <code>str</code> <p>string defining the robot. can be \"niryo\" or \"widowx\"</p> <code>'niryo'</code> <code>verbose</code> <code>bool</code> <code>False</code> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef __init__(\n    self, environment: \"Environment\", use_simulation: bool = False, robot_id: str = \"niryo\", verbose: bool = False\n):\n    \"\"\"\n    Creates robot object. Creates these objects:\n    - RobotController\n\n    Args:\n        environment:\n        use_simulation: if True, then simulate the robot, else the real robot is used\n        robot_id: string defining the robot. can be \"niryo\" or \"widowx\"\n        verbose:\n    \"\"\"\n    super().__init__()\n\n    self._environment = environment\n    self._verbose = verbose\n    self._object_last_picked = None\n\n    self._logger = get_package_logger(__name__, verbose)\n    self._logger.info(f\"Initializing robot: {robot_id}\")\n\n    if robot_id == \"niryo\":\n        self._robot = NiryoRobotController(self, use_simulation, verbose)\n    else:\n        self._robot = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.calibrate","title":"<code>calibrate()</code>","text":"<p>Calibrates the Robot.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if calibration was successful, else False</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def calibrate(self) -&gt; bool:\n    \"\"\"\n    Calibrates the Robot.\n\n    Returns:\n        True, if calibration was successful, else False\n    \"\"\"\n    return self._robot.calibrate()\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.get_detected_objects","title":"<code>get_detected_objects()</code>","text":"<p>Get latest detected objects from memory.</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def get_detected_objects(self):\n    \"\"\"Get latest detected objects from memory.\"\"\"\n    latest_objects = self._environment.get_detected_objects_from_memory()\n    return latest_objects\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.get_pose","title":"<code>get_pose()</code>","text":"<p>Get current pose of gripper of robot.</p> <p>Returns:</p> Type Description <code>PoseObjectPNP</code> <p>current pose of gripper of robot.</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def get_pose(self) -&gt; PoseObjectPNP:\n    \"\"\"\n    Get current pose of gripper of robot.\n\n    Returns:\n        current pose of gripper of robot.\n    \"\"\"\n    return self._robot.get_pose()\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.get_target_pose_from_rel","title":"<code>get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code>","text":"<p>Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw), calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the given coordinate an object that has the given orientation. For this method to work, it is important that only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <code>u_rel</code> <code>float</code> <p>horizontal coordinate in image of workspace, normalized between 0 and 1</p> required <code>v_rel</code> <code>float</code> <p>vertical coordinate in image of workspace, normalized between 0 and 1</p> required <code>yaw</code> <code>float</code> <p>orientation of an object at the pixel coordinates [u_rel, v_rel].</p> required <p>Returns:</p> Name Type Description <code>pose_object</code> <code>PoseObjectPNP</code> <p>Pose of the point in world coordinates of the robot.</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; PoseObjectPNP:\n    \"\"\"\n    Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n    calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n    given coordinate an object that has the given orientation. For this method to work, it is important that\n    only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n    this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n    Args:\n        workspace_id: id of the workspace\n        u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n        v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n        yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n    Returns:\n        pose_object: Pose of the point in world coordinates of the robot.\n    \"\"\"\n    self._logger.debug(f\"robot::get_target_pose_from_rel {workspace_id}, {u_rel}, {v_rel}, {yaw}\")\n\n    return self._robot.get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.handle_object_detection","title":"<code>handle_object_detection(objects_dict_list)</code>","text":"<p>Process incoming object detections from Redis</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def handle_object_detection(self, objects_dict_list: List[Dict[str, Any]]) -&gt; None:\n    \"\"\"Process incoming object detections from Redis\"\"\"\n    # Convert dictionaries back to Object instances\n    objects = Objects.dict_list_to_objects(objects_dict_list, self.environment().get_workspace(0))\n\n    # Now work with Object instances as before\n    for obj in objects:\n        self._logger.debug(f\"Received object: {obj.label()} at {obj.xy_com()}\")\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.move2observation_pose","title":"<code>move2observation_pose(workspace_id)</code>","text":"<p>The robot will move to a pose where it can observe (the gripper hovers over) the workspace given by workspace_id. Before a robot can pick up or place an object in a workspace, it must first move to this observation pose of the corresponding workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef move2observation_pose(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    The robot will move to a pose where it can observe (the gripper hovers over) the workspace given by workspace_id.\n    Before a robot can pick up or place an object in a workspace, it must first move to this observation pose of the corresponding workspace.\n\n    Args:\n        workspace_id: id of the workspace\n\n    Returns:\n        None\n    \"\"\"\n    self._robot.move2observation_pose(workspace_id)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.pick_object","title":"<code>pick_object(object_name, pick_coordinate, z_offset=0.001)</code>","text":"<p>Command the pick-and-place robot arm to pick up a specific object using its gripper. The gripper will move to the specified 'pick_coordinate' and pick the named object.</p> <p>Example call:</p> <p>robot.pick_object(\"pen\", [0.01, -0.15]) --&gt; Picks the pen that is located at world coordinates [0.01, -0.15].</p> <p>robot.pick_object(\"pen\", [0.01, -0.15], z_offset=0.02) --&gt; Picks the pen with a 2cm offset above its detected position (useful for stacked objects).</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>The name of the object to be picked up. Ensure this name matches an object visible in</p> required <code>pick_coordinate</code> <code>List</code> <p>The world coordinates [x, y] where the object should be picked up. Use these</p> required <code>z_offset</code> <code>float</code> <p>Additional height offset in meters to apply when picking (default: 0.001).</p> <code>0.001</code> <p>Returns:     bool: True</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef pick_object(self, object_name: str, pick_coordinate: List[float], z_offset: float = 0.001) -&gt; bool:\n    \"\"\"\n    Command the pick-and-place robot arm to pick up a specific object using its gripper. The gripper will move to\n    the specified 'pick_coordinate' and pick the named object.\n\n    Example call:\n\n    robot.pick_object(\"pen\", [0.01, -0.15])\n    --&gt; Picks the pen that is located at world coordinates [0.01, -0.15].\n\n    robot.pick_object(\"pen\", [0.01, -0.15], z_offset=0.02)\n    --&gt; Picks the pen with a 2cm offset above its detected position (useful for stacked objects).\n\n    Args:\n        object_name (str): The name of the object to be picked up. Ensure this name matches an object visible in\n        the robot's workspace.\n        pick_coordinate (List): The world coordinates [x, y] where the object should be picked up. Use these\n        coordinates to identify the object's exact position.\n        z_offset (float): Additional height offset in meters to apply when picking (default: 0.001).\n        Useful for picking objects that are stacked on top of other objects.\n    Returns:\n        bool: True\n    \"\"\"\n    coords_str = \"[\" + \", \".join(f\"{x:.2f}\" for x in pick_coordinate) + \"]\"\n    message = f\"Going to pick {object_name} at coordinate {coords_str}.\"\n    self._logger.info(message)\n\n    self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n    obj_to_pick = self._get_nearest_object(object_name, pick_coordinate)\n\n    if obj_to_pick:\n        self._object_last_picked = obj_to_pick\n\n        # Apply z_offset to the pick pose\n        pick_pose = obj_to_pick.pose_com()\n        pick_pose = pick_pose.copy_with_offsets(z_offset=z_offset)\n\n        success = self._robot.robot_pick_object(pick_pose)\n    else:\n        success = False\n\n    # thread_oral.join()\n\n    return success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.pick_object_from_workspace","title":"<code>pick_object_from_workspace(object_name, workspace_id, pick_coordinate, z_offset=0.001)</code>","text":"<p>Pick an object from a specific workspace.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>Name of the object to pick</p> required <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <code>pick_coordinate</code> <code>List[float]</code> <p>[x, y] coordinate in workspace</p> required <code>z_offset</code> <code>float</code> <p>Additional height offset in meters (default: 0.001)</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def pick_object_from_workspace(\n    self, object_name: str, workspace_id: str, pick_coordinate: List[float], z_offset: float = 0.001\n) -&gt; bool:\n    \"\"\"\n    Pick an object from a specific workspace.\n\n    Args:\n        object_name: Name of the object to pick\n        workspace_id: ID of the workspace\n        pick_coordinate: [x, y] coordinate in workspace\n        z_offset: Additional height offset in meters (default: 0.001)\n\n    Returns:\n        bool: True if successful\n    \"\"\"\n    coords_str = \"[\" + \", \".join(f\"{x:.2f}\" for x in pick_coordinate) + \"]\"\n    message = f\"Picking {object_name} from workspace {workspace_id} at {coords_str}.\"\n    self._logger.info(message)\n\n    self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n    # Get object from specific workspace memory\n    obj_to_pick = self._get_nearest_object_in_workspace(object_name, workspace_id, pick_coordinate)\n\n    if obj_to_pick:\n        self._object_last_picked = obj_to_pick\n        self._object_source_workspace = workspace_id\n\n        # Apply z_offset to the pick pose\n        pick_pose = obj_to_pick.pose_com()\n        pick_pose = pick_pose.copy_with_offsets(z_offset=z_offset)\n\n        success = self._robot.robot_pick_object(pick_pose)\n    else:\n        success = False\n\n    # thread_oral.join()\n    return success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.pick_place_object","title":"<code>pick_place_object(object_name, pick_coordinate, place_coordinate, location=None, z_offset=0.001)</code>","text":"<p>Instructs the pick-and-place robot arm to pick a specific object and place it using its gripper. The gripper will move to the specified 'pick_coordinate' and pick the named object. It will then move to the specified 'place_coordinate' and place the object there. If you need to pick-and-place an object, call this function and not robot_pick_object() followed by robot_place_object().</p> <p>Example call:</p> <p>robot.pick_place_object(     object_name='chocolate bar',     pick_coordinate=[-0.1, 0.01],     place_coordinate=[0.1, 0.11],     location=Location.RIGHT_NEXT_TO ) --&gt; Picks the chocolate bar that is located at world coordinates [-0.1, 0.01] and places it right next to an object that exists at world coordinate [0.1, 0.11].</p> <p>robot.pick_place_object(     object_name='cube',     pick_coordinate=[0.2, 0.05],     place_coordinate=[0.3, 0.1],     location=Location.ON_TOP_OF,     z_offset=0.02 ) --&gt; Picks the cube with a 2cm z-offset (useful if it's on top of another object).</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>The name of the object to be picked up. Ensure this name matches an object visible in</p> required <code>pick_coordinate</code> <code>List</code> <p>The world coordinates [x, y] where the object should be picked up. Use these</p> required <code>place_coordinate</code> <code>List</code> <p>The world coordinates [x, y] where the object should be placed at.</p> required <code>location</code> <code>Location</code> <p>Specifies the relative placement position of the picked object with respect to an object</p> <code>None</code> <code>being at the 'place_coordinate'. Possible values are defined in the `Location` Enum</code> <ul> <li><code>Location.LEFT_NEXT_TO</code>: Left of the reference object.</li> <li><code>Location.RIGHT_NEXT_TO</code>: Right of the reference object.</li> <li><code>Location.ABOVE</code>: Above the reference object.</li> <li><code>Location.BELOW</code>: Below the reference object.</li> <li><code>Location.ON_TOP_OF</code>: On top of the reference object.</li> <li><code>Location.INSIDE</code>: Inside the reference object.</li> <li><code>Location.NONE</code>: No specific location relative to another object.</li> </ul> required <code>z_offset</code> <code>float</code> <p>Additional height offset in meters to apply when picking (default: 0.001).</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef pick_place_object(\n    self,\n    object_name: str,\n    pick_coordinate: List[float],\n    place_coordinate: List[float],\n    location: Union[\"Location\", str, None] = None,\n    z_offset: float = 0.001,\n) -&gt; bool:\n    \"\"\"\n    Instructs the pick-and-place robot arm to pick a specific object and place it using its gripper.\n    The gripper will move to the specified 'pick_coordinate' and pick the named object. It will then move to the\n    specified 'place_coordinate' and place the object there. If you need to pick-and-place an object, call this\n    function and not robot_pick_object() followed by robot_place_object().\n\n    Example call:\n\n    robot.pick_place_object(\n        object_name='chocolate bar',\n        pick_coordinate=[-0.1, 0.01],\n        place_coordinate=[0.1, 0.11],\n        location=Location.RIGHT_NEXT_TO\n    )\n    --&gt; Picks the chocolate bar that is located at world coordinates [-0.1, 0.01] and places it right next to an\n    object that exists at world coordinate [0.1, 0.11].\n\n    robot.pick_place_object(\n        object_name='cube',\n        pick_coordinate=[0.2, 0.05],\n        place_coordinate=[0.3, 0.1],\n        location=Location.ON_TOP_OF,\n        z_offset=0.02\n    )\n    --&gt; Picks the cube with a 2cm z-offset (useful if it's on top of another object).\n\n    Args:\n        object_name (str): The name of the object to be picked up. Ensure this name matches an object visible in\n        the robot's workspace.\n        pick_coordinate (List): The world coordinates [x, y] where the object should be picked up. Use these\n        coordinates to identify the object's exact position.\n        place_coordinate (List): The world coordinates [x, y] where the object should be placed at.\n        location (Location): Specifies the relative placement position of the picked object with respect to an object\n        being at the 'place_coordinate'. Possible values are defined in the `Location` Enum:\n            - `Location.LEFT_NEXT_TO`: Left of the reference object.\n            - `Location.RIGHT_NEXT_TO`: Right of the reference object.\n            - `Location.ABOVE`: Above the reference object.\n            - `Location.BELOW`: Below the reference object.\n            - `Location.ON_TOP_OF`: On top of the reference object.\n            - `Location.INSIDE`: Inside the reference object.\n            - `Location.NONE`: No specific location relative to another object.\n        z_offset (float): Additional height offset in meters to apply when picking (default: 0.001).\n        Useful for picking objects that are stacked on top of other objects.\n\n    Returns:\n        bool: True if successful\n    \"\"\"\n    success = self.pick_object(object_name, pick_coordinate, z_offset=z_offset)\n\n    if success:\n        place_success = self.place_object(place_coordinate, location)\n        return place_success\n    else:\n        return False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.pick_place_object_across_workspaces","title":"<code>pick_place_object_across_workspaces(object_name, pick_workspace_id, pick_coordinate, place_workspace_id, place_coordinate, location=None, z_offset=0.001)</code>","text":"<p>Pick an object from one workspace and place it in another workspace.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>Name of the object to pick</p> required <code>pick_workspace_id</code> <code>str</code> <p>ID of the workspace to pick from</p> required <code>pick_coordinate</code> <code>List[float]</code> <p>[x, y] coordinate in pick workspace</p> required <code>place_workspace_id</code> <code>str</code> <p>ID of the workspace to place in</p> required <code>place_coordinate</code> <code>List[float]</code> <p>[x, y] coordinate in place workspace</p> required <code>location</code> <code>Union['Location', str, None]</code> <p>Relative placement location (Location enum or string)</p> <code>None</code> <code>z_offset</code> <code>float</code> <p>Additional height offset in meters when picking (default: 0.001)</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful, False otherwise</p> Example <p>robot.pick_place_object_across_workspaces(     object_name='cube',     pick_workspace_id='niryo_ws_left',     pick_coordinate=[0.2, 0.05],     place_workspace_id='niryo_ws_right',     place_coordinate=[0.25, -0.05],     location=Location.RIGHT_NEXT_TO,     z_offset=0.02 )</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def pick_place_object_across_workspaces(\n    self,\n    object_name: str,\n    pick_workspace_id: str,\n    pick_coordinate: List[float],\n    place_workspace_id: str,\n    place_coordinate: List[float],\n    location: Union[\"Location\", str, None] = None,\n    z_offset: float = 0.001,\n) -&gt; bool:\n    \"\"\"\n    Pick an object from one workspace and place it in another workspace.\n\n    Args:\n        object_name: Name of the object to pick\n        pick_workspace_id: ID of the workspace to pick from\n        pick_coordinate: [x, y] coordinate in pick workspace\n        place_workspace_id: ID of the workspace to place in\n        place_coordinate: [x, y] coordinate in place workspace\n        location: Relative placement location (Location enum or string)\n        z_offset: Additional height offset in meters when picking (default: 0.001)\n\n    Returns:\n        bool: True if successful, False otherwise\n\n    Example:\n        robot.pick_place_object_across_workspaces(\n            object_name='cube',\n            pick_workspace_id='niryo_ws_left',\n            pick_coordinate=[0.2, 0.05],\n            place_workspace_id='niryo_ws_right',\n            place_coordinate=[0.25, -0.05],\n            location=Location.RIGHT_NEXT_TO,\n            z_offset=0.02\n        )\n    \"\"\"\n    self._logger.debug(f\"Multi-workspace operation: {object_name}\")\n    self._logger.debug(f\"  Pick from: {pick_workspace_id} at {pick_coordinate}\")\n    self._logger.debug(f\"  Place in: {place_workspace_id} at {place_coordinate}\")\n\n    # Step 1: Move to pick workspace observation pose\n    self.move2observation_pose(pick_workspace_id)\n    self.environment()._current_workspace_id = pick_workspace_id\n\n    # Step 2: Pick the object\n    success = self.pick_object_from_workspace(object_name, pick_workspace_id, pick_coordinate, z_offset=z_offset)\n\n    if not success:\n        self._logger.error(f\"Failed to pick {object_name} from {pick_workspace_id}\")\n        return False\n\n    # Step 3: Move to place workspace observation pose\n    self.move2observation_pose(place_workspace_id)\n    self.environment()._current_workspace_id = place_workspace_id\n\n    # Step 4: Place the object\n    place_success = self.place_object_in_workspace(place_workspace_id, place_coordinate, location)\n\n    if place_success:\n        # Update memory: remove from source, add to target\n        self.environment().update_object_in_workspace(\n            source_workspace_id=pick_workspace_id,\n            target_workspace_id=place_workspace_id,\n            object_label=object_name,\n            old_coordinate=pick_coordinate,\n            new_coordinate=place_coordinate,\n        )\n\n        self._logger.info(f\"Successfully moved {object_name} from {pick_workspace_id} to {place_workspace_id}\")\n\n    return place_success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.place_object","title":"<code>place_object(place_coordinate, location=None)</code>","text":"<p>Instruct the pick-and-place robot arm to place a picked object at the specified 'place_coordinate'. The function moves the gripper to the specified 'place_coordinate' and calculates the exact placement position from the given 'location'. Before calling this function you have to call robot_pick_object() to pick an object.</p> <p>Example call:</p> <p>robot.place_object([0.2, 0.0], \"left next to\") --&gt; Places the already gripped object left next to the world coordinate [0.2, 0.0].</p> <p>Parameters:</p> Name Type Description Default <code>place_coordinate</code> <code>List[float]</code> <p>The world coordinates [x, y] of the target object.</p> required <code>location</code> <code>str</code> <p>Specifies the relative placement position of the picked object in relation to an object</p> <code>None</code> <code>being at the 'place_coordinate'. Possible positions</code> <p>'left next to', 'right next to', 'above', 'below',</p> required <p>Returns:     bool: True</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef place_object(self, place_coordinate: List[float], location: Union[\"Location\", str, None] = None) -&gt; bool:\n    \"\"\"\n    Instruct the pick-and-place robot arm to place a picked object at the specified 'place_coordinate'. The\n    function moves the gripper to the specified 'place_coordinate' and calculates the exact placement position from\n    the given 'location'. Before calling this function you have to call robot_pick_object() to pick an object.\n\n    Example call:\n\n    robot.place_object([0.2, 0.0], \"left next to\")\n    --&gt; Places the already gripped object left next to the world coordinate [0.2, 0.0].\n\n    Args:\n        place_coordinate: The world coordinates [x, y] of the target object.\n        location (str): Specifies the relative placement position of the picked object in relation to an object\n        being at the 'place_coordinate'. Possible positions: 'left next to', 'right next to', 'above', 'below',\n        'on top of', 'inside', or None. Set to None, if there is no location given in the task.\n    Returns:\n        bool: True\n    \"\"\"\n    location = Location.convert_str2location(location)\n\n    if self._object_last_picked:\n        old_coordinate = [self._object_last_picked.x_com(), self._object_last_picked.y_com()]\n        message = (\n            f\"Going to place {self._object_last_picked.label()} {location} coordinate [\"\n            f\"{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n        )\n    else:\n        old_coordinate = None\n        message = f\"Going to place it {location} coordinate [{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n\n    self._logger.info(message)\n\n    self.environment().oralcom_call_text2speech_async(message, priority=8)\n    obj_where_to_place = None\n\n    if location is not None and location is not Location.NONE:\n        obj_where_to_place = self._get_nearest_object(None, place_coordinate)\n        if obj_where_to_place is None:\n            place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n        else:\n            place_pose = obj_where_to_place.pose_center()\n    else:\n        place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n        self._logger.debug(f\"place_object: {place_pose}\")\n\n    x_off = 0.02\n    y_off = 0.02\n\n    if self._object_last_picked:\n        x_off += self._object_last_picked.height_m() / 2\n        y_off += self._object_last_picked.width_m() / 2\n\n    if place_pose:\n        # TODO: use height of object instead\n        if location == Location.ON_TOP_OF:\n            place_pose.z += 0.02\n        elif location == Location.INSIDE:\n            place_pose.z += 0.01\n        elif location == Location.RIGHT_NEXT_TO:\n            place_pose.y -= obj_where_to_place.width_m() / 2 + y_off\n        elif location == Location.LEFT_NEXT_TO:\n            place_pose.y += obj_where_to_place.width_m() / 2 + y_off\n        elif location == Location.BELOW:\n            # print(obj_where_to_place.height_m(), self._object_last_picked.width_m(), x_off)\n            # TODO: nutze hier auch width, da width immer die gr\u00f6\u00dfere gr\u00f6\u00dfe ist und nicht eine koordinatenrichtugn hat\n            #  ich muss anstatt width und height eine gr\u00f6\u00dfe haben dim_x und dim_y, die a x und y koordinate gebunden sind\n            #  ich habe das in object klasse repariert, width geht immer entlang y-achse jetzt. pr\u00fcfen hier\n            place_pose.x -= obj_where_to_place.height_m() / 2 + x_off\n        elif location == Location.ABOVE:\n            # TODO: nutze hier auch width, da width immer die gr\u00f6\u00dfere gr\u00f6\u00dfe ist und nicht eine koordinatenrichtugn hat\n            #  ich habe das in object klasse repariert, width geht immer entlang y-achse jetzt. pr\u00fcfen hier\n            print(obj_where_to_place.height_m(), self._object_last_picked.width_m(), x_off)\n            place_pose.x += obj_where_to_place.height_m() / 2 + x_off\n            self._logger.debug(f\"{place_pose}\")\n        elif location is Location.NONE or location is None:\n            pass  # I do not have to do anything as the given location is where to place the object\n        else:\n            self._logger.error(f\"Unknown location: {location} (type: {type(location)})\")\n\n        success = self._robot.robot_place_object(place_pose)\n\n        # update position of placed object to the new position\n        # Update memory after successful placement\n        if success and self._object_last_picked and old_coordinate:\n            final_coordinate = [place_pose.x, place_pose.y]\n            self._logger.debug(f\"final_coordinate: {final_coordinate}\")\n\n            self.environment().update_object_in_memory(\n                self._object_last_picked.label(), old_coordinate, new_pose=place_pose\n            )\n\n            # Give the memory system a moment to register the update\n            import time\n\n            time.sleep(0.1)\n    else:\n        success = False\n\n    self._object_last_picked = None\n\n    # thread_oral.join()\n\n    return success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.place_object_in_workspace","title":"<code>place_object_in_workspace(workspace_id, place_coordinate, location=None)</code>","text":"<p>Place a picked object in a specific workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the target workspace</p> required <code>place_coordinate</code> <code>List[float]</code> <p>[x, y] coordinate in workspace</p> required <code>location</code> <code>Union['Location', str, None]</code> <p>Relative placement location</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if successful</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def place_object_in_workspace(\n    self, workspace_id: str, place_coordinate: List[float], location: Union[\"Location\", str, None] = None\n) -&gt; bool:\n    \"\"\"\n    Place a picked object in a specific workspace.\n\n    Args:\n        workspace_id: ID of the target workspace\n        place_coordinate: [x, y] coordinate in workspace\n        location: Relative placement location\n\n    Returns:\n        bool: True if successful\n    \"\"\"\n    location = Location.convert_str2location(location)\n\n    if self._object_last_picked:\n        message = (\n            f\"Placing {self._object_last_picked.label()} in workspace \"\n            f\"{workspace_id} {location} coordinate \"\n            f\"[{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n        )\n    else:\n        message = (\n            f\"Placing object in workspace {workspace_id} {location} \"\n            f\"coordinate [{place_coordinate[0]:.2f}, {place_coordinate[1]:.2f}].\"\n        )\n\n    self._logger.info(message)\n    self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n    # Get workspace for coordinate transformation\n    workspace = self.environment().get_workspace_by_id(workspace_id)\n    if workspace is None:\n        self._logger.error(f\"Workspace {workspace_id} not found\")\n        # thread_oral.join()\n        return False\n\n    # Find reference object in target workspace if location specified\n    obj_where_to_place = None\n    if location is not None and location is not Location.NONE:\n        obj_where_to_place = self._get_nearest_object_in_workspace(None, workspace_id, place_coordinate)\n\n        if obj_where_to_place is None:\n            place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n        else:\n            place_pose = obj_where_to_place.pose_center()\n    else:\n        place_pose = PoseObjectPNP(place_coordinate[0], place_coordinate[1], 0.09, 0.0, 1.57, 0.0)\n\n    # Calculate placement offset based on location\n    if place_pose and obj_where_to_place:\n        x_off = 0.02\n        y_off = 0.02\n\n        if self._object_last_picked:\n            x_off += self._object_last_picked.height_m() / 2\n            y_off += self._object_last_picked.width_m() / 2\n\n        if location == Location.ON_TOP_OF:\n            place_pose.z += 0.02\n        elif location == Location.INSIDE:\n            place_pose.z += 0.01\n        elif location == Location.RIGHT_NEXT_TO:\n            place_pose.y -= obj_where_to_place.width_m() / 2 + y_off\n        elif location == Location.LEFT_NEXT_TO:\n            place_pose.y += obj_where_to_place.width_m() / 2 + y_off\n        elif location == Location.BELOW:\n            place_pose.x -= obj_where_to_place.height_m() / 2 + x_off\n        elif location == Location.ABOVE:\n            place_pose.x += obj_where_to_place.height_m() / 2 + x_off\n\n    success = self._robot.robot_place_object(place_pose)\n\n    # Clear last picked object\n    self._object_last_picked = None\n    if \"_object_source_workspace\" in self.__dict__:\n        del self._object_source_workspace\n\n    # thread_oral.join()\n    return success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.push_object","title":"<code>push_object(object_name, push_coordinate, direction, distance)</code>","text":"<p>Instruct the pick-and-place robot arm to push a specific object to a new position. This function should only be called if it is not possible to pick the object. An object cannot be picked if its shorter side is larger than the gripper.</p> <p>Parameters:</p> Name Type Description Default <code>object_name</code> <code>str</code> <p>The name of the object to be pushed.</p> required <code>push_coordinate</code> <code>List[float]</code> <p>The world coordinates [x, y] where the object to push is located.</p> required <code>direction</code> <code>str</code> <p>The direction in which to push the object.</p> required <code>Valid options are</code> <p>\"up\", \"down\", \"left\", \"right\".</p> required <code>distance</code> <code>float</code> <p>The distance (in millimeters) to push the object in the specified direction.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>@log_start_end_cls()\ndef push_object(self, object_name: str, push_coordinate: List[float], direction: str, distance: float) -&gt; bool:\n    \"\"\"\n    Instruct the pick-and-place robot arm to push a specific object to a new position.\n    This function should only be called if it is not possible to pick the object.\n    An object cannot be picked if its shorter side is larger than the gripper.\n\n    Args:\n        object_name (str): The name of the object to be pushed.\n        Ensure the name matches an object in the robot's environment.\n        push_coordinate: The world coordinates [x, y] where the object to push is located.\n        These coordinates indicate the initial position of the object.\n        direction (str): The direction in which to push the object.\n        Valid options are: \"up\", \"down\", \"left\", \"right\".\n        distance: The distance (in millimeters) to push the object in the specified direction.\n        Ensure the value is within the robot's operating range.\n\n    Returns:\n        bool: True\n    \"\"\"\n    message = f\"Calling push with {object_name} and {direction}\"\n    self._logger.info(message)\n\n    self.environment().oralcom_call_text2speech_async(message, priority=8)\n\n    obj_to_push = self._get_nearest_object(object_name, push_coordinate)\n\n    push_pose = obj_to_push.pose_com()\n\n    # it is certainly better when pushing up to move under the object with a closed gripper so we can\n    #  actually push up. same for the other directions.\n    if direction == \"up\":\n        push_pose.x -= obj_to_push.height_m() / 2.0\n        # gripper 90\u00b0 rotated. TODO: I have to test these orientations\n        push_pose.yaw = math.pi / 2.0\n    elif direction == \"down\":\n        push_pose.x += obj_to_push.height_m() / 2.0\n        # gripper 90\u00b0 rotated. TODO: I have to test these orientations\n        push_pose.yaw = math.pi / 2.0\n    elif direction == \"left\":\n        push_pose.y += obj_to_push.width_m() / 2.0\n        # gripper 0\u00b0 rotated. TODO: I have to test these orientations\n        push_pose.yaw = 0.0\n    elif direction == \"right\":\n        push_pose.y -= obj_to_push.width_m() / 2.0\n        # gripper 0\u00b0 rotated. TODO: I have to test these orientations\n        push_pose.yaw = 0.0\n    else:\n        self._logger.error(f\"Unknown direction: {direction}\")\n\n    if obj_to_push is not None:\n        success = self._robot.robot_push_object(push_pose, direction, distance)\n    else:\n        success = False\n\n    # thread_oral.join()\n\n    return success\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.robot","title":"<code>robot()</code>","text":"<p>Returns:</p> Name Type Description <code>RobotController</code> <code>RobotController</code> <p>object that controls the robot.</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def robot(self) -&gt; RobotController:\n    \"\"\"\n    Returns:\n        RobotController: object that controls the robot.\n    \"\"\"\n    return self._robot\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.robot_in_motion","title":"<code>robot_in_motion()</code>","text":"<p>:return: value of _robot_in_motion: False: robot is not in motion True: robot is in motion and therefore maybe cannot see the workspace markers</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def robot_in_motion(self) -&gt; bool:\n    \"\"\"\n    :return: value of _robot_in_motion:\n    False: robot is not in motion\n    True: robot is in motion and therefore maybe cannot see the workspace markers\n    \"\"\"\n    return self._robot.is_in_motion()\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot.Robot.verbose","title":"<code>verbose()</code>","text":"<p>Returns: True, if verbose is on, else False</p> Source code in <code>robot_environment/robot/robot.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"\n    Returns: True, if verbose is on, else False\n    \"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/robot/#controllers","title":"Controllers","text":""},{"location":"api/robot/#robotcontroller","title":"RobotController","text":""},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController","title":"<code>robot_environment.robot.robot_controller.RobotController</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract class for the pick-and-place robot that provides the primitive tasks of the robot like pick and place operations.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>class RobotController(ABC):\n    \"\"\"\n    An abstract class for the pick-and-place robot that provides the primitive tasks of the robot like\n    pick and place operations.\n\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    def __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n        \"\"\"\n        Initializes the robot (connects to it and calibrates it).\n\n        Args:\n            robot: object of the Robot class.\n            use_simulation: True, if working with a simulation model of the robot,\n            else False if we work with a real robot.\n            verbose:\n        \"\"\"\n        super().__init__()\n\n        # Initialize the asyncio lock\n        self._lock = threading.Lock()  # asyncio.Lock()\n\n        self._verbose = verbose\n        self._robot = robot\n        self._in_motion = False\n\n        self._init_robot(use_simulation)\n\n    # Deleting (Calling destructor)\n    def __del__(self):\n        pass\n\n    # *** PUBLIC SET methods ***\n\n    # *** PUBLIC GET methods ***\n\n    @abstractmethod\n    def get_pose(self) -&gt; \"PoseObjectPNP\":\n        \"\"\"\n        Get current pose of gripper of robot.\n\n        Returns:\n            current pose of gripper of robot.\n        \"\"\"\n        pass\n\n    # *** PUBLIC methods ***\n\n    @abstractmethod\n    def calibrate(self) -&gt; bool:\n        \"\"\"\n        Calibrates the Robot.\n\n        Returns:\n            True, if calibration was successful, else False\n        \"\"\"\n        pass\n\n    # TODO: also possible to only pass PoseObject of the object. The advantage of passing Object might be\n    #  that an object has more then one pick position. then robot can try and pick at a few positions.\n    # @abstractmethod\n    # def robot_pick_object(self, obj2pick: \"Object\") -&gt; bool:\n    #     \"\"\"\n    #     Calls the pick command of the self._robot_ctrl to pick the given Object\n    #\n    #     Args:\n    #         obj2pick: Object that shall be picked\n    #\n    #     Returns:\n    #         True, if pick was successful, else False\n    #     \"\"\"\n    #     return False\n    @abstractmethod\n    def robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Calls the pick command of the self._robot_ctrl to pick an object at the given pose.\n\n        Args:\n            pick_pose: Pose where the object should be picked (includes z-offset if needed)\n\n        Returns:\n            True, if pick was successful, else False\n        \"\"\"\n        return False\n\n    @abstractmethod\n    def robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Places an already picked object at the given place_pose.\n\n        Args:\n            place_pose: Pose where to place the already picked object\n\n        Returns:\n            True, if place was successful, else False\n        \"\"\"\n        return False\n\n    @abstractmethod\n    def robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n        \"\"\"\n        Push given object (its Pose) into the given direction by the given distance.\n\n        Args:\n            push_pose: the Pose of the object that should be pushed.\n            direction: \"up\", \"down\", \"left\", \"right\"\n            distance: distance in millimeters\n\n        Returns:\n            True, if push was successful, else False\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n        \"\"\"\n        Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n        calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n        given coordinate an object that has the given orientation. For this method to work, it is important that\n        only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n        this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n        Args:\n            workspace_id: id of the workspace\n            u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n            v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n            yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n        Returns:\n            pose_object: Pose of the point in world coordinates of the robot.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def move2observation_pose(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        The robot should move to a pose where it can observe the workspace given by workspace_id.\n\n        Args:\n            workspace_id: id of the workspace\n        \"\"\"\n        pass\n\n    # *** PUBLIC STATIC/CLASS GET methods ***\n\n    # *** PRIVATE methods ***\n\n    @abstractmethod\n    def _init_robot(self, use_simulation: bool) -&gt; bool:\n        \"\"\"\n        Code to initialize the robot. After calling this method the robot should be ready to receive commands.\n\n        Args:\n            use_simulation: True, if working with a simulation model of the robot,\n            else False if we work with a real robot.\n\n        Returns:\n            bool: True, if initialization was successful, else False\n        \"\"\"\n        pass\n\n    def _set_in_motion(self, in_motion: bool):\n        \"\"\"Set the robot motion state.\"\"\"\n        self._in_motion = in_motion\n        # if hasattr(self._robot, \"_robot_in_motion\"):\n        #     self._robot._robot_in_motion = in_motion\n\n    # *** PUBLIC properties ***\n\n    def is_in_motion(self) -&gt; bool:\n        \"\"\"Check if robot is currently in motion.\"\"\"\n        return self._in_motion\n\n    def robot_ctrl(self):\n        \"\"\"\n\n        Returns:\n            the object of the underlying robot. For the Niryo Ned2 it is an object of the NiryoRobot class.\n        \"\"\"\n        return self._robot_ctrl\n\n    def robot(self) -&gt; \"Robot\":\n        return self._robot\n\n    def lock(self) -&gt; threading.Lock:\n        \"\"\"\n\n        Returns:\n            a lock, to only call the robot interface with one method and not many methods in parallel, because for Niryo\n            the interface is not thread safe.\n        \"\"\"\n        return self._lock\n\n    def verbose(self) -&gt; bool:\n        \"\"\"\n\n        Returns: True, if verbose is on, else False\n\n        \"\"\"\n        return self._verbose\n\n    # *** PRIVATE variables ***\n\n    # the object of the underlying robot. For the Niryo Ned2 it is an object of the NiryoRobot class.\n    _robot_ctrl = None\n\n    # object of the Robot class\n    _robot = None\n\n    # a lock, to only call the robot interface with one method and not many methods in parallel, because for Niryo\n    # the interface is not thread safe.\n    _lock = None\n\n    _verbose = False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController-functions","title":"Functions","text":""},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.__init__","title":"<code>__init__(robot, use_simulation, verbose=False)</code>","text":"<p>Initializes the robot (connects to it and calibrates it).</p> <p>Parameters:</p> Name Type Description Default <code>robot</code> <code>Robot</code> <p>object of the Robot class.</p> required <code>use_simulation</code> <code>bool</code> <p>True, if working with a simulation model of the robot,</p> required <code>verbose</code> <code>bool</code> <code>False</code> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>def __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n    \"\"\"\n    Initializes the robot (connects to it and calibrates it).\n\n    Args:\n        robot: object of the Robot class.\n        use_simulation: True, if working with a simulation model of the robot,\n        else False if we work with a real robot.\n        verbose:\n    \"\"\"\n    super().__init__()\n\n    # Initialize the asyncio lock\n    self._lock = threading.Lock()  # asyncio.Lock()\n\n    self._verbose = verbose\n    self._robot = robot\n    self._in_motion = False\n\n    self._init_robot(use_simulation)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.calibrate","title":"<code>calibrate()</code>  <code>abstractmethod</code>","text":"<p>Calibrates the Robot.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if calibration was successful, else False</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef calibrate(self) -&gt; bool:\n    \"\"\"\n    Calibrates the Robot.\n\n    Returns:\n        True, if calibration was successful, else False\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.get_pose","title":"<code>get_pose()</code>  <code>abstractmethod</code>","text":"<p>Get current pose of gripper of robot.</p> <p>Returns:</p> Type Description <code>PoseObjectPNP</code> <p>current pose of gripper of robot.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef get_pose(self) -&gt; \"PoseObjectPNP\":\n    \"\"\"\n    Get current pose of gripper of robot.\n\n    Returns:\n        current pose of gripper of robot.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.get_target_pose_from_rel","title":"<code>get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code>  <code>abstractmethod</code>","text":"<p>Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw), calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the given coordinate an object that has the given orientation. For this method to work, it is important that only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <code>u_rel</code> <code>float</code> <p>horizontal coordinate in image of workspace, normalized between 0 and 1</p> required <code>v_rel</code> <code>float</code> <p>vertical coordinate in image of workspace, normalized between 0 and 1</p> required <code>yaw</code> <code>float</code> <p>orientation of an object at the pixel coordinates [u_rel, v_rel].</p> required <p>Returns:</p> Name Type Description <code>pose_object</code> <code>PoseObjectPNP</code> <p>Pose of the point in world coordinates of the robot.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n    \"\"\"\n    Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n    calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n    given coordinate an object that has the given orientation. For this method to work, it is important that\n    only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n    this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n    Args:\n        workspace_id: id of the workspace\n        u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n        v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n        yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n    Returns:\n        pose_object: Pose of the point in world coordinates of the robot.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.is_in_motion","title":"<code>is_in_motion()</code>","text":"<p>Check if robot is currently in motion.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>def is_in_motion(self) -&gt; bool:\n    \"\"\"Check if robot is currently in motion.\"\"\"\n    return self._in_motion\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.lock","title":"<code>lock()</code>","text":"<p>Returns:</p> Type Description <code>Lock</code> <p>a lock, to only call the robot interface with one method and not many methods in parallel, because for Niryo</p> <code>Lock</code> <p>the interface is not thread safe.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>def lock(self) -&gt; threading.Lock:\n    \"\"\"\n\n    Returns:\n        a lock, to only call the robot interface with one method and not many methods in parallel, because for Niryo\n        the interface is not thread safe.\n    \"\"\"\n    return self._lock\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.move2observation_pose","title":"<code>move2observation_pose(workspace_id)</code>  <code>abstractmethod</code>","text":"<p>The robot should move to a pose where it can observe the workspace given by workspace_id.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef move2observation_pose(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    The robot should move to a pose where it can observe the workspace given by workspace_id.\n\n    Args:\n        workspace_id: id of the workspace\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.robot_ctrl","title":"<code>robot_ctrl()</code>","text":"<p>Returns:</p> Type Description <p>the object of the underlying robot. For the Niryo Ned2 it is an object of the NiryoRobot class.</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>def robot_ctrl(self):\n    \"\"\"\n\n    Returns:\n        the object of the underlying robot. For the Niryo Ned2 it is an object of the NiryoRobot class.\n    \"\"\"\n    return self._robot_ctrl\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.robot_pick_object","title":"<code>robot_pick_object(pick_pose)</code>  <code>abstractmethod</code>","text":"<p>Calls the pick command of the self._robot_ctrl to pick an object at the given pose.</p> <p>Parameters:</p> Name Type Description Default <code>pick_pose</code> <code>PoseObjectPNP</code> <p>Pose where the object should be picked (includes z-offset if needed)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if pick was successful, else False</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Calls the pick command of the self._robot_ctrl to pick an object at the given pose.\n\n    Args:\n        pick_pose: Pose where the object should be picked (includes z-offset if needed)\n\n    Returns:\n        True, if pick was successful, else False\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.robot_place_object","title":"<code>robot_place_object(place_pose)</code>  <code>abstractmethod</code>","text":"<p>Places an already picked object at the given place_pose.</p> <p>Parameters:</p> Name Type Description Default <code>place_pose</code> <code>PoseObjectPNP</code> <p>Pose where to place the already picked object</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if place was successful, else False</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Places an already picked object at the given place_pose.\n\n    Args:\n        place_pose: Pose where to place the already picked object\n\n    Returns:\n        True, if place was successful, else False\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.robot_push_object","title":"<code>robot_push_object(push_pose, direction, distance)</code>  <code>abstractmethod</code>","text":"<p>Push given object (its Pose) into the given direction by the given distance.</p> <p>Parameters:</p> Name Type Description Default <code>push_pose</code> <code>PoseObjectPNP</code> <p>the Pose of the object that should be pushed.</p> required <code>direction</code> <code>str</code> <p>\"up\", \"down\", \"left\", \"right\"</p> required <code>distance</code> <code>float</code> <p>distance in millimeters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if push was successful, else False</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>@abstractmethod\ndef robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n    \"\"\"\n    Push given object (its Pose) into the given direction by the given distance.\n\n    Args:\n        push_pose: the Pose of the object that should be pushed.\n        direction: \"up\", \"down\", \"left\", \"right\"\n        distance: distance in millimeters\n\n    Returns:\n        True, if push was successful, else False\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.robot_controller.RobotController.verbose","title":"<code>verbose()</code>","text":"<p>Returns: True, if verbose is on, else False</p> Source code in <code>robot_environment/robot/robot_controller.py</code> <pre><code>def verbose(self) -&gt; bool:\n    \"\"\"\n\n    Returns: True, if verbose is on, else False\n\n    \"\"\"\n    return self._verbose\n</code></pre>"},{"location":"api/robot/#niryorobotcontroller","title":"NiryoRobotController","text":""},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController","title":"<code>robot_environment.robot.niryo_robot_controller.NiryoRobotController</code>","text":"<p>               Bases: <code>RobotController</code></p> <p>Class for the pick-and-place niryo ned2 robot that provides the primitive tasks of the robot like pick and place operations.</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>class NiryoRobotController(RobotController):\n    \"\"\"\n    Class for the pick-and-place niryo ned2 robot that provides the primitive tasks of the robot like\n    pick and place operations.\n    \"\"\"\n\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n        \"\"\"\n        Initializes the robot (connects to it and calibrates it).\n\n        Args:\n            robot: object of the Robot class.\n            use_simulation: True, if working with a simulation model of the robot,\n            else False if we work with a real robot.\n            verbose:\n        \"\"\"\n        self._executor = ThreadPoolExecutor(max_workers=1)\n        self._shutdown_v = False\n        self._logger = get_package_logger(__name__, verbose)\n\n        super().__init__(robot, use_simulation, verbose)\n\n    # Deleting (Calling destructor)\n    def __del__(self):\n        super().__del__()\n\n        if hasattr(self, \"_executor\") and self._executor:\n            self._logger.debug(\"Shutting down ThreadPoolExecutor in destructor...\")\n            self._executor.shutdown(wait=True)\n\n        self._logger.debug(\"Destructor called, Robot deleted.\")\n        with self._lock:\n            self._logger.debug(\"Destructor called, Robot deleted.2\")\n            self._shutdown()\n\n    def cleanup(self):\n        \"\"\"\n        Explicit cleanup method - call this when you're done with the object.\n        This is more reliable than relying on __del__.\n        \"\"\"\n        if hasattr(self, \"_executor\") and self._executor:\n            self._logger.info(\"Shutting down ThreadPoolExecutor...\")\n            self._shutdown_v = True\n            self._executor.shutdown(wait=True)\n            self._executor = None\n\n    # *** PUBLIC SET methods ***\n\n    # *** PUBLIC GET methods ***\n\n    def get_pose(self) -&gt; PoseObjectPNP:\n        \"\"\"\n        Get current pose of gripper of robot.\n\n        Returns:\n            current pose of gripper of robot.\n        \"\"\"\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                pose = self._robot_ctrl.arm.get_pose()\n            else:\n                pose = self._robot_ctrl.get_pose()\n\n        return PoseObjectPNP.convert_niryo_pose_object2pose_object(pose)\n\n    def get_camera_intrinsics(self):\n        # all calls of methods of the _robot (NiryoRobot) object are locked, because they are not safe thread\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                mtx, dist = self._robot_ctrl.vision.get_camera_intrinsics()\n            else:\n                mtx, dist = self._robot_ctrl.get_camera_intrinsics()\n\n        return mtx, dist\n\n    def get_img_compressed(self) -&gt; np.ndarray:\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                img_compressed = self._robot_ctrl.vision.get_img_compressed()\n            else:\n                img_compressed = self._robot_ctrl.get_img_compressed()\n\n        return img_compressed\n\n    # *** PUBLIC methods ***\n\n    def calibrate(self) -&gt; bool:\n        \"\"\"\n        Calibrates the NiryoRobot.\n\n        Returns:\n            True, if calibration was successful, else False\n        \"\"\"\n        self._calibrate_auto()\n        return True\n\n    def reset_connection(self) -&gt; None:\n        \"\"\"\n        Reset the connection to the robot by safely disconnecting and reconnecting.\n        \"\"\"\n        self._logger.info(\"Resetting the robot connection...\")\n        try:\n            # Attempt to close the connection safely\n            if self._robot_ctrl is not None:\n                with self._lock:\n                    self._shutdown()\n\n        except Exception as e:\n            self._logger.error(f\"Error while closing connection: {e}\", exc_info=True)\n\n        # Reinitialize the connection\n        try:\n            self._create_robot()\n            self._logger.info(\"Connection successfully reset.\")\n        except Exception as e:\n            self._logger.error(f\"Failed to reconnect to the robot: {e}\", exc_info=True)\n            self._robot = None\n\n    @log_start_end_cls()\n    def robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Calls the pick command of the self._robot_ctrl to pick the object at the given pose.\n\n        Args:\n            pick_pose: Pose where to pick the object (z-offset already applied if needed)\n\n        Returns:\n            True, if pick was successful, else False\n        \"\"\"\n        # Convert to Niryo format\n        pick_pose_niryo = PoseObjectPNP.convert_pose_object2niryo_pose_object(pick_pose)\n\n        self._set_in_motion(True)\n        try:\n            with self._lock:\n                if pyniryo_v == \"pyniryo2\":\n                    self._robot_ctrl.pick_place.pick_from_pose(pick_pose_niryo)\n                else:\n                    self._robot_ctrl.pick_from_pose(pick_pose_niryo)\n\n            self._logger.info(\"Finished pick_from_pose\")\n            return True\n            # TODO: in newest version available\n            # return not self._robot_ctrl.collision_detected\n\n        except Exception as e:\n            self._logger.error(f\"Pick failed: {e}\")\n            return False\n        finally:\n            self._set_in_motion(False)\n\n    @log_start_end_cls()\n    def robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Places an already picked object at the given place_pose.\n\n        Args:\n            place_pose: Pose where to place the already picked object\n\n        Returns:\n            True, if place was successful, else False\n        \"\"\"\n        place_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(place_pose)\n        place_pose = place_pose.copy_with_offsets(z_offset=0.005)\n\n        self._logger.debug(f\"Place pose: {place_pose}\")\n\n        self._set_in_motion(True)\n        try:\n            with self._lock:\n                if pyniryo_v == \"pyniryo2\":\n                    self._robot_ctrl.pick_place.place_from_pose(place_pose)\n                else:\n                    self._robot_ctrl.place_from_pose(place_pose)\n\n            return True\n            # TODO: in newest version available\n            # return not self._robot_ctrl.collision_detected\n        except Exception as e:\n            self._logger.error(f\"Place failed: {e}\")\n            return False\n        finally:\n            self._set_in_motion(False)\n\n    @log_start_end_cls()\n    def robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n        \"\"\"\n        Push given object (its Pose) into the given direction by the given distance.\n\n        Args:\n            push_pose: the Pose of the object that should be pushed.\n            direction: \"up\", \"down\", \"left\", \"right\"\n            distance: distance in millimeters\n\n        Returns:\n            True, if push was successful, else False\n        \"\"\"\n        push_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(push_pose)\n\n        self._logger.debug(f\"Push pose: {push_pose}\")\n\n        self._set_in_motion(True)\n        try:\n            with self._lock:\n                if pyniryo_v == \"pyniryo2\":\n                    self._robot_ctrl.tool.close_gripper()\n                else:\n                    self._robot_ctrl.close_gripper()\n\n                self._move_pose(push_pose)\n\n                if direction == \"up\":\n                    self._shift_pose(RobotAxis.X, distance)\n                elif direction == \"down\":\n                    self._shift_pose(RobotAxis.X, -distance)\n                elif direction == \"left\":\n                    self._shift_pose(RobotAxis.Y, distance)\n                elif direction == \"right\":\n                    self._shift_pose(RobotAxis.Y, -distance)\n                else:\n                    self._logger.error(f\"Unknown direction: {direction}\")\n\n            return True\n            # TODO: in newest version available\n            # return not self._robot_ctrl.collision_detected\n        except Exception as e:\n            self._logger.error(f\"Push failed: {e}\")\n            return False\n        finally:\n            self._set_in_motion(False)\n\n    def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n        \"\"\"\n        Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n        calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n        given coordinate an object that has the given orientation. For this method to work, it is important that\n        only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n        this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n        Args:\n            workspace_id: id of the workspace\n            u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n            v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n            yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n        Returns:\n            pose_object: Pose of the point in world coordinates of the robot.\n        \"\"\"\n        self._logger.debug(f\"Thread {threading.current_thread().name}: {workspace_id}, {u_rel}, {v_rel}, {yaw}\")\n\n        # Use the asyncio lock for thread-safe access\n        with self._lock:\n            self._logger.debug(f\"Thread {threading.current_thread().name} acquired lock\")\n\n            try:\n                x_rel = max(0.0, min(u_rel, 1.0))\n                y_rel = max(0.0, min(v_rel, 1.0))\n\n                if pyniryo_v == \"pyniryo2\":\n                    obj_coords = self._robot_ctrl.vision.get_target_pose_from_rel(workspace_id, 0.0, x_rel, y_rel, yaw)\n                else:\n                    obj_coords = self._robot_ctrl.get_target_pose_from_rel(workspace_id, 0.0, x_rel, y_rel, yaw)\n\n                obj_coords = PoseObjectPNP.convert_niryo_pose_object2pose_object(obj_coords)\n\n            except (NiryoRobotException, UnicodeDecodeError, SyntaxError, TcpCommandException) as e:\n                self._logger.error(f\"Thread {threading.current_thread().name} Error: {e}\", exc_info=True)\n                obj_coords = PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n            finally:\n                self._logger.debug(f\"Thread {threading.current_thread().name} releasing lock\")\n\n        self._logger.debug(f\"Thread {threading.current_thread().name} exiting: {obj_coords}\")\n\n        return obj_coords\n\n    def get_target_pose_from_rel_timeout(\n        self, workspace_id: str, x_rel: float, y_rel: float, yaw: float, timeout: float = 0.75\n    ) -&gt; \"PoseObjectPNP\":\n        self._logger.debug(f\"Thread {threading.current_thread().name} entering: {workspace_id}, {x_rel}, {y_rel}, {yaw}\")\n\n        if not self._lock.acquire(timeout=timeout):\n            self._logger.error(f\"Thread {threading.current_thread().name} failed to acquire lock within timeout\")\n            return PoseObject(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n        try:\n            self._logger.debug(f\"Thread {threading.current_thread().name} acquired lock\")\n            future = self._executor.submit(self._robot_ctrl.get_target_pose_from_rel, workspace_id, 0.0, x_rel, y_rel, yaw)\n\n            try:\n                obj_coords = future.result(timeout=timeout)\n            except FuturesTimeoutError:\n                print(f\"Thread {threading.current_thread().name} timeout waiting for robot response\")\n                # TODO: Ich kann nicht einfach die Verbindung resetten, da ja auch an anderen Orten auf den Roboter\n                #  in threads zugegriffen wird.\n                # self.reset_connection()\n                obj_coords = PoseObject(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n                future.cancel()  # Attempt to cancel the task if it is still running\n\n        except (NiryoRobotException, UnicodeDecodeError, SyntaxError, TcpCommandException) as e:\n            self._logger.error(f\"Thread {threading.current_thread().name} Error: {e}\", exc_info=True)\n            obj_coords = PoseObject(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n        finally:\n            self._logger.debug(f\"Thread {threading.current_thread().name} releasing lock\")\n            self._lock.release()\n\n        self._logger.debug(f\"Thread {threading.current_thread().name} exiting: {obj_coords}\")\n        obj_coords = PoseObjectPNP.convert_niryo_pose_object2pose_object(obj_coords)\n\n        return obj_coords\n\n    @log_start_end_cls()\n    def move2observation_pose(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        The robot moves to a pose where it can observe the workspace given by workspace_id.\n\n        Args:\n            workspace_id: id of the workspace\n        \"\"\"\n        observation_pose = self._robot.environment().get_observation_pose(workspace_id)\n\n        if observation_pose is None:\n            self._logger.warning(f\"observation_pose is None for workspace: {workspace_id}\")\n            return\n\n        # Only convert if we have a valid pose\n        observation_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(observation_pose)\n\n        self._set_in_motion(True)\n        try:\n            with self._lock:\n                self._move_pose(observation_pose)\n        except UnicodeDecodeError as e:\n            self._logger.error(f\"move2observation_pose error: {e}, pose: {observation_pose}\", exc_info=True)\n        finally:\n            self._set_in_motion(False)\n\n        self._logger.debug(f\"move_pose finished, current: {self.get_pose()}, target: {observation_pose}\")\n\n    def _shutdown(self) -&gt; None:\n        \"\"\"\n        Closes connection to NiryoRobot.\n        \"\"\"\n        if pyniryo_v == \"pyniryo2\":\n            # End Robot Connection\n            self._robot_ctrl.end()\n        else:\n            self._robot_ctrl.close_connection()\n\n    def _shift_pose(self, axis: RobotAxis, distance: float) -&gt; None:\n        \"\"\"\n        Shifts the gripper along the given axis for the specified distance.\n\n        Args:\n            axis (RobotAxis): axis of the robot to shift the gripper along\n            distance: distance in meters to shift the gripper along\n        \"\"\"\n        if pyniryo_v == \"pyniryo2\":\n            self._robot_ctrl.arm.shift_pose(axis, distance / 1000)\n        else:\n            self._robot_ctrl.shift_pose(axis, distance / 1000)\n\n    def _move_pose(self, pose: PoseObject) -&gt; None:\n        \"\"\"\n        Move gripper of robot to given pose.\n\n        Args:\n            pose (PoseObject): pose of gripper\n        \"\"\"\n        if pyniryo_v == \"pyniryo2\":\n            self._robot_ctrl.arm.move_pose(pose)\n        else:\n            self._robot_ctrl.move_pose(pose)\n\n    @log_start_end_cls()\n    def _create_robot(self) -&gt; None:\n        \"\"\"\n        Creates the NiryoRobot object and calibrates the robot.\n        \"\"\"\n        with self._lock:\n            self._robot_ctrl = NiryoRobot(self._robot_ip_address)\n        self._calibrate_auto()\n\n    def _calibrate_auto(self) -&gt; None:\n        \"\"\"\n        Calibrates the NiryoRobot.\n        \"\"\"\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                self._robot_ctrl.tool.update_tool()\n                self._robot_ctrl.arm.calibrate_auto()\n            else:\n                self._robot_ctrl.update_tool()\n                self._robot_ctrl.calibrate_auto()\n\n    @log_start_end_cls()\n    def _init_robot(self, use_simulation: bool) -&gt; bool:\n        \"\"\"\n        Creates the NiryoRobot object and connects to it.\n\n        Args:\n            use_simulation:\n\n        Returns:\n\n        \"\"\"\n        # Connect to Niryo Robot\n        if not use_simulation:\n            robot_ip_address = \"192.168.0.140\"\n        else:\n            robot_ip_address = \"192.168.247.128\"\n\n        self._robot_ip_address = robot_ip_address\n\n        self._create_robot()\n\n        return True\n\n    # *** PUBLIC properties ***\n\n    # *** PRIVATE variables ***\n\n    # ip address of the robot\n    _robot_ip_address = \"\"\n    _logger = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController-functions","title":"Functions","text":""},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.__init__","title":"<code>__init__(robot, use_simulation, verbose=False)</code>","text":"<p>Initializes the robot (connects to it and calibrates it).</p> <p>Parameters:</p> Name Type Description Default <code>robot</code> <code>'Robot'</code> <p>object of the Robot class.</p> required <code>use_simulation</code> <code>bool</code> <p>True, if working with a simulation model of the robot,</p> required <code>verbose</code> <code>bool</code> <code>False</code> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n    \"\"\"\n    Initializes the robot (connects to it and calibrates it).\n\n    Args:\n        robot: object of the Robot class.\n        use_simulation: True, if working with a simulation model of the robot,\n        else False if we work with a real robot.\n        verbose:\n    \"\"\"\n    self._executor = ThreadPoolExecutor(max_workers=1)\n    self._shutdown_v = False\n    self._logger = get_package_logger(__name__, verbose)\n\n    super().__init__(robot, use_simulation, verbose)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.calibrate","title":"<code>calibrate()</code>","text":"<p>Calibrates the NiryoRobot.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if calibration was successful, else False</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>def calibrate(self) -&gt; bool:\n    \"\"\"\n    Calibrates the NiryoRobot.\n\n    Returns:\n        True, if calibration was successful, else False\n    \"\"\"\n    self._calibrate_auto()\n    return True\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.cleanup","title":"<code>cleanup()</code>","text":"<p>Explicit cleanup method - call this when you're done with the object. This is more reliable than relying on del.</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>def cleanup(self):\n    \"\"\"\n    Explicit cleanup method - call this when you're done with the object.\n    This is more reliable than relying on __del__.\n    \"\"\"\n    if hasattr(self, \"_executor\") and self._executor:\n        self._logger.info(\"Shutting down ThreadPoolExecutor...\")\n        self._shutdown_v = True\n        self._executor.shutdown(wait=True)\n        self._executor = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.get_pose","title":"<code>get_pose()</code>","text":"<p>Get current pose of gripper of robot.</p> <p>Returns:</p> Type Description <code>PoseObjectPNP</code> <p>current pose of gripper of robot.</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>def get_pose(self) -&gt; PoseObjectPNP:\n    \"\"\"\n    Get current pose of gripper of robot.\n\n    Returns:\n        current pose of gripper of robot.\n    \"\"\"\n    with self._lock:\n        if pyniryo_v == \"pyniryo2\":\n            pose = self._robot_ctrl.arm.get_pose()\n        else:\n            pose = self._robot_ctrl.get_pose()\n\n    return PoseObjectPNP.convert_niryo_pose_object2pose_object(pose)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.get_target_pose_from_rel","title":"<code>get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code>","text":"<p>Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw), calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the given coordinate an object that has the given orientation. For this method to work, it is important that only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <code>u_rel</code> <code>float</code> <p>horizontal coordinate in image of workspace, normalized between 0 and 1</p> required <code>v_rel</code> <code>float</code> <p>vertical coordinate in image of workspace, normalized between 0 and 1</p> required <code>yaw</code> <code>float</code> <p>orientation of an object at the pixel coordinates [u_rel, v_rel].</p> required <p>Returns:</p> Name Type Description <code>pose_object</code> <code>'PoseObjectPNP'</code> <p>Pose of the point in world coordinates of the robot.</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n    \"\"\"\n    Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n    calculate the corresponding pose in world coordinates. The parameter yaw is useful, if we want to pick at the\n    given coordinate an object that has the given orientation. For this method to work, it is important that\n    only the workspace of the robot is visible in the image and nothing else. At least for the Niryo robot\n    this is important. This means, (u_rel, v_rel) = (0, 0), is the upper left corner of the workspace.\n\n    Args:\n        workspace_id: id of the workspace\n        u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n        v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n        yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n    Returns:\n        pose_object: Pose of the point in world coordinates of the robot.\n    \"\"\"\n    self._logger.debug(f\"Thread {threading.current_thread().name}: {workspace_id}, {u_rel}, {v_rel}, {yaw}\")\n\n    # Use the asyncio lock for thread-safe access\n    with self._lock:\n        self._logger.debug(f\"Thread {threading.current_thread().name} acquired lock\")\n\n        try:\n            x_rel = max(0.0, min(u_rel, 1.0))\n            y_rel = max(0.0, min(v_rel, 1.0))\n\n            if pyniryo_v == \"pyniryo2\":\n                obj_coords = self._robot_ctrl.vision.get_target_pose_from_rel(workspace_id, 0.0, x_rel, y_rel, yaw)\n            else:\n                obj_coords = self._robot_ctrl.get_target_pose_from_rel(workspace_id, 0.0, x_rel, y_rel, yaw)\n\n            obj_coords = PoseObjectPNP.convert_niryo_pose_object2pose_object(obj_coords)\n\n        except (NiryoRobotException, UnicodeDecodeError, SyntaxError, TcpCommandException) as e:\n            self._logger.error(f\"Thread {threading.current_thread().name} Error: {e}\", exc_info=True)\n            obj_coords = PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n        finally:\n            self._logger.debug(f\"Thread {threading.current_thread().name} releasing lock\")\n\n    self._logger.debug(f\"Thread {threading.current_thread().name} exiting: {obj_coords}\")\n\n    return obj_coords\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.move2observation_pose","title":"<code>move2observation_pose(workspace_id)</code>","text":"<p>The robot moves to a pose where it can observe the workspace given by workspace_id.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef move2observation_pose(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    The robot moves to a pose where it can observe the workspace given by workspace_id.\n\n    Args:\n        workspace_id: id of the workspace\n    \"\"\"\n    observation_pose = self._robot.environment().get_observation_pose(workspace_id)\n\n    if observation_pose is None:\n        self._logger.warning(f\"observation_pose is None for workspace: {workspace_id}\")\n        return\n\n    # Only convert if we have a valid pose\n    observation_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(observation_pose)\n\n    self._set_in_motion(True)\n    try:\n        with self._lock:\n            self._move_pose(observation_pose)\n    except UnicodeDecodeError as e:\n        self._logger.error(f\"move2observation_pose error: {e}, pose: {observation_pose}\", exc_info=True)\n    finally:\n        self._set_in_motion(False)\n\n    self._logger.debug(f\"move_pose finished, current: {self.get_pose()}, target: {observation_pose}\")\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.reset_connection","title":"<code>reset_connection()</code>","text":"<p>Reset the connection to the robot by safely disconnecting and reconnecting.</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>def reset_connection(self) -&gt; None:\n    \"\"\"\n    Reset the connection to the robot by safely disconnecting and reconnecting.\n    \"\"\"\n    self._logger.info(\"Resetting the robot connection...\")\n    try:\n        # Attempt to close the connection safely\n        if self._robot_ctrl is not None:\n            with self._lock:\n                self._shutdown()\n\n    except Exception as e:\n        self._logger.error(f\"Error while closing connection: {e}\", exc_info=True)\n\n    # Reinitialize the connection\n    try:\n        self._create_robot()\n        self._logger.info(\"Connection successfully reset.\")\n    except Exception as e:\n        self._logger.error(f\"Failed to reconnect to the robot: {e}\", exc_info=True)\n        self._robot = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.robot_pick_object","title":"<code>robot_pick_object(pick_pose)</code>","text":"<p>Calls the pick command of the self._robot_ctrl to pick the object at the given pose.</p> <p>Parameters:</p> Name Type Description Default <code>pick_pose</code> <code>'PoseObjectPNP'</code> <p>Pose where to pick the object (z-offset already applied if needed)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if pick was successful, else False</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Calls the pick command of the self._robot_ctrl to pick the object at the given pose.\n\n    Args:\n        pick_pose: Pose where to pick the object (z-offset already applied if needed)\n\n    Returns:\n        True, if pick was successful, else False\n    \"\"\"\n    # Convert to Niryo format\n    pick_pose_niryo = PoseObjectPNP.convert_pose_object2niryo_pose_object(pick_pose)\n\n    self._set_in_motion(True)\n    try:\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                self._robot_ctrl.pick_place.pick_from_pose(pick_pose_niryo)\n            else:\n                self._robot_ctrl.pick_from_pose(pick_pose_niryo)\n\n        self._logger.info(\"Finished pick_from_pose\")\n        return True\n        # TODO: in newest version available\n        # return not self._robot_ctrl.collision_detected\n\n    except Exception as e:\n        self._logger.error(f\"Pick failed: {e}\")\n        return False\n    finally:\n        self._set_in_motion(False)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.robot_place_object","title":"<code>robot_place_object(place_pose)</code>","text":"<p>Places an already picked object at the given place_pose.</p> <p>Parameters:</p> Name Type Description Default <code>place_pose</code> <code>'PoseObjectPNP'</code> <p>Pose where to place the already picked object</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if place was successful, else False</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Places an already picked object at the given place_pose.\n\n    Args:\n        place_pose: Pose where to place the already picked object\n\n    Returns:\n        True, if place was successful, else False\n    \"\"\"\n    place_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(place_pose)\n    place_pose = place_pose.copy_with_offsets(z_offset=0.005)\n\n    self._logger.debug(f\"Place pose: {place_pose}\")\n\n    self._set_in_motion(True)\n    try:\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                self._robot_ctrl.pick_place.place_from_pose(place_pose)\n            else:\n                self._robot_ctrl.place_from_pose(place_pose)\n\n        return True\n        # TODO: in newest version available\n        # return not self._robot_ctrl.collision_detected\n    except Exception as e:\n        self._logger.error(f\"Place failed: {e}\")\n        return False\n    finally:\n        self._set_in_motion(False)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.niryo_robot_controller.NiryoRobotController.robot_push_object","title":"<code>robot_push_object(push_pose, direction, distance)</code>","text":"<p>Push given object (its Pose) into the given direction by the given distance.</p> <p>Parameters:</p> Name Type Description Default <code>push_pose</code> <code>'PoseObjectPNP'</code> <p>the Pose of the object that should be pushed.</p> required <code>direction</code> <code>str</code> <p>\"up\", \"down\", \"left\", \"right\"</p> required <code>distance</code> <code>float</code> <p>distance in millimeters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if push was successful, else False</p> Source code in <code>robot_environment/robot/niryo_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n    \"\"\"\n    Push given object (its Pose) into the given direction by the given distance.\n\n    Args:\n        push_pose: the Pose of the object that should be pushed.\n        direction: \"up\", \"down\", \"left\", \"right\"\n        distance: distance in millimeters\n\n    Returns:\n        True, if push was successful, else False\n    \"\"\"\n    push_pose = PoseObjectPNP.convert_pose_object2niryo_pose_object(push_pose)\n\n    self._logger.debug(f\"Push pose: {push_pose}\")\n\n    self._set_in_motion(True)\n    try:\n        with self._lock:\n            if pyniryo_v == \"pyniryo2\":\n                self._robot_ctrl.tool.close_gripper()\n            else:\n                self._robot_ctrl.close_gripper()\n\n            self._move_pose(push_pose)\n\n            if direction == \"up\":\n                self._shift_pose(RobotAxis.X, distance)\n            elif direction == \"down\":\n                self._shift_pose(RobotAxis.X, -distance)\n            elif direction == \"left\":\n                self._shift_pose(RobotAxis.Y, distance)\n            elif direction == \"right\":\n                self._shift_pose(RobotAxis.Y, -distance)\n            else:\n                self._logger.error(f\"Unknown direction: {direction}\")\n\n        return True\n        # TODO: in newest version available\n        # return not self._robot_ctrl.collision_detected\n    except Exception as e:\n        self._logger.error(f\"Push failed: {e}\")\n        return False\n    finally:\n        self._set_in_motion(False)\n</code></pre>"},{"location":"api/robot/#widowxrobotcontroller","title":"WidowXRobotController","text":""},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController","title":"<code>robot_environment.robot.widowx_robot_controller.WidowXRobotController</code>","text":"<p>               Bases: <code>RobotController</code></p> <p>Class for the pick-and-place WidowX robot that provides the primitive tasks of the robot like pick and place operations using the InterbotixManipulatorXS interface.</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>class WidowXRobotController(RobotController):\n    \"\"\"\n    Class for the pick-and-place WidowX robot that provides the primitive tasks of the robot like\n    pick and place operations using the InterbotixManipulatorXS interface.\n    \"\"\"\n\n    # Default pose used when the actual robot pose cannot be determined\n    # Typically corresponds to the home position\n    DEFAULT_HOME_POSE = PoseObjectPNP(0.3, 0.0, 0.2, 0.0, 1.57, 0.0)\n\n    # *** CONSTRUCTORS ***\n    @log_start_end_cls()\n    def __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n        \"\"\"\n        Initializes the robot (connects to it and sets up the interface).\n\n        Args:\n            robot: object of the Robot class.\n            use_simulation: True, if working with a simulation model of the robot,\n                else False if we work with a real robot.\n            verbose: enable verbose output\n        \"\"\"\n        if not INTERBOTIX_AVAILABLE:\n            raise ImportError(\"interbotix_xs_modules is required for WidowX controller\")\n\n        super().__init__(robot, use_simulation, verbose)\n\n    # Deleting (Calling destructor)\n    def __del__(self):\n        super().__del__()\n        if hasattr(self, \"_robot_ctrl\") and self._robot_ctrl is not None:\n            if self.verbose():\n                print(\"Shutting down WidowX robot controller...\")\n            with self._lock:\n                self._shutdown()\n\n    # *** PUBLIC GET methods ***\n\n    def get_pose(self) -&gt; PoseObjectPNP:\n        \"\"\"\n        Get current pose of gripper of robot.\n\n        Returns:\n            current pose of gripper of robot.\n        \"\"\"\n        with self._lock:\n            try:\n                # InterbotixManipulatorXS stores current pose internally\n                # This is a simplified version - in practice you'd use the arm's FK\n                # or track the last commanded pose\n                if hasattr(self, \"_last_pose\") and self._last_pose is not None:\n                    return self._last_pose\n                else:\n                    # Return default home pose as fallback\n                    return self.DEFAULT_HOME_POSE\n            except Exception as e:\n                if self.verbose():\n                    print(f\"Error getting pose: {e}\")\n                return self.DEFAULT_HOME_POSE\n\n    def get_camera_intrinsics(self):\n        \"\"\"\n        Get camera intrinsics for the WidowX camera (if available).\n\n        Returns:\n            tuple: (camera_matrix, distortion_coefficients)\n        \"\"\"\n        # WidowX typically uses external camera (e.g., RealSense)\n        # These are placeholder values - should be calibrated for your setup\n\n        # Default camera matrix for Intel RealSense D435 (640x480)\n        mtx = np.array([[615.0, 0.0, 320.0], [0.0, 615.0, 240.0], [0.0, 0.0, 1.0]])\n\n        # Distortion coefficients (k1, k2, p1, p2, k3)\n        dist = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n        return mtx, dist\n\n    # *** PUBLIC methods ***\n\n    def calibrate(self) -&gt; bool:\n        \"\"\"\n        Calibrates the WidowX robot.\n\n        Returns:\n            True, if calibration was successful, else False\n        \"\"\"\n        # TODO: implement calibration\n\n        return True\n\n    def reset_connection(self) -&gt; None:\n        \"\"\"\n        Reset the connection to the robot by safely disconnecting and reconnecting.\n        \"\"\"\n        if self.verbose():\n            print(\"Resetting WidowX connection...\")\n        try:\n            if self._robot_ctrl is not None:\n                with self._lock:\n                    self._shutdown()\n        except Exception as e:\n            print(f\"Error while closing connection: {e}\")\n\n        # Reinitialize the connection\n        try:\n            self._create_robot()\n            if self.verbose():\n                print(\"Connection successfully reset.\")\n        except Exception as e:\n            print(f\"Failed to reconnect to the robot: {e}\")\n            self._robot_ctrl = None\n\n    @log_start_end_cls()\n    def robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Calls the pick command to pick an object at the given pose.\n\n        Args:\n            pick_pose: Pose where to pick the object (z-offset already applied if needed)\n\n        Returns:\n            True, if pick was successful, else False\n        \"\"\"\n        try:\n            # Add small z-offset for approach (additional to any z-offset already in pick_pose)\n            pick_pose_approach = pick_pose.copy_with_offsets(z_offset=0.05)\n            pick_pose_grasp = pick_pose  # Use the pose as-is (z-offset already applied)\n\n            with self._lock:\n                # Open gripper\n                self._robot_ctrl.gripper.release()\n\n                # Move to approach pose (above object)\n                self._move_to_pose(pick_pose_approach)\n\n                # Move down to grasp pose\n                self._move_to_pose(pick_pose_grasp)\n\n                # Close gripper to grasp\n                self._robot_ctrl.gripper.grasp()\n\n                # Lift object\n                lift_pose = pick_pose.copy_with_offsets(z_offset=0.05)\n                self._move_to_pose(lift_pose)\n\n            if self.verbose():\n                print(\"Pick operation completed successfully\")\n\n            return True\n\n        except Exception as e:\n            print(f\"Error during pick operation: {e}\")\n            return False\n\n    @log_start_end_cls()\n    def robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n        \"\"\"\n        Places an already picked object at the given place_pose.\n\n        Args:\n            place_pose: Pose where to place the already picked object\n\n        Returns:\n            True, if place was successful, else False\n        \"\"\"\n        try:\n            # Add z-offset for approach\n            place_pose_approach = place_pose.copy_with_offsets(z_offset=0.05)\n            place_pose_final = place_pose.copy_with_offsets(z_offset=0.005)\n\n            with self._lock:\n                # Move to approach pose\n                self._move_to_pose(place_pose_approach)\n\n                # Move down to place pose\n                self._move_to_pose(place_pose_final)\n\n                # Release gripper\n                self._robot_ctrl.gripper.release()\n\n                # Retract\n                self._move_to_pose(place_pose_approach)\n\n            if self.verbose():\n                print(\"Place operation completed successfully\")\n\n            return True\n\n        except Exception as e:\n            print(f\"Error during place operation: {e}\")\n            return False\n\n    @log_start_end_cls()\n    def robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n        \"\"\"\n        Push given object (its Pose) into the given direction by the given distance.\n\n        Args:\n            push_pose: the Pose of the object that should be pushed.\n            direction: \"up\", \"down\", \"left\", \"right\"\n            distance: distance in millimeters\n\n        Returns:\n            True, if push was successful, else False\n        \"\"\"\n        try:\n            with self._lock:\n                # Close gripper first\n                self._robot_ctrl.gripper.release()\n\n                # Move to push starting position\n                self._move_to_pose(push_pose)\n\n                # Calculate push distance in meters\n                push_dist_m = distance / 1000.0\n\n                # Perform push based on direction\n                if direction == \"up\":\n                    # Push along positive X axis\n                    self._robot_ctrl.arm.set_ee_cartesian_trajectory(x=push_dist_m)\n                elif direction == \"down\":\n                    # Push along negative X axis\n                    self._robot_ctrl.arm.set_ee_cartesian_trajectory(x=-push_dist_m)\n                elif direction == \"left\":\n                    # Push along positive Y axis\n                    self._robot_ctrl.arm.set_ee_cartesian_trajectory(y=push_dist_m)\n                elif direction == \"right\":\n                    # Push along negative Y axis\n                    self._robot_ctrl.arm.set_ee_cartesian_trajectory(y=-push_dist_m)\n                else:\n                    print(f\"Unknown direction: {direction}\")\n                    return False\n\n            if self.verbose():\n                print(f\"Push operation completed: {direction}, {distance}mm\")\n\n            return True\n\n        except Exception as e:\n            print(f\"Error during push operation: {e}\")\n            return False\n\n    def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n        \"\"\"\n        Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n        calculate the corresponding pose in world coordinates.\n\n        Args:\n            workspace_id: id of the workspace\n            u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n            v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n            yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n        Returns:\n            pose_object: Pose of the point in world coordinates of the robot.\n        \"\"\"\n        with self._lock:\n            try:\n                # Clamp coordinates to [0, 1]\n                u_rel = max(0.0, min(u_rel, 1.0))\n                v_rel = max(0.0, min(v_rel, 1.0))\n\n                # Get workspace from environment\n                workspace = self._robot.environment().get_workspace_by_id(workspace_id)\n\n                if workspace is None:\n                    if self.verbose():\n                        print(f\"Workspace {workspace_id} not found\")\n                    return PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n                # Use workspace transformation\n                pose = workspace.transform_camera2world_coords(workspace_id, u_rel, v_rel, yaw)\n\n                return pose\n\n            except Exception as e:\n                if self.verbose():\n                    print(f\"Error in get_target_pose_from_rel: {e}\")\n                return PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n    @log_start_end_cls()\n    def move2observation_pose(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        The robot moves to a pose where it can observe the workspace given by workspace_id.\n\n        Args:\n            workspace_id: id of the workspace\n        \"\"\"\n        try:\n            observation_pose = self._robot.environment().get_observation_pose(workspace_id)\n\n            if observation_pose is None:\n                if self.verbose():\n                    print(f\"No observation pose defined for workspace: {workspace_id}\")\n                return\n\n            with self._lock:\n                self._move_to_pose(observation_pose)\n\n            if self.verbose():\n                print(f\"Moved to observation pose for workspace: {workspace_id}\")\n\n        except Exception as e:\n            print(f\"Error moving to observation pose: {e}\")\n\n    # *** PRIVATE methods ***\n\n    def _shutdown(self) -&gt; None:\n        \"\"\"\n        Closes connection to InterbotixManipulatorXS.\n        \"\"\"\n        if self._robot_ctrl is not None:\n            try:\n                # Move to sleep pose before shutdown\n                self._robot_ctrl.arm.go_to_sleep_pose()\n                # Shutdown the robot interface\n                self._robot_ctrl.shutdown()\n            except Exception as e:\n                print(f\"Error during shutdown: {e}\")\n\n    def _move_to_pose(self, pose: \"PoseObjectPNP\") -&gt; None:\n        \"\"\"\n        Move gripper of robot to given pose using set_ee_pose_components.\n\n        Args:\n            pose: pose of gripper (PoseObjectPNP)\n        \"\"\"\n        try:\n            # Use set_ee_pose_components method\n            # Note: InterbotixManipulatorXS uses roll, pitch for orientation\n            self._robot_ctrl.arm.set_ee_pose_components(x=pose.x, y=pose.y, z=pose.z, roll=pose.roll, pitch=pose.pitch)\n\n            # Store last commanded pose\n            self._last_pose = pose\n\n        except Exception as e:\n            if self.verbose():\n                print(f\"Error moving to pose: {e}\")\n            raise\n\n    @log_start_end_cls()\n    def _create_robot(self) -&gt; None:\n        \"\"\"\n        Creates the InterbotixManipulatorXS object and initializes the robot.\n        \"\"\"\n        with self._lock:\n            # Create robot interface\n            self._robot_ctrl = InterbotixManipulatorXS(\n                robot_model=\"wx250s\",\n                group_name=\"arm\",\n                gripper_name=\"gripper\",\n            )\n\n            # Move to home pose on initialization\n            self._robot_ctrl.arm.go_to_home_pose()\n\n            # Initialize last pose\n            self._last_pose = self.DEFAULT_HOME_POSE\n\n    @log_start_end_cls()\n    def _init_robot(self, use_simulation: bool) -&gt; bool:\n        \"\"\"\n        Creates the InterbotixManipulatorXS object and connects to it.\n\n        Args:\n            use_simulation: Currently not used for WidowX (uses ROS parameter)\n\n        Returns:\n            bool: True if initialization was successful, else False\n        \"\"\"\n        try:\n            self._create_robot()\n\n            if self.verbose():\n                print(\"WidowX robot initialized successfully\")\n\n            return True\n\n        except Exception as e:\n            print(f\"Failed to initialize WidowX robot: {e}\")\n            return False\n\n    # *** PUBLIC properties ***\n\n    # *** PRIVATE variables ***\n\n    # Last commanded pose (for tracking)\n    _last_pose = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController-functions","title":"Functions","text":""},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.__init__","title":"<code>__init__(robot, use_simulation, verbose=False)</code>","text":"<p>Initializes the robot (connects to it and sets up the interface).</p> <p>Parameters:</p> Name Type Description Default <code>robot</code> <code>'Robot'</code> <p>object of the Robot class.</p> required <code>use_simulation</code> <code>bool</code> <p>True, if working with a simulation model of the robot, else False if we work with a real robot.</p> required <code>verbose</code> <code>bool</code> <p>enable verbose output</p> <code>False</code> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef __init__(self, robot: \"Robot\", use_simulation: bool, verbose: bool = False):\n    \"\"\"\n    Initializes the robot (connects to it and sets up the interface).\n\n    Args:\n        robot: object of the Robot class.\n        use_simulation: True, if working with a simulation model of the robot,\n            else False if we work with a real robot.\n        verbose: enable verbose output\n    \"\"\"\n    if not INTERBOTIX_AVAILABLE:\n        raise ImportError(\"interbotix_xs_modules is required for WidowX controller\")\n\n    super().__init__(robot, use_simulation, verbose)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.calibrate","title":"<code>calibrate()</code>","text":"<p>Calibrates the WidowX robot.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True, if calibration was successful, else False</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>def calibrate(self) -&gt; bool:\n    \"\"\"\n    Calibrates the WidowX robot.\n\n    Returns:\n        True, if calibration was successful, else False\n    \"\"\"\n    # TODO: implement calibration\n\n    return True\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.get_camera_intrinsics","title":"<code>get_camera_intrinsics()</code>","text":"<p>Get camera intrinsics for the WidowX camera (if available).</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(camera_matrix, distortion_coefficients)</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>def get_camera_intrinsics(self):\n    \"\"\"\n    Get camera intrinsics for the WidowX camera (if available).\n\n    Returns:\n        tuple: (camera_matrix, distortion_coefficients)\n    \"\"\"\n    # WidowX typically uses external camera (e.g., RealSense)\n    # These are placeholder values - should be calibrated for your setup\n\n    # Default camera matrix for Intel RealSense D435 (640x480)\n    mtx = np.array([[615.0, 0.0, 320.0], [0.0, 615.0, 240.0], [0.0, 0.0, 1.0]])\n\n    # Distortion coefficients (k1, k2, p1, p2, k3)\n    dist = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n\n    return mtx, dist\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.get_pose","title":"<code>get_pose()</code>","text":"<p>Get current pose of gripper of robot.</p> <p>Returns:</p> Type Description <code>PoseObjectPNP</code> <p>current pose of gripper of robot.</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>def get_pose(self) -&gt; PoseObjectPNP:\n    \"\"\"\n    Get current pose of gripper of robot.\n\n    Returns:\n        current pose of gripper of robot.\n    \"\"\"\n    with self._lock:\n        try:\n            # InterbotixManipulatorXS stores current pose internally\n            # This is a simplified version - in practice you'd use the arm's FK\n            # or track the last commanded pose\n            if hasattr(self, \"_last_pose\") and self._last_pose is not None:\n                return self._last_pose\n            else:\n                # Return default home pose as fallback\n                return self.DEFAULT_HOME_POSE\n        except Exception as e:\n            if self.verbose():\n                print(f\"Error getting pose: {e}\")\n            return self.DEFAULT_HOME_POSE\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.get_target_pose_from_rel","title":"<code>get_target_pose_from_rel(workspace_id, u_rel, v_rel, yaw)</code>","text":"<p>Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw), calculate the corresponding pose in world coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required <code>u_rel</code> <code>float</code> <p>horizontal coordinate in image of workspace, normalized between 0 and 1</p> required <code>v_rel</code> <code>float</code> <p>vertical coordinate in image of workspace, normalized between 0 and 1</p> required <code>yaw</code> <code>float</code> <p>orientation of an object at the pixel coordinates [u_rel, v_rel].</p> required <p>Returns:</p> Name Type Description <code>pose_object</code> <code>'PoseObjectPNP'</code> <p>Pose of the point in world coordinates of the robot.</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>def get_target_pose_from_rel(self, workspace_id: str, u_rel: float, v_rel: float, yaw: float) -&gt; \"PoseObjectPNP\":\n    \"\"\"\n    Given relative image coordinates [u_rel, v_rel] and optionally an orientation of the point (yaw),\n    calculate the corresponding pose in world coordinates.\n\n    Args:\n        workspace_id: id of the workspace\n        u_rel: horizontal coordinate in image of workspace, normalized between 0 and 1\n        v_rel: vertical coordinate in image of workspace, normalized between 0 and 1\n        yaw: orientation of an object at the pixel coordinates [u_rel, v_rel].\n\n    Returns:\n        pose_object: Pose of the point in world coordinates of the robot.\n    \"\"\"\n    with self._lock:\n        try:\n            # Clamp coordinates to [0, 1]\n            u_rel = max(0.0, min(u_rel, 1.0))\n            v_rel = max(0.0, min(v_rel, 1.0))\n\n            # Get workspace from environment\n            workspace = self._robot.environment().get_workspace_by_id(workspace_id)\n\n            if workspace is None:\n                if self.verbose():\n                    print(f\"Workspace {workspace_id} not found\")\n                return PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\n            # Use workspace transformation\n            pose = workspace.transform_camera2world_coords(workspace_id, u_rel, v_rel, yaw)\n\n            return pose\n\n        except Exception as e:\n            if self.verbose():\n                print(f\"Error in get_target_pose_from_rel: {e}\")\n            return PoseObjectPNP(0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.move2observation_pose","title":"<code>move2observation_pose(workspace_id)</code>","text":"<p>The robot moves to a pose where it can observe the workspace given by workspace_id.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>id of the workspace</p> required Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef move2observation_pose(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    The robot moves to a pose where it can observe the workspace given by workspace_id.\n\n    Args:\n        workspace_id: id of the workspace\n    \"\"\"\n    try:\n        observation_pose = self._robot.environment().get_observation_pose(workspace_id)\n\n        if observation_pose is None:\n            if self.verbose():\n                print(f\"No observation pose defined for workspace: {workspace_id}\")\n            return\n\n        with self._lock:\n            self._move_to_pose(observation_pose)\n\n        if self.verbose():\n            print(f\"Moved to observation pose for workspace: {workspace_id}\")\n\n    except Exception as e:\n        print(f\"Error moving to observation pose: {e}\")\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.reset_connection","title":"<code>reset_connection()</code>","text":"<p>Reset the connection to the robot by safely disconnecting and reconnecting.</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>def reset_connection(self) -&gt; None:\n    \"\"\"\n    Reset the connection to the robot by safely disconnecting and reconnecting.\n    \"\"\"\n    if self.verbose():\n        print(\"Resetting WidowX connection...\")\n    try:\n        if self._robot_ctrl is not None:\n            with self._lock:\n                self._shutdown()\n    except Exception as e:\n        print(f\"Error while closing connection: {e}\")\n\n    # Reinitialize the connection\n    try:\n        self._create_robot()\n        if self.verbose():\n            print(\"Connection successfully reset.\")\n    except Exception as e:\n        print(f\"Failed to reconnect to the robot: {e}\")\n        self._robot_ctrl = None\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.robot_pick_object","title":"<code>robot_pick_object(pick_pose)</code>","text":"<p>Calls the pick command to pick an object at the given pose.</p> <p>Parameters:</p> Name Type Description Default <code>pick_pose</code> <code>'PoseObjectPNP'</code> <p>Pose where to pick the object (z-offset already applied if needed)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if pick was successful, else False</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_pick_object(self, pick_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Calls the pick command to pick an object at the given pose.\n\n    Args:\n        pick_pose: Pose where to pick the object (z-offset already applied if needed)\n\n    Returns:\n        True, if pick was successful, else False\n    \"\"\"\n    try:\n        # Add small z-offset for approach (additional to any z-offset already in pick_pose)\n        pick_pose_approach = pick_pose.copy_with_offsets(z_offset=0.05)\n        pick_pose_grasp = pick_pose  # Use the pose as-is (z-offset already applied)\n\n        with self._lock:\n            # Open gripper\n            self._robot_ctrl.gripper.release()\n\n            # Move to approach pose (above object)\n            self._move_to_pose(pick_pose_approach)\n\n            # Move down to grasp pose\n            self._move_to_pose(pick_pose_grasp)\n\n            # Close gripper to grasp\n            self._robot_ctrl.gripper.grasp()\n\n            # Lift object\n            lift_pose = pick_pose.copy_with_offsets(z_offset=0.05)\n            self._move_to_pose(lift_pose)\n\n        if self.verbose():\n            print(\"Pick operation completed successfully\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error during pick operation: {e}\")\n        return False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.robot_place_object","title":"<code>robot_place_object(place_pose)</code>","text":"<p>Places an already picked object at the given place_pose.</p> <p>Parameters:</p> Name Type Description Default <code>place_pose</code> <code>'PoseObjectPNP'</code> <p>Pose where to place the already picked object</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if place was successful, else False</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_place_object(self, place_pose: \"PoseObjectPNP\") -&gt; bool:\n    \"\"\"\n    Places an already picked object at the given place_pose.\n\n    Args:\n        place_pose: Pose where to place the already picked object\n\n    Returns:\n        True, if place was successful, else False\n    \"\"\"\n    try:\n        # Add z-offset for approach\n        place_pose_approach = place_pose.copy_with_offsets(z_offset=0.05)\n        place_pose_final = place_pose.copy_with_offsets(z_offset=0.005)\n\n        with self._lock:\n            # Move to approach pose\n            self._move_to_pose(place_pose_approach)\n\n            # Move down to place pose\n            self._move_to_pose(place_pose_final)\n\n            # Release gripper\n            self._robot_ctrl.gripper.release()\n\n            # Retract\n            self._move_to_pose(place_pose_approach)\n\n        if self.verbose():\n            print(\"Place operation completed successfully\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error during place operation: {e}\")\n        return False\n</code></pre>"},{"location":"api/robot/#robot_environment.robot.widowx_robot_controller.WidowXRobotController.robot_push_object","title":"<code>robot_push_object(push_pose, direction, distance)</code>","text":"<p>Push given object (its Pose) into the given direction by the given distance.</p> <p>Parameters:</p> Name Type Description Default <code>push_pose</code> <code>'PoseObjectPNP'</code> <p>the Pose of the object that should be pushed.</p> required <code>direction</code> <code>str</code> <p>\"up\", \"down\", \"left\", \"right\"</p> required <code>distance</code> <code>float</code> <p>distance in millimeters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, if push was successful, else False</p> Source code in <code>robot_environment/robot/widowx_robot_controller.py</code> <pre><code>@log_start_end_cls()\ndef robot_push_object(self, push_pose: \"PoseObjectPNP\", direction: str, distance: float) -&gt; bool:\n    \"\"\"\n    Push given object (its Pose) into the given direction by the given distance.\n\n    Args:\n        push_pose: the Pose of the object that should be pushed.\n        direction: \"up\", \"down\", \"left\", \"right\"\n        distance: distance in millimeters\n\n    Returns:\n        True, if push was successful, else False\n    \"\"\"\n    try:\n        with self._lock:\n            # Close gripper first\n            self._robot_ctrl.gripper.release()\n\n            # Move to push starting position\n            self._move_to_pose(push_pose)\n\n            # Calculate push distance in meters\n            push_dist_m = distance / 1000.0\n\n            # Perform push based on direction\n            if direction == \"up\":\n                # Push along positive X axis\n                self._robot_ctrl.arm.set_ee_cartesian_trajectory(x=push_dist_m)\n            elif direction == \"down\":\n                # Push along negative X axis\n                self._robot_ctrl.arm.set_ee_cartesian_trajectory(x=-push_dist_m)\n            elif direction == \"left\":\n                # Push along positive Y axis\n                self._robot_ctrl.arm.set_ee_cartesian_trajectory(y=push_dist_m)\n            elif direction == \"right\":\n                # Push along negative Y axis\n                self._robot_ctrl.arm.set_ee_cartesian_trajectory(y=-push_dist_m)\n            else:\n                print(f\"Unknown direction: {direction}\")\n                return False\n\n        if self.verbose():\n            print(f\"Push operation completed: {direction}, {distance}mm\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error during push operation: {e}\")\n        return False\n</code></pre>"},{"location":"api/utils/","title":"Utilities","text":""},{"location":"api/utils/#workspace-utils","title":"Workspace Utils","text":""},{"location":"api/utils/#robot_environment.utils.workspace_utils","title":"<code>robot_environment.utils.workspace_utils</code>","text":""},{"location":"api/utils/#robot_environment.utils.workspace_utils-functions","title":"Functions","text":""},{"location":"api/utils/#robot_environment.utils.workspace_utils.calculate_largest_free_space","title":"<code>calculate_largest_free_space(workspace, detected_objects, grid_resolution=100, visualize=False, logger=None)</code>","text":"<p>Determines the largest free space in the workspace in square metres and its center coordinate in metres.</p> <p>Parameters:</p> Name Type Description Default <code>workspace</code> <code>Workspace</code> <p>The workspace object to analyze.</p> required <code>detected_objects</code> <code>Objects</code> <p>Collection of objects detected in the workspace.</p> required <code>grid_resolution</code> <code>int</code> <p>Resolution of the workspace grid (default: 100x100).</p> <code>100</code> <code>visualize</code> <code>bool</code> <p>If True, displays the grid visualization (requires GUI).</p> <code>False</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger for debug information.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[float, float, float]</code> <p>(largest_area_m2, center_x, center_y)</p> Source code in <code>robot_environment/utils/workspace_utils.py</code> <pre><code>def calculate_largest_free_space(\n    workspace: Workspace,\n    detected_objects: Objects,\n    grid_resolution: int = 100,\n    visualize: bool = False,\n    logger: Optional[logging.Logger] = None,\n) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Determines the largest free space in the workspace in square metres and its center coordinate in metres.\n\n    Args:\n        workspace: The workspace object to analyze.\n        detected_objects: Collection of objects detected in the workspace.\n        grid_resolution: Resolution of the workspace grid (default: 100x100).\n        visualize: If True, displays the grid visualization (requires GUI).\n        logger: Optional logger for debug information.\n\n    Returns:\n        tuple: (largest_area_m2, center_x, center_y)\n    \"\"\"\n    if logger is None:\n        logger = logging.getLogger(__name__)\n\n    workspace_top_left = workspace.xy_ul_wc()\n    workspace_bottom_right = workspace.xy_lr_wc()\n\n    x_max, y_max = workspace_top_left.x, workspace_top_left.y\n    x_min, y_min = workspace_bottom_right.x, workspace_bottom_right.y\n\n    logger.debug(f\"Workspace bounds: x=[{x_min}, {x_max}], y=[{y_min}, {y_max}]\")\n\n    workspace_width = abs(y_max - y_min)\n    workspace_height = abs(x_max - x_min)\n\n    # Create a grid to represent the workspace\n    grid = np.zeros((grid_resolution, grid_resolution), dtype=int)\n\n    # Map world coordinates to grid indices\n    def to_grid_coords(x, y):\n        v = int((x_max - x) / workspace_height * grid_resolution)\n        u = int((y_max - y) / workspace_width * grid_resolution)\n        # Clip to ensure indices are within grid bounds\n        v = max(0, min(v, grid_resolution - 1))\n        u = max(0, min(u, grid_resolution - 1))\n        return u, v\n\n    # Map grid indices back to world coordinates\n    def to_world_coords(u, v):\n        x = x_max - (v + 0.5) * (workspace_height / grid_resolution)\n        y = y_max - (u + 0.5) * (workspace_width / grid_resolution)\n        return x, y\n\n    # Mark the grid cells occupied by objects\n    for obj in detected_objects:\n        x_start = obj.x_com() - obj.height_m() / 2\n        x_end = obj.x_com() + obj.height_m() / 2\n        y_start = obj.y_com() - obj.width_m() / 2\n        y_end = obj.y_com() + obj.width_m() / 2\n\n        # Convert object bounds to grid indices\n        u_end, v_end = to_grid_coords(x_start, y_start)\n        u_start, v_start = to_grid_coords(x_end, y_end)\n\n        logger.debug(f\"Object bounds: x=[{x_start}, {x_end}], y=[{y_start}, {y_end}]\")\n        logger.debug(f\"Grid coords: u=[{u_start}, {u_end}], v=[{v_start}, {v_end}]\")\n\n        # Mark grid cells as occupied (ensuring correct order for slicing)\n        v_min_idx = min(v_start, v_end)\n        v_max_idx = max(v_start, v_end)\n        u_min_idx = min(u_start, u_end)\n        u_max_idx = max(u_start, u_end)\n        grid[v_min_idx : v_max_idx + 1, u_min_idx : u_max_idx + 1] = 1\n\n    # Find the largest rectangle of zeros in the grid\n    largest_area_cells, (v_start_rect, u_start_rect), (v_end_rect, u_end_rect) = _max_rectangle_area(grid)\n    largest_area_m2 = (largest_area_cells / (grid_resolution**2)) * (workspace_width * workspace_height)\n\n    # Calculate the center of the largest rectangle in grid coordinates\n    v_center = (v_start_rect + v_end_rect) // 2\n    u_center = (u_start_rect + u_end_rect) // 2\n\n    # Map the center to world coordinates\n    center_x, center_y = to_world_coords(u_center, v_center)\n\n    if visualize:\n        try:\n            # Mark center in the grid for visualization\n            grid_vis = grid.copy()\n            grid_vis[v_center : v_center + 1, u_center : u_center + 1] = 2\n            # Normalize grid to 0\u2013255 for visualization\n            grid_visual = (grid_vis * 255 // 2).astype(np.uint8)\n            cv2.imshow(\"Largest Free Space Grid\", grid_visual)\n            cv2.waitKey(1)  # Use 1 instead of 0 to avoid blocking in non-interactive mode\n        except Exception as e:\n            logger.warning(f\"Could not visualize free space grid: {e}\")\n\n    logger.info(f\"Largest free area: {largest_area_m2:.4f} square meters\")\n    logger.info(f\"Center: ({center_x:.4f}, {center_y:.4f}) meters\")\n\n    return largest_area_m2, center_x, center_y\n</code></pre>"},{"location":"api/utils/#object-memory-manager","title":"Object Memory Manager","text":""},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager","title":"<code>robot_environment.object_memory_manager.ObjectMemoryManager</code>","text":"<p>Manages object position memory with workspace tracking.</p> <p>Features: - Multi-workspace memory management - Manual update tracking (for pick/place operations) - Thread-safe operations - Workspace visibility state tracking - Intelligent memory clearing based on robot state</p> Example <p>manager = ObjectMemoryManager(verbose=True)</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>class ObjectMemoryManager:\n    \"\"\"\n    Manages object position memory with workspace tracking.\n\n    Features:\n    - Multi-workspace memory management\n    - Manual update tracking (for pick/place operations)\n    - Thread-safe operations\n    - Workspace visibility state tracking\n    - Intelligent memory clearing based on robot state\n\n    Example:\n        manager = ObjectMemoryManager(verbose=True)\n\n        # Initialize workspaces\n        manager.initialize_workspace(\"niryo_ws\")\n        manager.initialize_workspace(\"niryo_ws_left\")\n\n        # Update memory with detected objects\n        manager.update(\"niryo_ws\", detected_objects, at_observation=True)\n\n        # Get objects from memory\n        objects = manager.get(\"niryo_ws\")\n\n        # Manual update after placing object\n        manager.mark_manual_update(\"niryo_ws\", \"cube\",\n                                   old_coord=[0.2, 0.0],\n                                   new_pose=new_pose)\n    \"\"\"\n\n    def __init__(self, manual_update_timeout: float = 5.0, position_tolerance: float = 0.05, verbose: bool = False):\n        \"\"\"\n        Initialize the Object Memory Manager.\n\n        Args:\n            manual_update_timeout: Seconds to keep manual updates (default: 5.0)\n            position_tolerance: Distance threshold for duplicate detection in meters (default: 0.05)\n            verbose: Enable verbose logging (default: False)\n        \"\"\"\n        # Workspace memories\n        self._memories: Dict[str, Objects] = {}\n\n        # Thread safety (using RLock to allow reentrant calls from update/move_object to initialize_workspace)\n        self._lock = threading.RLock()\n\n        # Manual updates tracking: {workspace_id: {object_label: timestamp}}\n        self._manual_updates: Dict[str, Dict[str, float]] = {}\n\n        # Configuration\n        self._manual_update_timeout = manual_update_timeout\n        self._position_tolerance = position_tolerance\n\n        # State tracking\n        self._workspace_visibility: Dict[str, bool] = {}\n        self._workspace_was_lost: Dict[str, bool] = {}\n\n        # Logging\n        self._verbose = verbose\n        self._logger = logging.getLogger(__name__)\n        if verbose:\n            self._logger.setLevel(logging.DEBUG)\n\n    def initialize_workspace(self, workspace_id: str) -&gt; None:\n        \"\"\"\n        Initialize memory for a new workspace.\n\n        Args:\n            workspace_id: ID of the workspace to initialize\n        \"\"\"\n        with self._lock:\n            if workspace_id not in self._memories:\n                self._memories[workspace_id] = Objects()\n                self._manual_updates[workspace_id] = {}\n                self._workspace_visibility[workspace_id] = False\n                self._workspace_was_lost[workspace_id] = False\n\n                self._logger.debug(f\"Initialized memory for workspace: {workspace_id}\")\n\n    def update(\n        self, workspace_id: str, detected_objects: Objects, at_observation_pose: bool, robot_in_motion: bool\n    ) -&gt; Tuple[int, int]:\n        \"\"\"\n        Update memory with newly detected objects.\n\n        Only updates when conditions are appropriate (at observation pose, not moving).\n        Respects manual updates from pick/place operations.\n\n        Args:\n            workspace_id: ID of the workspace\n            detected_objects: Objects detected in current frame\n            at_observation_pose: Whether robot is at observation pose\n            robot_in_motion: Whether robot is currently moving\n\n        Returns:\n            Tuple of (objects_added, objects_updated)\n        \"\"\"\n        with self._lock:\n            # Ensure workspace is initialized\n            if workspace_id not in self._memories:\n                self.initialize_workspace(workspace_id)\n\n            # Check if we should clear memory first\n            if self._should_clear_memory(workspace_id, at_observation_pose, robot_in_motion):\n                self._clear_workspace_internal(workspace_id)\n\n            # Update visibility tracking\n            self._update_visibility_state(workspace_id, at_observation_pose, robot_in_motion)\n\n            # Only update when at observation pose\n            if not self._should_update_memory(at_observation_pose, robot_in_motion):\n                self._logger.debug(f\"Skipping memory update for {workspace_id} - conditions not met\")\n                return 0, 0\n\n            # Clean up expired manual updates\n            self._cleanup_expired_manual_updates(workspace_id)\n\n            # Update memory with new detections\n            objects_added, objects_updated = self._merge_detections(workspace_id, detected_objects)\n\n            if objects_added &gt; 0 or objects_updated &gt; 0:\n                self._logger.debug(\n                    f\"Memory update for '{workspace_id}': \"\n                    f\"added={objects_added}, updated={objects_updated}, \"\n                    f\"total={len(self._memories[workspace_id])}\"\n                )\n\n            return objects_added, objects_updated\n\n    def get(self, workspace_id: str) -&gt; Objects:\n        \"\"\"\n        Get a copy of objects from workspace memory.\n\n        Args:\n            workspace_id: ID of the workspace\n\n        Returns:\n            Copy of objects in memory for this workspace\n        \"\"\"\n        with self._lock:\n            if workspace_id not in self._memories:\n                self._logger.warning(f\"Workspace {workspace_id} not initialized\")\n                return Objects()\n\n            # Return a copy to avoid external modifications\n            return Objects(list(self._memories[workspace_id]))\n\n    def get_all(self) -&gt; Dict[str, Objects]:\n        \"\"\"\n        Get objects from all workspaces.\n\n        Returns:\n            Dictionary mapping workspace_id to Objects collection\n        \"\"\"\n        with self._lock:\n            return {ws_id: Objects(list(objects)) for ws_id, objects in self._memories.items()}\n\n    def clear(self, workspace_id: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Clear memory for specific workspace or all workspaces.\n\n        Args:\n            workspace_id: ID of workspace to clear, or None to clear all\n        \"\"\"\n        with self._lock:\n            if workspace_id is None:\n                # Clear all workspaces\n                for ws_id in self._memories:\n                    self._clear_workspace_internal(ws_id)\n                self._logger.info(\"Cleared memory for all workspaces\")\n            else:\n                if workspace_id in self._memories:\n                    self._clear_workspace_internal(workspace_id)\n                    self._logger.info(f\"Cleared memory for workspace: {workspace_id}\")\n                else:\n                    self._logger.warning(f\"Workspace {workspace_id} not found\")\n\n    def remove_object(self, workspace_id: str, object_label: str, coordinate: List[float]) -&gt; bool:\n        \"\"\"\n        Remove an object from workspace memory.\n\n        Args:\n            workspace_id: ID of the workspace\n            object_label: Label of the object to remove\n            coordinate: Last known coordinate [x, y]\n\n        Returns:\n            True if object was found and removed, False otherwise\n        \"\"\"\n        with self._lock:\n            if workspace_id not in self._memories:\n                self._logger.warning(f\"Workspace {workspace_id} not found\")\n                return False\n\n            workspace_objects = self._memories[workspace_id]\n\n            for i, obj in enumerate(workspace_objects):\n                if obj.label() == object_label and self._is_same_position(obj, coordinate):\n\n                    del workspace_objects[i]\n\n                    # Clear manual update tracking\n                    if workspace_id in self._manual_updates:\n                        self._manual_updates[workspace_id].pop(object_label, None)\n\n                    self._logger.info(f\"Removed {object_label} from {workspace_id} at {coordinate}\")\n                    return True\n\n            self._logger.warning(f\"Could not find {object_label} at {coordinate} in {workspace_id}\")\n            return False\n\n    def mark_manual_update(\n        self, workspace_id: str, object_label: str, old_coordinate: List[float], new_pose: PoseObjectPNP\n    ) -&gt; bool:\n        \"\"\"\n        Update an object's position after manual manipulation.\n\n        Args:\n            workspace_id: ID of the workspace\n            object_label: Label of the object\n            old_coordinate: Previous coordinate [x, y]\n            new_pose: New pose after movement\n\n        Returns:\n            True if object was found and updated, False otherwise\n        \"\"\"\n        with self._lock:\n            if workspace_id not in self._memories:\n                self._logger.warning(f\"Workspace {workspace_id} not found\")\n                return False\n\n            workspace_objects = self._memories[workspace_id]\n\n            for obj in workspace_objects:\n                if obj.label() == object_label and self._is_same_position(obj, old_coordinate):\n\n                    # Update position\n                    obj.set_pose_com(new_pose)\n\n                    # Track manual update\n                    if workspace_id not in self._manual_updates:\n                        self._manual_updates[workspace_id] = {}\n                    self._manual_updates[workspace_id][object_label] = time.time()\n\n                    self._logger.info(\n                        f\"Updated {object_label} in {workspace_id}: \"\n                        f\"{old_coordinate} -&gt; [{new_pose.x:.3f}, {new_pose.y:.3f}]\"\n                    )\n                    return True\n\n            self._logger.warning(f\"Could not find {object_label} at {old_coordinate} in {workspace_id}\")\n            return False\n\n    def move_object(\n        self,\n        source_workspace_id: str,\n        target_workspace_id: str,\n        object_label: str,\n        old_coordinate: List[float],\n        new_coordinate: List[float],\n    ) -&gt; bool:\n        \"\"\"\n        Move an object from one workspace to another in memory.\n\n        Args:\n            source_workspace_id: ID of source workspace\n            target_workspace_id: ID of target workspace\n            object_label: Label of the object\n            old_coordinate: Current coordinate in source workspace\n            new_coordinate: New coordinate in target workspace\n\n        Returns:\n            True if object was found and moved, False otherwise\n        \"\"\"\n        with self._lock:\n            # Validate workspaces\n            if source_workspace_id not in self._memories:\n                self._logger.warning(f\"Source workspace {source_workspace_id} not found\")\n                return False\n\n            if target_workspace_id not in self._memories:\n                self.initialize_workspace(target_workspace_id)\n\n            # Find and remove from source\n            source_objects = self._memories[source_workspace_id]\n            obj_to_move = None\n\n            for i, obj in enumerate(source_objects):\n                if obj.label() == object_label and self._is_same_position(obj, old_coordinate):\n\n                    obj_to_move = obj\n                    del source_objects[i]\n                    break\n\n            if obj_to_move is None:\n                self._logger.warning(f\"Could not find {object_label} at {old_coordinate} \" f\"in {source_workspace_id}\")\n                return False\n\n            # Update object's position\n            obj_to_move._x_com = new_coordinate[0]\n            obj_to_move._y_com = new_coordinate[1]\n\n            # Add to target workspace\n            self._memories[target_workspace_id].append(obj_to_move)\n\n            # Track manual update in target workspace\n            if target_workspace_id not in self._manual_updates:\n                self._manual_updates[target_workspace_id] = {}\n            self._manual_updates[target_workspace_id][object_label] = time.time()\n\n            self._logger.info(f\"Moved {object_label} from {source_workspace_id} \" f\"to {target_workspace_id}\")\n            return True\n\n    def get_memory_stats(self) -&gt; Dict[str, Dict]:\n        \"\"\"\n        Get statistics about memory contents.\n\n        Returns:\n            Dictionary with stats for each workspace\n        \"\"\"\n        with self._lock:\n            stats = {}\n            for ws_id, objects in self._memories.items():\n                manual_updates = self._manual_updates.get(ws_id, {})\n                stats[ws_id] = {\n                    \"object_count\": len(objects),\n                    \"manual_updates\": len(manual_updates),\n                    \"visible\": self._workspace_visibility.get(ws_id, False),\n                    \"was_lost\": self._workspace_was_lost.get(ws_id, False),\n                    \"objects\": [\n                        {\n                            \"label\": obj.label(),\n                            \"position\": [obj.x_com(), obj.y_com()],\n                            \"manually_updated\": obj.label() in manual_updates,\n                        }\n                        for obj in objects\n                    ],\n                }\n            return stats\n\n    # Private helper methods\n\n    def _should_update_memory(self, at_observation_pose: bool, robot_in_motion: bool) -&gt; bool:\n        \"\"\"Determine if memory should be updated.\"\"\"\n        return at_observation_pose and not robot_in_motion\n\n    def _should_clear_memory(self, workspace_id: str, at_observation_pose: bool, robot_in_motion: bool) -&gt; bool:\n        \"\"\"Determine if memory should be cleared.\"\"\"\n        was_lost = self._workspace_was_lost.get(workspace_id, False)\n        now_visible = at_observation_pose and not robot_in_motion\n\n        # Clear memory when workspace becomes visible again after being lost\n        should_clear = was_lost and now_visible\n\n        if should_clear:\n            self._workspace_was_lost[workspace_id] = False\n\n        return should_clear\n\n    def _update_visibility_state(self, workspace_id: str, at_observation_pose: bool, robot_in_motion: bool) -&gt; None:\n        \"\"\"Update workspace visibility tracking.\"\"\"\n        was_visible = self._workspace_visibility.get(workspace_id, False)\n        now_visible = at_observation_pose and not robot_in_motion\n\n        self._workspace_visibility[workspace_id] = now_visible\n\n        # Detect when workspace is lost\n        if was_visible and not now_visible:\n            self._workspace_was_lost[workspace_id] = True\n            self._logger.debug(f\"Workspace {workspace_id} lost - robot moved\")\n\n        # Clear lost flag when workspace becomes visible again\n        if now_visible and self._workspace_was_lost.get(workspace_id, False):\n            self._logger.debug(f\"Workspace {workspace_id} visible again - will clear memory on next update\")\n\n    def _cleanup_expired_manual_updates(self, workspace_id: str) -&gt; None:\n        \"\"\"Remove expired manual updates.\"\"\"\n        if workspace_id not in self._manual_updates:\n            return\n\n        current_time = time.time()\n        manual_updates = self._manual_updates[workspace_id]\n\n        expired_labels = [\n            label for label, timestamp in manual_updates.items() if current_time - timestamp &gt; self._manual_update_timeout\n        ]\n\n        for label in expired_labels:\n            del manual_updates[label]\n            self._logger.debug(f\"Manual update expired for {label} in {workspace_id}\")\n\n    def _merge_detections(self, workspace_id: str, detected_objects: Objects) -&gt; Tuple[int, int]:\n        \"\"\"\n        Merge new detections into memory.\n\n        Returns:\n            Tuple of (objects_added, objects_updated)\n        \"\"\"\n        workspace_memory = self._memories[workspace_id]\n        manual_updates = self._manual_updates.get(workspace_id, {})\n\n        objects_added = 0\n        objects_updated = 0\n\n        for obj in detected_objects:\n            x_center, y_center = obj.xy_com()\n            label = obj.label()\n\n            # Check if this object has a recent manual update\n            if label in manual_updates:\n                # Find the manually updated object\n                found_manual = False\n                for memory_obj in workspace_memory:\n                    if memory_obj.label() == label:\n                        manual_dist = ((memory_obj.x_com() - x_center) ** 2 + (memory_obj.y_com() - y_center) ** 2) ** 0.5\n\n                        if manual_dist &gt; self._position_tolerance:\n                            # Keep manual update, ignore detection\n                            self._logger.debug(f\"Keeping manual update for {label} \" f\"(distance: {manual_dist:.3f}m)\")\n                            found_manual = True\n                            break\n                        else:\n                            # Detection confirms manual update\n                            memory_obj._x_com = x_center\n                            memory_obj._y_com = y_center\n                            objects_updated += 1\n                            found_manual = True\n                            self._logger.debug(f\"Detection confirms manual update for {label}\")\n                            break\n\n                if found_manual:\n                    continue\n\n            # Check if object already exists in memory\n            is_duplicate = False\n            for memory_obj in workspace_memory:\n                if memory_obj.label() == label and self._is_same_position(memory_obj, [x_center, y_center]):\n                    is_duplicate = True\n                    break\n\n            if not is_duplicate:\n                workspace_memory.append(obj)\n                objects_added += 1\n\n        return objects_added, objects_updated\n\n    def _is_same_position(self, obj: Object, coordinate: List[float]) -&gt; bool:\n        \"\"\"Check if object is at the same position within tolerance.\"\"\"\n        return (\n            abs(obj.x_com() - coordinate[0]) &lt;= self._position_tolerance\n            and abs(obj.y_com() - coordinate[1]) &lt;= self._position_tolerance\n        )\n\n    def _clear_workspace_internal(self, workspace_id: str) -&gt; None:\n        \"\"\"Internal method to clear workspace memory (assumes lock is held).\"\"\"\n        if workspace_id in self._memories:\n            count = len(self._memories[workspace_id])\n            self._memories[workspace_id].clear()\n\n            if workspace_id in self._manual_updates:\n                self._manual_updates[workspace_id].clear()\n\n            self._workspace_was_lost[workspace_id] = False\n\n            self._logger.debug(f\"Cleared {count} objects from {workspace_id}\")\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager--initialize-workspaces","title":"Initialize workspaces","text":"<p>manager.initialize_workspace(\"niryo_ws\") manager.initialize_workspace(\"niryo_ws_left\")</p>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager--update-memory-with-detected-objects","title":"Update memory with detected objects","text":"<p>manager.update(\"niryo_ws\", detected_objects, at_observation=True)</p>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager--get-objects-from-memory","title":"Get objects from memory","text":"<p>objects = manager.get(\"niryo_ws\")</p>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager--manual-update-after-placing-object","title":"Manual update after placing object","text":"<p>manager.mark_manual_update(\"niryo_ws\", \"cube\",                            old_coord=[0.2, 0.0],                            new_pose=new_pose)</p>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager-functions","title":"Functions","text":""},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.__init__","title":"<code>__init__(manual_update_timeout=5.0, position_tolerance=0.05, verbose=False)</code>","text":"<p>Initialize the Object Memory Manager.</p> <p>Parameters:</p> Name Type Description Default <code>manual_update_timeout</code> <code>float</code> <p>Seconds to keep manual updates (default: 5.0)</p> <code>5.0</code> <code>position_tolerance</code> <code>float</code> <p>Distance threshold for duplicate detection in meters (default: 0.05)</p> <code>0.05</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging (default: False)</p> <code>False</code> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def __init__(self, manual_update_timeout: float = 5.0, position_tolerance: float = 0.05, verbose: bool = False):\n    \"\"\"\n    Initialize the Object Memory Manager.\n\n    Args:\n        manual_update_timeout: Seconds to keep manual updates (default: 5.0)\n        position_tolerance: Distance threshold for duplicate detection in meters (default: 0.05)\n        verbose: Enable verbose logging (default: False)\n    \"\"\"\n    # Workspace memories\n    self._memories: Dict[str, Objects] = {}\n\n    # Thread safety (using RLock to allow reentrant calls from update/move_object to initialize_workspace)\n    self._lock = threading.RLock()\n\n    # Manual updates tracking: {workspace_id: {object_label: timestamp}}\n    self._manual_updates: Dict[str, Dict[str, float]] = {}\n\n    # Configuration\n    self._manual_update_timeout = manual_update_timeout\n    self._position_tolerance = position_tolerance\n\n    # State tracking\n    self._workspace_visibility: Dict[str, bool] = {}\n    self._workspace_was_lost: Dict[str, bool] = {}\n\n    # Logging\n    self._verbose = verbose\n    self._logger = logging.getLogger(__name__)\n    if verbose:\n        self._logger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.clear","title":"<code>clear(workspace_id=None)</code>","text":"<p>Clear memory for specific workspace or all workspaces.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>Optional[str]</code> <p>ID of workspace to clear, or None to clear all</p> <code>None</code> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def clear(self, workspace_id: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Clear memory for specific workspace or all workspaces.\n\n    Args:\n        workspace_id: ID of workspace to clear, or None to clear all\n    \"\"\"\n    with self._lock:\n        if workspace_id is None:\n            # Clear all workspaces\n            for ws_id in self._memories:\n                self._clear_workspace_internal(ws_id)\n            self._logger.info(\"Cleared memory for all workspaces\")\n        else:\n            if workspace_id in self._memories:\n                self._clear_workspace_internal(workspace_id)\n                self._logger.info(f\"Cleared memory for workspace: {workspace_id}\")\n            else:\n                self._logger.warning(f\"Workspace {workspace_id} not found\")\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.get","title":"<code>get(workspace_id)</code>","text":"<p>Get a copy of objects from workspace memory.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <p>Returns:</p> Type Description <code>Objects</code> <p>Copy of objects in memory for this workspace</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def get(self, workspace_id: str) -&gt; Objects:\n    \"\"\"\n    Get a copy of objects from workspace memory.\n\n    Args:\n        workspace_id: ID of the workspace\n\n    Returns:\n        Copy of objects in memory for this workspace\n    \"\"\"\n    with self._lock:\n        if workspace_id not in self._memories:\n            self._logger.warning(f\"Workspace {workspace_id} not initialized\")\n            return Objects()\n\n        # Return a copy to avoid external modifications\n        return Objects(list(self._memories[workspace_id]))\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.get_all","title":"<code>get_all()</code>","text":"<p>Get objects from all workspaces.</p> <p>Returns:</p> Type Description <code>Dict[str, Objects]</code> <p>Dictionary mapping workspace_id to Objects collection</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def get_all(self) -&gt; Dict[str, Objects]:\n    \"\"\"\n    Get objects from all workspaces.\n\n    Returns:\n        Dictionary mapping workspace_id to Objects collection\n    \"\"\"\n    with self._lock:\n        return {ws_id: Objects(list(objects)) for ws_id, objects in self._memories.items()}\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.get_memory_stats","title":"<code>get_memory_stats()</code>","text":"<p>Get statistics about memory contents.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict]</code> <p>Dictionary with stats for each workspace</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def get_memory_stats(self) -&gt; Dict[str, Dict]:\n    \"\"\"\n    Get statistics about memory contents.\n\n    Returns:\n        Dictionary with stats for each workspace\n    \"\"\"\n    with self._lock:\n        stats = {}\n        for ws_id, objects in self._memories.items():\n            manual_updates = self._manual_updates.get(ws_id, {})\n            stats[ws_id] = {\n                \"object_count\": len(objects),\n                \"manual_updates\": len(manual_updates),\n                \"visible\": self._workspace_visibility.get(ws_id, False),\n                \"was_lost\": self._workspace_was_lost.get(ws_id, False),\n                \"objects\": [\n                    {\n                        \"label\": obj.label(),\n                        \"position\": [obj.x_com(), obj.y_com()],\n                        \"manually_updated\": obj.label() in manual_updates,\n                    }\n                    for obj in objects\n                ],\n            }\n        return stats\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.initialize_workspace","title":"<code>initialize_workspace(workspace_id)</code>","text":"<p>Initialize memory for a new workspace.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace to initialize</p> required Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def initialize_workspace(self, workspace_id: str) -&gt; None:\n    \"\"\"\n    Initialize memory for a new workspace.\n\n    Args:\n        workspace_id: ID of the workspace to initialize\n    \"\"\"\n    with self._lock:\n        if workspace_id not in self._memories:\n            self._memories[workspace_id] = Objects()\n            self._manual_updates[workspace_id] = {}\n            self._workspace_visibility[workspace_id] = False\n            self._workspace_was_lost[workspace_id] = False\n\n            self._logger.debug(f\"Initialized memory for workspace: {workspace_id}\")\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.mark_manual_update","title":"<code>mark_manual_update(workspace_id, object_label, old_coordinate, new_pose)</code>","text":"<p>Update an object's position after manual manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <code>object_label</code> <code>str</code> <p>Label of the object</p> required <code>old_coordinate</code> <code>List[float]</code> <p>Previous coordinate [x, y]</p> required <code>new_pose</code> <code>PoseObjectPNP</code> <p>New pose after movement</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if object was found and updated, False otherwise</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def mark_manual_update(\n    self, workspace_id: str, object_label: str, old_coordinate: List[float], new_pose: PoseObjectPNP\n) -&gt; bool:\n    \"\"\"\n    Update an object's position after manual manipulation.\n\n    Args:\n        workspace_id: ID of the workspace\n        object_label: Label of the object\n        old_coordinate: Previous coordinate [x, y]\n        new_pose: New pose after movement\n\n    Returns:\n        True if object was found and updated, False otherwise\n    \"\"\"\n    with self._lock:\n        if workspace_id not in self._memories:\n            self._logger.warning(f\"Workspace {workspace_id} not found\")\n            return False\n\n        workspace_objects = self._memories[workspace_id]\n\n        for obj in workspace_objects:\n            if obj.label() == object_label and self._is_same_position(obj, old_coordinate):\n\n                # Update position\n                obj.set_pose_com(new_pose)\n\n                # Track manual update\n                if workspace_id not in self._manual_updates:\n                    self._manual_updates[workspace_id] = {}\n                self._manual_updates[workspace_id][object_label] = time.time()\n\n                self._logger.info(\n                    f\"Updated {object_label} in {workspace_id}: \"\n                    f\"{old_coordinate} -&gt; [{new_pose.x:.3f}, {new_pose.y:.3f}]\"\n                )\n                return True\n\n        self._logger.warning(f\"Could not find {object_label} at {old_coordinate} in {workspace_id}\")\n        return False\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.move_object","title":"<code>move_object(source_workspace_id, target_workspace_id, object_label, old_coordinate, new_coordinate)</code>","text":"<p>Move an object from one workspace to another in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_workspace_id</code> <code>str</code> <p>ID of source workspace</p> required <code>target_workspace_id</code> <code>str</code> <p>ID of target workspace</p> required <code>object_label</code> <code>str</code> <p>Label of the object</p> required <code>old_coordinate</code> <code>List[float]</code> <p>Current coordinate in source workspace</p> required <code>new_coordinate</code> <code>List[float]</code> <p>New coordinate in target workspace</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if object was found and moved, False otherwise</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def move_object(\n    self,\n    source_workspace_id: str,\n    target_workspace_id: str,\n    object_label: str,\n    old_coordinate: List[float],\n    new_coordinate: List[float],\n) -&gt; bool:\n    \"\"\"\n    Move an object from one workspace to another in memory.\n\n    Args:\n        source_workspace_id: ID of source workspace\n        target_workspace_id: ID of target workspace\n        object_label: Label of the object\n        old_coordinate: Current coordinate in source workspace\n        new_coordinate: New coordinate in target workspace\n\n    Returns:\n        True if object was found and moved, False otherwise\n    \"\"\"\n    with self._lock:\n        # Validate workspaces\n        if source_workspace_id not in self._memories:\n            self._logger.warning(f\"Source workspace {source_workspace_id} not found\")\n            return False\n\n        if target_workspace_id not in self._memories:\n            self.initialize_workspace(target_workspace_id)\n\n        # Find and remove from source\n        source_objects = self._memories[source_workspace_id]\n        obj_to_move = None\n\n        for i, obj in enumerate(source_objects):\n            if obj.label() == object_label and self._is_same_position(obj, old_coordinate):\n\n                obj_to_move = obj\n                del source_objects[i]\n                break\n\n        if obj_to_move is None:\n            self._logger.warning(f\"Could not find {object_label} at {old_coordinate} \" f\"in {source_workspace_id}\")\n            return False\n\n        # Update object's position\n        obj_to_move._x_com = new_coordinate[0]\n        obj_to_move._y_com = new_coordinate[1]\n\n        # Add to target workspace\n        self._memories[target_workspace_id].append(obj_to_move)\n\n        # Track manual update in target workspace\n        if target_workspace_id not in self._manual_updates:\n            self._manual_updates[target_workspace_id] = {}\n        self._manual_updates[target_workspace_id][object_label] = time.time()\n\n        self._logger.info(f\"Moved {object_label} from {source_workspace_id} \" f\"to {target_workspace_id}\")\n        return True\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.remove_object","title":"<code>remove_object(workspace_id, object_label, coordinate)</code>","text":"<p>Remove an object from workspace memory.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <code>object_label</code> <code>str</code> <p>Label of the object to remove</p> required <code>coordinate</code> <code>List[float]</code> <p>Last known coordinate [x, y]</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if object was found and removed, False otherwise</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def remove_object(self, workspace_id: str, object_label: str, coordinate: List[float]) -&gt; bool:\n    \"\"\"\n    Remove an object from workspace memory.\n\n    Args:\n        workspace_id: ID of the workspace\n        object_label: Label of the object to remove\n        coordinate: Last known coordinate [x, y]\n\n    Returns:\n        True if object was found and removed, False otherwise\n    \"\"\"\n    with self._lock:\n        if workspace_id not in self._memories:\n            self._logger.warning(f\"Workspace {workspace_id} not found\")\n            return False\n\n        workspace_objects = self._memories[workspace_id]\n\n        for i, obj in enumerate(workspace_objects):\n            if obj.label() == object_label and self._is_same_position(obj, coordinate):\n\n                del workspace_objects[i]\n\n                # Clear manual update tracking\n                if workspace_id in self._manual_updates:\n                    self._manual_updates[workspace_id].pop(object_label, None)\n\n                self._logger.info(f\"Removed {object_label} from {workspace_id} at {coordinate}\")\n                return True\n\n        self._logger.warning(f\"Could not find {object_label} at {coordinate} in {workspace_id}\")\n        return False\n</code></pre>"},{"location":"api/utils/#robot_environment.object_memory_manager.ObjectMemoryManager.update","title":"<code>update(workspace_id, detected_objects, at_observation_pose, robot_in_motion)</code>","text":"<p>Update memory with newly detected objects.</p> <p>Only updates when conditions are appropriate (at observation pose, not moving). Respects manual updates from pick/place operations.</p> <p>Parameters:</p> Name Type Description Default <code>workspace_id</code> <code>str</code> <p>ID of the workspace</p> required <code>detected_objects</code> <code>Objects</code> <p>Objects detected in current frame</p> required <code>at_observation_pose</code> <code>bool</code> <p>Whether robot is at observation pose</p> required <code>robot_in_motion</code> <code>bool</code> <p>Whether robot is currently moving</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple of (objects_added, objects_updated)</p> Source code in <code>robot_environment/object_memory_manager.py</code> <pre><code>def update(\n    self, workspace_id: str, detected_objects: Objects, at_observation_pose: bool, robot_in_motion: bool\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    Update memory with newly detected objects.\n\n    Only updates when conditions are appropriate (at observation pose, not moving).\n    Respects manual updates from pick/place operations.\n\n    Args:\n        workspace_id: ID of the workspace\n        detected_objects: Objects detected in current frame\n        at_observation_pose: Whether robot is at observation pose\n        robot_in_motion: Whether robot is currently moving\n\n    Returns:\n        Tuple of (objects_added, objects_updated)\n    \"\"\"\n    with self._lock:\n        # Ensure workspace is initialized\n        if workspace_id not in self._memories:\n            self.initialize_workspace(workspace_id)\n\n        # Check if we should clear memory first\n        if self._should_clear_memory(workspace_id, at_observation_pose, robot_in_motion):\n            self._clear_workspace_internal(workspace_id)\n\n        # Update visibility tracking\n        self._update_visibility_state(workspace_id, at_observation_pose, robot_in_motion)\n\n        # Only update when at observation pose\n        if not self._should_update_memory(at_observation_pose, robot_in_motion):\n            self._logger.debug(f\"Skipping memory update for {workspace_id} - conditions not met\")\n            return 0, 0\n\n        # Clean up expired manual updates\n        self._cleanup_expired_manual_updates(workspace_id)\n\n        # Update memory with new detections\n        objects_added, objects_updated = self._merge_detections(workspace_id, detected_objects)\n\n        if objects_added &gt; 0 or objects_updated &gt; 0:\n            self._logger.debug(\n                f\"Memory update for '{workspace_id}': \"\n                f\"added={objects_added}, updated={objects_updated}, \"\n                f\"total={len(self._memories[workspace_id])}\"\n            )\n\n        return objects_added, objects_updated\n</code></pre>"},{"location":"api/utils/#performance-metrics","title":"Performance Metrics","text":""},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics","title":"<code>robot_environment.performance_metrics.PerformanceMetrics</code>","text":"<p>Centralized performance monitoring for robot_environment.</p> <p>Tracks timing and throughput metrics for all major operations: - Camera frame capture - Object detection - Robot movements (pick, place, push) - Memory updates - Redis communication</p> Example <p>metrics = PerformanceMetrics(history_size=100)</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>class PerformanceMetrics:\n    \"\"\"\n    Centralized performance monitoring for robot_environment.\n\n    Tracks timing and throughput metrics for all major operations:\n    - Camera frame capture\n    - Object detection\n    - Robot movements (pick, place, push)\n    - Memory updates\n    - Redis communication\n\n    Example:\n        metrics = PerformanceMetrics(history_size=100)\n\n        # Using context manager\n        with metrics.timer('frame_capture'):\n            frame = capture_frame()\n\n        # Manual recording\n        metrics.record_timing('detection', 45.2)\n\n        # Get statistics\n        stats = metrics.get_stats()\n        print(f\"Average FPS: {stats['camera']['fps']:.1f}\")\n    \"\"\"\n\n    def __init__(self, history_size: int = 100, verbose: bool = False):\n        \"\"\"\n        Initialize performance metrics tracker.\n\n        Args:\n            history_size: Number of samples to keep for each metric\n            verbose: Enable verbose logging of metrics\n        \"\"\"\n        self._history_size = history_size\n        self._verbose = verbose\n        self._logger = logging.getLogger(__name__)\n\n        # Thread safety\n        self._lock = threading.Lock()\n\n        # Timing metrics (stored as deques for efficient rolling windows)\n        self._timings: Dict[str, deque] = {\n            # Camera operations\n            \"frame_capture\": deque(maxlen=history_size),\n            \"frame_publish\": deque(maxlen=history_size),\n            # Vision operations\n            \"object_detection\": deque(maxlen=history_size),\n            \"object_fetch_redis\": deque(maxlen=history_size),\n            # Memory operations\n            \"memory_update\": deque(maxlen=history_size),\n            \"memory_get\": deque(maxlen=history_size),\n            \"memory_clear\": deque(maxlen=history_size),\n            # Robot operations\n            \"robot_pick\": deque(maxlen=history_size),\n            \"robot_place\": deque(maxlen=history_size),\n            \"robot_push\": deque(maxlen=history_size),\n            \"robot_move_observation\": deque(maxlen=history_size),\n            \"robot_get_pose\": deque(maxlen=history_size),\n            # High-level operations\n            \"pick_place_total\": deque(maxlen=history_size),\n            \"camera_loop_iteration\": deque(maxlen=history_size),\n            # Communication\n            \"redis_publish\": deque(maxlen=history_size),\n            \"redis_fetch\": deque(maxlen=history_size),\n        }\n\n        # Counter metrics\n        self._counters: Dict[str, int] = {\n            \"frames_captured\": 0,\n            \"objects_detected\": 0,\n            \"pick_operations\": 0,\n            \"place_operations\": 0,\n            \"push_operations\": 0,\n            \"pick_successes\": 0,\n            \"place_successes\": 0,\n            \"pick_failures\": 0,\n            \"place_failures\": 0,\n            \"memory_updates\": 0,\n            \"memory_clears\": 0,\n        }\n\n        # Start time for uptime tracking\n        self._start_time = datetime.now()\n\n        # Last values for rate calculations\n        self._last_frame_time = None\n        self._last_stats_time = perf_counter()\n\n        if verbose:\n            self._logger.setLevel(logging.DEBUG)\n\n    def timer(self, metric_name: str) -&gt; PerformanceTimer:\n        \"\"\"\n        Create a context manager timer for an operation.\n\n        Args:\n            metric_name: Name of the metric to record\n\n        Returns:\n            PerformanceTimer context manager\n\n        Example:\n            with metrics.timer('frame_capture'):\n                frame = camera.get_frame()\n        \"\"\"\n        return PerformanceTimer(self, metric_name)\n\n    def record_timing(self, metric_name: str, duration_ms: float) -&gt; None:\n        \"\"\"\n        Record a timing measurement.\n\n        Args:\n            metric_name: Name of the metric\n            duration_ms: Duration in milliseconds\n        \"\"\"\n        with self._lock:\n            if metric_name in self._timings:\n                self._timings[metric_name].append(duration_ms)\n\n                if self._verbose and len(self._timings[metric_name]) % 10 == 0:\n                    stats = TimingStats.from_samples(list(self._timings[metric_name]))\n                    self._logger.debug(\n                        f\"{metric_name}: {duration_ms:.1f}ms \" f\"(avg: {stats.mean:.1f}ms, p95: {stats.p95:.1f}ms)\"\n                    )\n            else:\n                # FIX: Create the metric on-the-fly if it doesn't exist\n                from collections import deque\n\n                self._timings[metric_name] = deque([duration_ms], maxlen=self._history_size)\n                self._logger.debug(f\"Created new timing metric: {metric_name}\")\n\n    def increment_counter(self, counter_name: str, amount: int = 1) -&gt; None:\n        \"\"\"\n        Increment a counter metric.\n\n        Args:\n            counter_name: Name of the counter\n            amount: Amount to increment by (default: 1)\n        \"\"\"\n        with self._lock:\n            if counter_name in self._counters:\n                self._counters[counter_name] += amount\n            else:\n                self._logger.warning(f\"Unknown counter: {counter_name}\")\n\n    def record_frame_captured(self, duration_ms: float) -&gt; None:\n        \"\"\"Record a frame capture event.\"\"\"\n        self.record_timing(\"frame_capture\", duration_ms)\n        self.increment_counter(\"frames_captured\")\n        self._last_frame_time = perf_counter()\n\n    def record_objects_detected(self, count: int, detection_time_ms: float) -&gt; None:\n        \"\"\"Record object detection results.\"\"\"\n        self.record_timing(\"object_detection\", detection_time_ms)\n        self.increment_counter(\"objects_detected\", count)\n\n    def record_pick_operation(self, duration_s: float, success: bool) -&gt; None:\n        \"\"\"Record a pick operation.\"\"\"\n        self.record_timing(\"robot_pick\", duration_s * 1000)\n        self.increment_counter(\"pick_operations\")\n        if success:\n            self.increment_counter(\"pick_successes\")\n        else:\n            self.increment_counter(\"pick_failures\")\n\n    def record_place_operation(self, duration_s: float, success: bool) -&gt; None:\n        \"\"\"Record a place operation.\"\"\"\n        self.record_timing(\"robot_place\", duration_s * 1000)\n        self.increment_counter(\"place_operations\")\n        if success:\n            self.increment_counter(\"place_successes\")\n        else:\n            self.increment_counter(\"place_failures\")\n\n    def record_memory_update(self, duration_ms: float, objects_added: int, objects_updated: int) -&gt; None:\n        \"\"\"Record a memory update operation.\"\"\"\n        self.record_timing(\"memory_update\", duration_ms)\n        self.increment_counter(\"memory_updates\")\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get comprehensive performance statistics.\n\n        Returns:\n            Dictionary containing all performance metrics and statistics\n        \"\"\"\n        with self._lock:\n            # current_time = perf_counter()\n            uptime_seconds = (datetime.now() - self._start_time).total_seconds()\n\n            stats = {\n                \"uptime_seconds\": uptime_seconds,\n                \"timestamp\": datetime.now().isoformat(),\n                # Camera metrics\n                \"camera\": {\n                    \"frames_captured\": self._counters[\"frames_captured\"],\n                    \"fps\": self._calculate_fps(),\n                    \"frame_capture\": self._get_timing_stats(\"frame_capture\"),\n                    \"frame_publish\": self._get_timing_stats(\"frame_publish\"),\n                    \"loop_iteration\": self._get_timing_stats(\"camera_loop_iteration\"),\n                },\n                # Vision metrics\n                \"vision\": {\n                    \"objects_detected\": self._counters[\"objects_detected\"],\n                    \"detection_time\": self._get_timing_stats(\"object_detection\"),\n                    \"redis_fetch_time\": self._get_timing_stats(\"object_fetch_redis\"),\n                },\n                # Memory metrics\n                \"memory\": {\n                    \"updates\": self._counters[\"memory_updates\"],\n                    \"clears\": self._counters[\"memory_clears\"],\n                    \"update_time\": self._get_timing_stats(\"memory_update\"),\n                    \"get_time\": self._get_timing_stats(\"memory_get\"),\n                },\n                # Robot operation metrics\n                \"robot\": {\n                    \"operations\": {\n                        \"pick\": {\n                            \"count\": self._counters[\"pick_operations\"],\n                            \"successes\": self._counters[\"pick_successes\"],\n                            \"failures\": self._counters[\"pick_failures\"],\n                            \"success_rate\": self._calculate_success_rate(\"pick\"),\n                            \"duration\": self._get_timing_stats(\"robot_pick\"),\n                        },\n                        \"place\": {\n                            \"count\": self._counters[\"place_operations\"],\n                            \"successes\": self._counters[\"place_successes\"],\n                            \"failures\": self._counters[\"place_failures\"],\n                            \"success_rate\": self._calculate_success_rate(\"place\"),\n                            \"duration\": self._get_timing_stats(\"robot_place\"),\n                        },\n                        \"push\": {\n                            \"count\": self._counters[\"push_operations\"],\n                            \"duration\": self._get_timing_stats(\"robot_push\"),\n                        },\n                    },\n                    \"movement\": {\n                        \"observation_pose\": self._get_timing_stats(\"robot_move_observation\"),\n                        \"get_pose\": self._get_timing_stats(\"robot_get_pose\"),\n                    },\n                    \"pick_place_total\": self._get_timing_stats(\"pick_place_total\"),\n                },\n                # Communication metrics\n                \"communication\": {\n                    \"redis_publish\": self._get_timing_stats(\"redis_publish\"),\n                    \"redis_fetch\": self._get_timing_stats(\"redis_fetch\"),\n                },\n            }\n\n            return stats\n\n    def get_summary(self) -&gt; str:\n        \"\"\"\n        Get a human-readable summary of performance metrics.\n\n        Returns:\n            Formatted string with key performance indicators\n        \"\"\"\n        stats = self.get_stats()\n\n        lines = [\n            \"=\" * 70,\n            \"PERFORMANCE METRICS SUMMARY\",\n            \"=\" * 70,\n            f\"Uptime: {stats['uptime_seconds']:.1f}s\",\n            \"\",\n            \"CAMERA:\",\n            f\"  Frames captured: {stats['camera']['frames_captured']}\",\n            f\"  Current FPS: {stats['camera']['fps']:.1f}\",\n            f\"  Frame capture time: {stats['camera']['frame_capture']['mean']:.1f}ms \"\n            f\"(p95: {stats['camera']['frame_capture']['p95']:.1f}ms)\",\n            \"\",\n            \"VISION:\",\n            f\"  Objects detected: {stats['vision']['objects_detected']}\",\n            f\"  Detection time: {stats['vision']['detection_time']['mean']:.1f}ms \"\n            f\"(p95: {stats['vision']['detection_time']['p95']:.1f}ms)\",\n            \"\",\n            \"ROBOT OPERATIONS:\",\n            f\"  Pick operations: {stats['robot']['operations']['pick']['count']} \"\n            f\"(success rate: {stats['robot']['operations']['pick']['success_rate']:.1f}%)\",\n            f\"  Pick duration: {stats['robot']['operations']['pick']['duration']['mean']:.0f}ms\",\n            f\"  Place operations: {stats['robot']['operations']['place']['count']} \"\n            f\"(success rate: {stats['robot']['operations']['place']['success_rate']:.1f}%)\",\n            f\"  Place duration: {stats['robot']['operations']['place']['duration']['mean']:.0f}ms\",\n            \"\",\n            \"MEMORY:\",\n            f\"  Updates: {stats['memory']['updates']}\",\n            f\"  Update time: {stats['memory']['update_time']['mean']:.1f}ms\",\n            \"=\" * 70,\n        ]\n\n        return \"\\n\".join(lines)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset all metrics.\"\"\"\n        with self._lock:\n            for timing_deque in self._timings.values():\n                timing_deque.clear()\n\n            for counter_name in self._counters:\n                self._counters[counter_name] = 0\n\n            self._start_time = datetime.now()\n            self._last_frame_time = None\n\n            self._logger.info(\"Performance metrics reset\")\n\n    def export_json(self, filepath: str) -&gt; None:\n        \"\"\"\n        Export metrics to JSON file.\n\n        Args:\n            filepath: Path to output file\n        \"\"\"\n        stats = self.get_stats()\n        with open(filepath, \"w\") as f:\n            json.dump(stats, f, indent=2)\n\n        self._logger.info(f\"Metrics exported to {filepath}\")\n\n    # Private helper methods\n\n    def _get_timing_stats(self, metric_name: str) -&gt; Dict[str, float]:\n        \"\"\"Get timing statistics for a metric.\"\"\"\n        if metric_name not in self._timings:\n            return TimingStats().to_dict()\n\n        samples = list(self._timings[metric_name])\n        return TimingStats.from_samples(samples).to_dict()\n\n    def _calculate_fps(self) -&gt; float:\n        \"\"\"Calculate current frames per second.\"\"\"\n        frame_times = list(self._timings[\"frame_capture\"])\n        if not frame_times:\n            return 0.0\n\n        # Use recent samples for more accurate instantaneous FPS\n        recent_samples = frame_times[-10:] if len(frame_times) &gt;= 10 else frame_times\n        avg_time_ms = np.mean(recent_samples)\n\n        if avg_time_ms &gt; 0:\n            return 1000.0 / avg_time_ms\n        return 0.0\n\n    def _calculate_success_rate(self, operation: str) -&gt; float:\n        \"\"\"Calculate success rate for an operation.\"\"\"\n        total = self._counters.get(f\"{operation}_operations\", 0)\n        if total == 0:\n            return 0.0\n\n        successes = self._counters.get(f\"{operation}_successes\", 0)\n        return (successes / total) * 100.0\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics--using-context-manager","title":"Using context manager","text":"<p>with metrics.timer('frame_capture'):     frame = capture_frame()</p>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics--manual-recording","title":"Manual recording","text":"<p>metrics.record_timing('detection', 45.2)</p>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics--get-statistics","title":"Get statistics","text":"<p>stats = metrics.get_stats() print(f\"Average FPS: {stats['camera']['fps']:.1f}\")</p>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics-functions","title":"Functions","text":""},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.__init__","title":"<code>__init__(history_size=100, verbose=False)</code>","text":"<p>Initialize performance metrics tracker.</p> <p>Parameters:</p> Name Type Description Default <code>history_size</code> <code>int</code> <p>Number of samples to keep for each metric</p> <code>100</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging of metrics</p> <code>False</code> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def __init__(self, history_size: int = 100, verbose: bool = False):\n    \"\"\"\n    Initialize performance metrics tracker.\n\n    Args:\n        history_size: Number of samples to keep for each metric\n        verbose: Enable verbose logging of metrics\n    \"\"\"\n    self._history_size = history_size\n    self._verbose = verbose\n    self._logger = logging.getLogger(__name__)\n\n    # Thread safety\n    self._lock = threading.Lock()\n\n    # Timing metrics (stored as deques for efficient rolling windows)\n    self._timings: Dict[str, deque] = {\n        # Camera operations\n        \"frame_capture\": deque(maxlen=history_size),\n        \"frame_publish\": deque(maxlen=history_size),\n        # Vision operations\n        \"object_detection\": deque(maxlen=history_size),\n        \"object_fetch_redis\": deque(maxlen=history_size),\n        # Memory operations\n        \"memory_update\": deque(maxlen=history_size),\n        \"memory_get\": deque(maxlen=history_size),\n        \"memory_clear\": deque(maxlen=history_size),\n        # Robot operations\n        \"robot_pick\": deque(maxlen=history_size),\n        \"robot_place\": deque(maxlen=history_size),\n        \"robot_push\": deque(maxlen=history_size),\n        \"robot_move_observation\": deque(maxlen=history_size),\n        \"robot_get_pose\": deque(maxlen=history_size),\n        # High-level operations\n        \"pick_place_total\": deque(maxlen=history_size),\n        \"camera_loop_iteration\": deque(maxlen=history_size),\n        # Communication\n        \"redis_publish\": deque(maxlen=history_size),\n        \"redis_fetch\": deque(maxlen=history_size),\n    }\n\n    # Counter metrics\n    self._counters: Dict[str, int] = {\n        \"frames_captured\": 0,\n        \"objects_detected\": 0,\n        \"pick_operations\": 0,\n        \"place_operations\": 0,\n        \"push_operations\": 0,\n        \"pick_successes\": 0,\n        \"place_successes\": 0,\n        \"pick_failures\": 0,\n        \"place_failures\": 0,\n        \"memory_updates\": 0,\n        \"memory_clears\": 0,\n    }\n\n    # Start time for uptime tracking\n    self._start_time = datetime.now()\n\n    # Last values for rate calculations\n    self._last_frame_time = None\n    self._last_stats_time = perf_counter()\n\n    if verbose:\n        self._logger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.export_json","title":"<code>export_json(filepath)</code>","text":"<p>Export metrics to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to output file</p> required Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def export_json(self, filepath: str) -&gt; None:\n    \"\"\"\n    Export metrics to JSON file.\n\n    Args:\n        filepath: Path to output file\n    \"\"\"\n    stats = self.get_stats()\n    with open(filepath, \"w\") as f:\n        json.dump(stats, f, indent=2)\n\n    self._logger.info(f\"Metrics exported to {filepath}\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.get_stats","title":"<code>get_stats()</code>","text":"<p>Get comprehensive performance statistics.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing all performance metrics and statistics</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get comprehensive performance statistics.\n\n    Returns:\n        Dictionary containing all performance metrics and statistics\n    \"\"\"\n    with self._lock:\n        # current_time = perf_counter()\n        uptime_seconds = (datetime.now() - self._start_time).total_seconds()\n\n        stats = {\n            \"uptime_seconds\": uptime_seconds,\n            \"timestamp\": datetime.now().isoformat(),\n            # Camera metrics\n            \"camera\": {\n                \"frames_captured\": self._counters[\"frames_captured\"],\n                \"fps\": self._calculate_fps(),\n                \"frame_capture\": self._get_timing_stats(\"frame_capture\"),\n                \"frame_publish\": self._get_timing_stats(\"frame_publish\"),\n                \"loop_iteration\": self._get_timing_stats(\"camera_loop_iteration\"),\n            },\n            # Vision metrics\n            \"vision\": {\n                \"objects_detected\": self._counters[\"objects_detected\"],\n                \"detection_time\": self._get_timing_stats(\"object_detection\"),\n                \"redis_fetch_time\": self._get_timing_stats(\"object_fetch_redis\"),\n            },\n            # Memory metrics\n            \"memory\": {\n                \"updates\": self._counters[\"memory_updates\"],\n                \"clears\": self._counters[\"memory_clears\"],\n                \"update_time\": self._get_timing_stats(\"memory_update\"),\n                \"get_time\": self._get_timing_stats(\"memory_get\"),\n            },\n            # Robot operation metrics\n            \"robot\": {\n                \"operations\": {\n                    \"pick\": {\n                        \"count\": self._counters[\"pick_operations\"],\n                        \"successes\": self._counters[\"pick_successes\"],\n                        \"failures\": self._counters[\"pick_failures\"],\n                        \"success_rate\": self._calculate_success_rate(\"pick\"),\n                        \"duration\": self._get_timing_stats(\"robot_pick\"),\n                    },\n                    \"place\": {\n                        \"count\": self._counters[\"place_operations\"],\n                        \"successes\": self._counters[\"place_successes\"],\n                        \"failures\": self._counters[\"place_failures\"],\n                        \"success_rate\": self._calculate_success_rate(\"place\"),\n                        \"duration\": self._get_timing_stats(\"robot_place\"),\n                    },\n                    \"push\": {\n                        \"count\": self._counters[\"push_operations\"],\n                        \"duration\": self._get_timing_stats(\"robot_push\"),\n                    },\n                },\n                \"movement\": {\n                    \"observation_pose\": self._get_timing_stats(\"robot_move_observation\"),\n                    \"get_pose\": self._get_timing_stats(\"robot_get_pose\"),\n                },\n                \"pick_place_total\": self._get_timing_stats(\"pick_place_total\"),\n            },\n            # Communication metrics\n            \"communication\": {\n                \"redis_publish\": self._get_timing_stats(\"redis_publish\"),\n                \"redis_fetch\": self._get_timing_stats(\"redis_fetch\"),\n            },\n        }\n\n        return stats\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.get_summary","title":"<code>get_summary()</code>","text":"<p>Get a human-readable summary of performance metrics.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with key performance indicators</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"\n    Get a human-readable summary of performance metrics.\n\n    Returns:\n        Formatted string with key performance indicators\n    \"\"\"\n    stats = self.get_stats()\n\n    lines = [\n        \"=\" * 70,\n        \"PERFORMANCE METRICS SUMMARY\",\n        \"=\" * 70,\n        f\"Uptime: {stats['uptime_seconds']:.1f}s\",\n        \"\",\n        \"CAMERA:\",\n        f\"  Frames captured: {stats['camera']['frames_captured']}\",\n        f\"  Current FPS: {stats['camera']['fps']:.1f}\",\n        f\"  Frame capture time: {stats['camera']['frame_capture']['mean']:.1f}ms \"\n        f\"(p95: {stats['camera']['frame_capture']['p95']:.1f}ms)\",\n        \"\",\n        \"VISION:\",\n        f\"  Objects detected: {stats['vision']['objects_detected']}\",\n        f\"  Detection time: {stats['vision']['detection_time']['mean']:.1f}ms \"\n        f\"(p95: {stats['vision']['detection_time']['p95']:.1f}ms)\",\n        \"\",\n        \"ROBOT OPERATIONS:\",\n        f\"  Pick operations: {stats['robot']['operations']['pick']['count']} \"\n        f\"(success rate: {stats['robot']['operations']['pick']['success_rate']:.1f}%)\",\n        f\"  Pick duration: {stats['robot']['operations']['pick']['duration']['mean']:.0f}ms\",\n        f\"  Place operations: {stats['robot']['operations']['place']['count']} \"\n        f\"(success rate: {stats['robot']['operations']['place']['success_rate']:.1f}%)\",\n        f\"  Place duration: {stats['robot']['operations']['place']['duration']['mean']:.0f}ms\",\n        \"\",\n        \"MEMORY:\",\n        f\"  Updates: {stats['memory']['updates']}\",\n        f\"  Update time: {stats['memory']['update_time']['mean']:.1f}ms\",\n        \"=\" * 70,\n    ]\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.increment_counter","title":"<code>increment_counter(counter_name, amount=1)</code>","text":"<p>Increment a counter metric.</p> <p>Parameters:</p> Name Type Description Default <code>counter_name</code> <code>str</code> <p>Name of the counter</p> required <code>amount</code> <code>int</code> <p>Amount to increment by (default: 1)</p> <code>1</code> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def increment_counter(self, counter_name: str, amount: int = 1) -&gt; None:\n    \"\"\"\n    Increment a counter metric.\n\n    Args:\n        counter_name: Name of the counter\n        amount: Amount to increment by (default: 1)\n    \"\"\"\n    with self._lock:\n        if counter_name in self._counters:\n            self._counters[counter_name] += amount\n        else:\n            self._logger.warning(f\"Unknown counter: {counter_name}\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_frame_captured","title":"<code>record_frame_captured(duration_ms)</code>","text":"<p>Record a frame capture event.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_frame_captured(self, duration_ms: float) -&gt; None:\n    \"\"\"Record a frame capture event.\"\"\"\n    self.record_timing(\"frame_capture\", duration_ms)\n    self.increment_counter(\"frames_captured\")\n    self._last_frame_time = perf_counter()\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_memory_update","title":"<code>record_memory_update(duration_ms, objects_added, objects_updated)</code>","text":"<p>Record a memory update operation.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_memory_update(self, duration_ms: float, objects_added: int, objects_updated: int) -&gt; None:\n    \"\"\"Record a memory update operation.\"\"\"\n    self.record_timing(\"memory_update\", duration_ms)\n    self.increment_counter(\"memory_updates\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_objects_detected","title":"<code>record_objects_detected(count, detection_time_ms)</code>","text":"<p>Record object detection results.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_objects_detected(self, count: int, detection_time_ms: float) -&gt; None:\n    \"\"\"Record object detection results.\"\"\"\n    self.record_timing(\"object_detection\", detection_time_ms)\n    self.increment_counter(\"objects_detected\", count)\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_pick_operation","title":"<code>record_pick_operation(duration_s, success)</code>","text":"<p>Record a pick operation.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_pick_operation(self, duration_s: float, success: bool) -&gt; None:\n    \"\"\"Record a pick operation.\"\"\"\n    self.record_timing(\"robot_pick\", duration_s * 1000)\n    self.increment_counter(\"pick_operations\")\n    if success:\n        self.increment_counter(\"pick_successes\")\n    else:\n        self.increment_counter(\"pick_failures\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_place_operation","title":"<code>record_place_operation(duration_s, success)</code>","text":"<p>Record a place operation.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_place_operation(self, duration_s: float, success: bool) -&gt; None:\n    \"\"\"Record a place operation.\"\"\"\n    self.record_timing(\"robot_place\", duration_s * 1000)\n    self.increment_counter(\"place_operations\")\n    if success:\n        self.increment_counter(\"place_successes\")\n    else:\n        self.increment_counter(\"place_failures\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.record_timing","title":"<code>record_timing(metric_name, duration_ms)</code>","text":"<p>Record a timing measurement.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>Name of the metric</p> required <code>duration_ms</code> <code>float</code> <p>Duration in milliseconds</p> required Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def record_timing(self, metric_name: str, duration_ms: float) -&gt; None:\n    \"\"\"\n    Record a timing measurement.\n\n    Args:\n        metric_name: Name of the metric\n        duration_ms: Duration in milliseconds\n    \"\"\"\n    with self._lock:\n        if metric_name in self._timings:\n            self._timings[metric_name].append(duration_ms)\n\n            if self._verbose and len(self._timings[metric_name]) % 10 == 0:\n                stats = TimingStats.from_samples(list(self._timings[metric_name]))\n                self._logger.debug(\n                    f\"{metric_name}: {duration_ms:.1f}ms \" f\"(avg: {stats.mean:.1f}ms, p95: {stats.p95:.1f}ms)\"\n                )\n        else:\n            # FIX: Create the metric on-the-fly if it doesn't exist\n            from collections import deque\n\n            self._timings[metric_name] = deque([duration_ms], maxlen=self._history_size)\n            self._logger.debug(f\"Created new timing metric: {metric_name}\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.reset","title":"<code>reset()</code>","text":"<p>Reset all metrics.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset all metrics.\"\"\"\n    with self._lock:\n        for timing_deque in self._timings.values():\n            timing_deque.clear()\n\n        for counter_name in self._counters:\n            self._counters[counter_name] = 0\n\n        self._start_time = datetime.now()\n        self._last_frame_time = None\n\n        self._logger.info(\"Performance metrics reset\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMetrics.timer","title":"<code>timer(metric_name)</code>","text":"<p>Create a context manager timer for an operation.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>Name of the metric to record</p> required <p>Returns:</p> Type Description <code>PerformanceTimer</code> <p>PerformanceTimer context manager</p> Example <p>with metrics.timer('frame_capture'):     frame = camera.get_frame()</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def timer(self, metric_name: str) -&gt; PerformanceTimer:\n    \"\"\"\n    Create a context manager timer for an operation.\n\n    Args:\n        metric_name: Name of the metric to record\n\n    Returns:\n        PerformanceTimer context manager\n\n    Example:\n        with metrics.timer('frame_capture'):\n            frame = camera.get_frame()\n    \"\"\"\n    return PerformanceTimer(self, metric_name)\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor","title":"<code>robot_environment.performance_metrics.PerformanceMonitor</code>","text":"<p>Background monitor that periodically logs performance metrics.</p> Example <p>metrics = PerformanceMetrics() monitor = PerformanceMonitor(metrics, interval_seconds=30) monitor.start()</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>class PerformanceMonitor:\n    \"\"\"\n    Background monitor that periodically logs performance metrics.\n\n    Example:\n        metrics = PerformanceMetrics()\n        monitor = PerformanceMonitor(metrics, interval_seconds=30)\n        monitor.start()\n\n        # ... do work ...\n\n        monitor.stop()\n    \"\"\"\n\n    def __init__(self, metrics: PerformanceMetrics, interval_seconds: float = 60.0, verbose: bool = True):\n        \"\"\"\n        Initialize performance monitor.\n\n        Args:\n            metrics: PerformanceMetrics instance to monitor\n            interval_seconds: Logging interval in seconds\n            verbose: Enable verbose logging\n        \"\"\"\n        self.metrics = metrics\n        self.interval_seconds = interval_seconds\n        self.verbose = verbose\n        self._logger = logging.getLogger(__name__)\n\n        self._stop_event = threading.Event()\n        self._thread: Optional[threading.Thread] = None\n\n    def start(self) -&gt; None:\n        \"\"\"Start the monitoring thread.\"\"\"\n        if self._thread is not None and self._thread.is_alive():\n            self._logger.warning(\"Monitor already running\")\n            return\n\n        self._stop_event.clear()\n        self._thread = threading.Thread(target=self._monitor_loop, daemon=True)\n        self._thread.start()\n\n        self._logger.info(f\"Performance monitor started (interval: {self.interval_seconds}s)\")\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop the monitoring thread.\"\"\"\n        if self._thread is None:\n            return\n\n        self._stop_event.set()\n        self._thread.join(timeout=5.0)\n\n        self._logger.info(\"Performance monitor stopped\")\n\n    def _monitor_loop(self) -&gt; None:\n        \"\"\"Main monitoring loop.\"\"\"\n        while not self._stop_event.wait(self.interval_seconds):\n            try:\n                if self.verbose:\n                    summary = self.metrics.get_summary()\n                    self._logger.info(f\"\\n{summary}\")\n                else:\n                    stats = self.metrics.get_stats()\n                    self._logger.info(\n                        f\"Performance: FPS={stats['camera']['fps']:.1f}, \"\n                        f\"Pick rate={stats['robot']['operations']['pick']['success_rate']:.0f}%, \"\n                        f\"Objects={stats['vision']['objects_detected']}\"\n                    )\n            except Exception as e:\n                self._logger.error(f\"Error in monitor loop: {e}\", exc_info=True)\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor--do-work","title":"... do work ...","text":"<p>monitor.stop()</p>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor-functions","title":"Functions","text":""},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor.__init__","title":"<code>__init__(metrics, interval_seconds=60.0, verbose=True)</code>","text":"<p>Initialize performance monitor.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>PerformanceMetrics</code> <p>PerformanceMetrics instance to monitor</p> required <code>interval_seconds</code> <code>float</code> <p>Logging interval in seconds</p> <code>60.0</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging</p> <code>True</code> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def __init__(self, metrics: PerformanceMetrics, interval_seconds: float = 60.0, verbose: bool = True):\n    \"\"\"\n    Initialize performance monitor.\n\n    Args:\n        metrics: PerformanceMetrics instance to monitor\n        interval_seconds: Logging interval in seconds\n        verbose: Enable verbose logging\n    \"\"\"\n    self.metrics = metrics\n    self.interval_seconds = interval_seconds\n    self.verbose = verbose\n    self._logger = logging.getLogger(__name__)\n\n    self._stop_event = threading.Event()\n    self._thread: Optional[threading.Thread] = None\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor.start","title":"<code>start()</code>","text":"<p>Start the monitoring thread.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the monitoring thread.\"\"\"\n    if self._thread is not None and self._thread.is_alive():\n        self._logger.warning(\"Monitor already running\")\n        return\n\n    self._stop_event.clear()\n    self._thread = threading.Thread(target=self._monitor_loop, daemon=True)\n    self._thread.start()\n\n    self._logger.info(f\"Performance monitor started (interval: {self.interval_seconds}s)\")\n</code></pre>"},{"location":"api/utils/#robot_environment.performance_metrics.PerformanceMonitor.stop","title":"<code>stop()</code>","text":"<p>Stop the monitoring thread.</p> Source code in <code>robot_environment/performance_metrics.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the monitoring thread.\"\"\"\n    if self._thread is None:\n        return\n\n    self._stop_event.set()\n    self._thread.join(timeout=5.0)\n\n    self._logger.info(\"Performance monitor stopped\")\n</code></pre>"}]}